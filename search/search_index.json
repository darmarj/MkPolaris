{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tech-Docs","text":"<p>Welcome to Material for MkDocs.</p>"},{"location":"DevOps/Ansible/ad-Hoc/","title":"Conducting an orchestra","text":"<ul> <li>Apply patches and updates via yum, apt and other package manangers.</li> <li>Check resource usage(disk space, memory, CPU, swap space, network).</li> <li>Check log files.</li> <li>Manage system users and groups.</li> <li>Manage DNS settings, hosts files, etc</li> <li>Copy files to and form server.</li> <li>Deploy applications or run applications maintenance.</li> <li>Reboot servers.</li> <li>Manage cron jobs.</li> </ul>"},{"location":"DevOps/Ansible/ad-Hoc/#inventory-file-for-multiple-servers","title":"Inventory file for multiple servers","text":"<pre><code>[defaults]\ninventory = hosts.ini\n</code></pre> <pre><code># hosts.ini\n# Lines beginning with a # are comments, and are only included for illustration. These comments are overkill for most inventory files.\n\n# Application servers\n[app]\n192.168.60.4\n192.168.60.5\n\n# Database server\n[db]\n192.168.60.6\n\n# Group 'multi' with all servers\n[multi:children]\napp\ndb\n\n# Variables that will be applied to all servers\n[multi:vars]\n$ ansible_user=vagrant\n$ ansible_ssh_private_key_file=~/.vagrant.d/insecure_private_key\n</code></pre>"},{"location":"DevOps/Ansible/ad-Hoc/#first-ad-hoc-commands","title":"First ad-hoc commands","text":""},{"location":"DevOps/Ansible/ad-Hoc/#discover-ansibles-parallel-nature","title":"Discover Ansible's parallel nature","text":"<pre><code>$ ansible multi -a \"hostname\"\n</code></pre> <p>Tip</p> <p>If Ansible reports No Hosts matched or returns some other inventory-related error, you might not have your <code>ansible.conf</code> file in the correct directory, or it might not be the correct syntax. You can also try overriding the inventory file path by setting the ANSIBLE_INVENTORY environment variable explicitly: export ANSIBLE_INVENTORY=hosts.ini.</p>"},{"location":"DevOps/Ansible/ad-Hoc/#learning-about-your-environment","title":"Learning about your environment","text":""},{"location":"DevOps/Ansible/ad-Hoc/#disk-space-on-the-servers","title":"Disk space on the servers","text":"<pre><code>$ ansible multi -a \"df -h\"\n</code></pre>"},{"location":"DevOps/Ansible/ad-Hoc/#memory-free","title":"Memory free","text":"<pre><code>$ ansible multi -a \"free -m\"\n</code></pre>"},{"location":"DevOps/Ansible/ad-Hoc/#make-changes-using-ansible-modules","title":"Make changes using Ansible modules","text":""},{"location":"DevOps/Ansible/ad-Hoc/#chrony-daemon-to-keep-time-in-sync","title":"Chrony daemon to keep time in sync","text":"<pre><code># -b = become -m = module\n$ ansible multi -b -m yum -a \"name=chrony state=present\"\n$ ansible multi -b -m service -a \"name=chrony state=started enabled=yes\"\n$ ansible multi -b -a \"chronyc tracking\"\n</code></pre>"},{"location":"DevOps/Ansible/ad-Hoc/#make-changes-to-just-specfic-server","title":"Make changes to just specfic server","text":"<pre><code>$ ansible app -b -a \"service chrony restart\" --limit \"192.168.60.4\"\n\n# Regular expression[~]\n$ ansible app -b -a \"service ntpd restart\" --limit ~\".*\\.4\"\n</code></pre>"},{"location":"DevOps/Ansible/ad-Hoc/#configure-groups-of-servers-or-individual-servers","title":"Configure groups of servers, or individual servers","text":"<p>Since set up two seperate gorups in inventory file, <code>app</code> and <code>db</code>, target commands to just the servers in those groups.</p>"},{"location":"DevOps/Ansible/ad-Hoc/#configure-the-application-servers","title":"Configure the Application servers","text":"<p>The hypothetical web application uses Django, need to make sure Django ans its dependencies are installed. Django is not the official CentOS yum repository, using Pip for installation: <pre><code>$ ansible app -b -m yum -a \"name=python3-pip state=present\"\n$ ansible app -b -m pip -a \"name=django&lt;4 state=present\"\n</code></pre></p> <p>Check to make sure Django is installed and working correctly: <pre><code>$ ansible app -a \"python -m django --version\"\n</code></pre></p> <p>Let's install MariaDB ans start it: <pre><code>$ ansible db -b -m yum -a \"name:mariadb-server state=present\"\n$ ansible db -b -m service -a \"name=mariadb state=started enabled=yes\"\n</code></pre></p> <p>Configure the system firewall to ensure only the app servers can access the database: <pre><code>$ ansible db -b -m yum -a \"name=firewalld state=present\"\n$ ansible db -b -m service -a \"name=firewalld state=started enabled=yes\"\n$ ansible db -b -m firewalld -a \"zone=database state=present permanent=yes\"\n$ ansible db -b -m firewalld -a \"source=192.168.60.0/24 zone=database state=enabled permanent=yes\"\n$ ansible db -b -m firewalld -a \"port=3306tcp zone=database state=enabled permanent=yes\"\n</code></pre></p> <p>Assorting mysql_* modules with Ansible to control MariaDB server: <pre><code>$ ansible db -b -m yum -a \"name=python3-PyMySQL state=present\"\n$ ansible db -b -m mysql_user -a \"name=django host=% password=1234 priv=*.*:ALL state=present\"\n</code></pre></p>"},{"location":"DevOps/Ansible/ad-Hoc/#manage-users-and-groups-user-group-module","title":"Manage users and groups --- user &amp; group module","text":""},{"location":"DevOps/Ansible/ad-Hoc/#ansibles-user-and-group-modules-make-things-pretty-simple-and-standard-across-any-linux-flavor","title":"Ansible's <code>user</code> and <code>group</code> modules make things pretty simple and standard across any Linux flavor.","text":"<p>Add an <code>admin</code> group on the app servers <pre><code>$ ansible app -b -m group -a \"name=admin state=present\"\n</code></pre></p> <p>Add the user <code>johndoe</code> to <code>admin</code> group <pre><code>$ ansible app -b -m user =a \"name=johndoe group=admin createhome=yes\"\n</code></pre></p> Tip <p>Additional parameters:</p> <p>1). generate_ssh_key=yes</p> <p>2). uid=[uid]</p> <p>3). gid=[gid]</p> <p>4). shell=[shell]</p> <p>5). password=[encrpyted-password]</p> <p>Delete the account <pre><code>$ ansible app -b -m user -a \"name=johndoe state=absent remove=yes\"\n</code></pre></p> <p> Official Ansible: User module</p>"},{"location":"DevOps/Ansible/ad-Hoc/#manage-packages-package-module","title":"Manage packages --- package module","text":"<p>A generic <code>package</code> module that can be used for easier cross-platform. <pre><code>$ ansible app -b -m package -a \"name=git state=present\"\n</code></pre></p>"},{"location":"DevOps/Ansible/ad-Hoc/#manage-files-and-direcoties","title":"Manage files and direcoties","text":""},{"location":"DevOps/Ansible/ad-Hoc/#get-information-about-a-file-stat-module","title":"Get information about a file --- stat module","text":"<p>If you need to check a file's permission, MD5, or owner, use Ansible's <code>stat</code> module: <pre><code>$ ansible multi -m stat -a \"path=/etc/environment\"\n</code></pre></p>"},{"location":"DevOps/Ansible/ad-Hoc/#copy-a-file-to-the-servers-copy-module","title":"Copy a file to the servers --- copy module","text":"<p>Most file copy operations can be completed with Ansible's <code>copy</code> module: <pre><code>$ ansible multi -m copy -a \"src=/etc/hosts dest=/tmp/hosts\"\n</code></pre></p> <p>Tip</p> <p>The copy module is perfect for single-file copies, and works very well with small directories. When you want to copy hundreds of files, especially in very deeply-nested directory structures, you should consider either copying then expanding an archive of the files with Ansible's unarchive module, or using synchronize or rsync modules.</p>"},{"location":"DevOps/Ansible/ad-Hoc/#retrieve-fetch-module","title":"Retrieve --- fetch module","text":"<pre><code>$ ansible multi -b -m fetch -a \"src=/etc/hosts des=/tmp\"\n</code></pre> <p>Tip</p> <p>The parameter flat could fetch the files directly into the <code>tmp</code> directory. However, filenames must be unique for working. This is only purpose for single host from.</p>"},{"location":"DevOps/Ansible/ad-Hoc/#create-directories-and-files-file-module","title":"Create directories and files --- file module","text":"<p>Use the <code>file</code> module to create files and directories(like touch), manage permissions and ownership, modify <pre><code>$ ansible multi -m file -a \"dest=/tmp/test mode=644 state=directory\"\n</code></pre> Symlink(set state=link): <pre><code>$ ansible multi -m file -a \"src=/src/file dest=/dest/symlink state=link\"\n</code></pre></p>"},{"location":"DevOps/Ansible/ad-Hoc/#run-operations-in-the-background","title":"Run operations in the background","text":"<p>It's more helpful when managing many servers. *   -B : the maximum amount of time(in seconds) to let the job run. *   -P : the amount of time(in seconds) to wait between polling the servers for an updated job status. <p>Warning</p> <p>As of Ansible 2.0, asynchronous polling on the command line(via the -P flag) no longer displays output in real time. Resolution</p> <pre><code>$ ansible multi -b -B 3600 -P 0 -a \"yum -y update\"\n</code></pre> <p>Check on the status elsewhere using Ansible's async_status <pre><code>$ ansible multi -b -m async_status -a \"jid=\"\n</code></pre></p> <p>Tip</p> <p>For tasks don't track remotely, it's usually a good idea to log the progress of the task somewhere, and also send some sort of alert on failure - especially, for example, when running backgrounded tasks that perform backup operations, or when running business-critical database mainternance task.</p>"},{"location":"DevOps/Ansible/ad-Hoc/#check-log-files","title":"Check log files","text":"<ol> <li>Operations that continuously monitor a file, like <code>tail -f</code>, won't work via Ansible, because only displays output after the operation is complete, and won't be able to send teh Control-C command to stop following the file. Someday, the async module might have this feature.</li> <li>It's not good idea to run a command that returns a huge amount of data via stdout via Ansible. If going to cat a file larger than a few KB, you should probably log into the server(s) individually.</li> <li>If redirect and filter output from a command run via Ansible, need to use the <code>shell</code> module instead of Ansible's defualt <code>command</code> module (add -m <code>shell</code> to your commands).</li> </ol> <p>A simple example: <pre><code>$ ansible multi -b -a \"tail /var/log/messages\"\n</code></pre></p>"},{"location":"DevOps/Ansible/ad-Hoc/#filter-the-messages-log-with-something-like-grep-cant-use-default-command-module-but-instead-shell","title":"Filter the messages log with something like grep, can't use default <code>command</code> module, but instead, shell:","text":"<pre><code>$ ansible multi -b -m shell -a \"tail /var/log/messages | grep ansible-command | wd -l\"\n</code></pre>"},{"location":"DevOps/Ansible/ad-Hoc/#manage-cron-jobs-cron-module","title":"Manage cron jobs --- cron module","text":"<p>If want to run a shell script on all the servers every day at 4 a.m., add the cron job with: <pre><code>$ ansible multi -b -m cron -a \"name='daily-cron-all-servers' hour=4 job='/path/to/daily-script.sh'\"\n</code></pre> Ansible will assume * for all values that don't specify(valid values are <code>day, hour, minute, month and weekday</code>). Also specify special time values like <code>reboot, yearly or monthly</code> using special_time=[value].</p> <p>Furthermore, can set the user the job will run under via user=[user], and ceate a backup of the current crontab by passing <code>backup=yes</code>.</p> <p>Delete the cron job with: <pre><code>$ ansible multi -b -m cron -a \"name='daily-cron-all-server' state=absent\"\n</code></pre></p>"},{"location":"DevOps/Ansible/ad-Hoc/#customize-crontab-files","title":"Customize crontab files","text":"<p>By specify the location to the cron file with:<code>cron_file=cron_file_name</code> (where corn_file_name is a cron file located in /etc/cron.d)</p>"},{"location":"DevOps/Ansible/ad-Hoc/#deploy-a-version-controlled-application-git-module","title":"Deploy a version-controlled application --- git module","text":"<p>Ansible's <code>git</code> module lets to specify a branch, tag, or even a specific commit with the <code>version</code> parameter (which also works for <code>branch name</code>, like <code>pro</code>) <pre><code>$ ansible app -b -m git -a \"repo=git://example.com/path/to/repo.git dest=/opt/myapp update=yes version=1.2.4\"\n</code></pre></p> <p>Info</p> <p>Error message <code>Failed to find required executable git</code>, need to install Git on the server. <code>ansible app -b -m package -a \"name=git state=present\"</code>.</p> <p>Error message <code>unknown hostkey</code>, ass the option <code>accept_hostkey=yes</code> to the command, or add the hostkey to your server's <code>known_hosts</code> file before running the command.</p>"},{"location":"DevOps/Ansible/architecture/","title":"Architecture","text":""},{"location":"DevOps/Ansible/architecture/#what-is-anible","title":"What is Anible?","text":"<p>Ansible can configure any resource on a server through its idempotent playbooks and even run ad-hoc scripts. It takes complex or cumbersome manual tasks and orchestrates them by automating the process. Tasks done wrong are typically repetitive in nature. Why? Because our brains get used to the process, get bored, and start making mistakes. Creating automation solutions using a programmable tool like Ansible takes away repetitive manual tasks. The manual task becomes code that is stored in source control, so others can review the work that will be done as well. Many configuration management tools exist from Chef, Puppet, Salt, CFEngine, and so on. What makes Ansible so different? The answer is simplicity. Tools like Chef and Puppet use actual programming languages to write automation instructions like Ruby. Ansible, on the other hand, doesn\u2019t use a programming language but a much simpler markup language called YAML. YAML is known for its simplicity and human-readable language style.</p>"},{"location":"DevOps/Ansible/architecture/#what-can-ansible-do","title":"What can Ansible do?","text":"<ul> <li>Container Automation: To deploy and manage containers with Ansible</li> <li>Infrastructure Automation: To manage, create, and deploy infrastructure</li> <li>Cloud Automation: To automate resource creation and service management in many clouds including Azure</li> <li>Application Management: To install, update, and manage applications on a server</li> <li>Software-Defined Networking: To define what a network looks like both on-prem and in the cloud with code</li> </ul>"},{"location":"DevOps/Ansible/architecture/#why-to-use","title":"Why to use?","text":""},{"location":"DevOps/Ansible/beyond-the-basic/","title":"Beyond the basic","text":"<p>Cover how to run plays with more granularity, how to organize tasks and playbooks for simplicity and usability, and other advanced playbook topics that will help to manage infrastructure with even more confidence.</p>"},{"location":"DevOps/Ansible/beyond-the-basic/#handlers","title":"Handlers","text":""},{"location":"DevOps/Ansible/beyond-the-basic/#in-some-circumstances-notify-multiple-handlers-or-even-have-handlers-notify-additional-handlers-are-necessary","title":"In some circumstances, notify multiple handlers, or even have handlers notify additional handlers are necessary.","text":"<p>Use a list for the <code>notify</code> option: <pre><code>- name: Rebuild application configuration.\n  command: /opt/app/rebuild.sh\n  notify:\n    - restart apache\n    - restart memcached\n</code></pre></p>"},{"location":"DevOps/Ansible/beyond-the-basic/#to-have-one-handlers-notify-another","title":"To have one handlers notify another","text":"<p>Add a <code>notify</code> option onto the handlers - handlers are basically glorified tasks that can be called by the <code>notify</code> option, but since they act as tasks themselves, they can chain themselves to other handlers: <pre><code>handlers:\n  - name: restart apache\n    service: name=apache2 state=started\n    notify: restart memcached\n\n  - name: restart memcached\n    service: name=memcached state=restarted\n</code></pre></p> <p>Warning</p> <ul> <li>Handlers will only be run if a task notifies the handlers; if a task that would've notified the handlers is skpped due to a <code>**when**</code> condition or something of the like, the handlers will not be run.</li> <li>Handlers will run once, and only once, at the end of a play. If you absolutely need to override this behavior and run handlers in the middle of a playbook, you can use the meta module to do so.     <pre><code>    - meta: flush_handlers\n</code></pre></li> <li>If the play <code>fails</code> on a particular host(or all hosts) before handlers are notified, the handlers will never be run. If it's desirable to always run handlers, even after the playbook has <code>failed</code>, you can use the <code>meta</code> module as describled above as a seperate task in the playbook, or you use the command line flag <code>--force-handlers</code> when running your playbook. Handlers won't run on any hosts that became unreachable during the playbook's run.</li> </ul>"},{"location":"DevOps/Ansible/beyond-the-basic/#enviornment-variables","title":"Enviornment variables","text":""},{"location":"DevOps/Ansible/beyond-the-basic/#set-some-environment-variables-for-your-remote-user-account","title":"Set some environment variables for your remote user account","text":"<p>Do by adding lines to the remote user's <code>.bash_profile</code>. <pre><code>    - name: Add an environment variable to the remote user's shell.\n      lineinfile:\n        dest: ~/.bash_profile\n        regexp: '^ENV_VAR='\n        line: \"ENV_VAR=value\"\n</code></pre></p>"},{"location":"DevOps/Ansible/beyond-the-basic/#to-use-an-environment-variable-in-further-per-tasks","title":"To use an environment variable in further Per-tasks","text":"<p>It's recommended to use a task's <code>register</code> option to store the environment variable in a variable Ansible can use later, for example: <pre><code>    - name: Add an environment variable to the remote user's shell\n      lineinfile:\n        dest: ~/.bash_profile\n        regexp: '^ENV_VAR='\n        line: \"ENV_VAR=value\"\n\n    - name: Get the value of the environment variable just added.\n      shell: 'source ~/.bash_profile &amp;&amp; echo $ENV_VAR'\n      register: foo\n\n    - name: Print the value of the environment variable.\n      debug:\n        msg: \"The variable is {{ foo.stdout }}\"\n</code></pre></p> <p>Bug</p> <p>In some situations, the tasks all run over a persisitent or quasi-cached SSH session, over which <code>$ENV_VAR</code> would't yet be defined.</p> <p>Use <code>source ~/.bash_profile</code> because to make sure Ansible using the latest environment configuration for the remote user.</p> <p>Why <code>~/.bash_profile</code>?</p> <p>There're many different places can store environment variables, including <code>.bashrc, .profile, and .bash_login</code> in a user's home folder.</p> <p>In some case, since want the environment variable to be available to Ansible, which runs a <code>pseudo-TTY</code> shell session, in which case <code>.bash_profile</code> is used to configure the environment.</p> <p> Configuring your login sessions with dot files</p> <p>Linux will also read global environment variables added to <code>/etc/environment</code>, so add variable there: <pre><code>    - name: Add a global environment variable.\n      lineinfile:\n        dest: /etc/environment\n        regexp: '^ENV_VAR='\n        line: \"ENV_VAR=value\"\n      become: yes\n</code></pre></p> <p>Tip</p> <p>If application requires many environment variables (as is the case in many Jave applications), might consider using <code>copy</code> or <code>template</code> with a local file instead of using <code>lineinfile</code> with a large list of itmes.</p>"},{"location":"DevOps/Ansible/beyond-the-basic/#per-tasks-environment-variables","title":"Per-tasks environment variables","text":""},{"location":"DevOps/Ansible/beyond-the-basic/#set-the-environment-for-just-one-taks","title":"Set the environment for just one taks.","text":"<pre><code>vars:\n  proxy_vars:\n    http_proxy: http://example-proxy:80/\n    https_proxy: https://example-proxy:80/\n    [etc...]\n\ntasks:\n- name: Download a file, using example-proxy as a proxy.\n  get_url:\n    url: http://www.example.com/file.tar.org\n    dest: ~/Downloads/\n  environment: proxy_vars\n# In the 'var' section of the playbook (set to 'absent' to remvoe):\nproxy_state: present\n\n# In the 'tasks' section of the playbook:\n- name: Configure the proxy.\n  lineinfile:\n    dest: /etc/environment\n    regexp: \"{{ item.regexp }}\"\n    line: \"{{ item.line }}\"\n    state: \"{{ proxy_state }}\"\n  with_items:\n    - regexp: \"^http_proxy=\"\n      line: \"http_proxy=http_proxy://example-proxy:80\"\n    - regexp: \"^https_proxy=\"\n      line: \"http_proxy=http_proxy://example-proxy:443\"\n    - regexp: \"^https_proxy=\"\n      line: \"ftp_proxy=ftp_proxy://example-proxy:80\"\n</code></pre>"},{"location":"DevOps/Ansible/beyond-the-basic/#variables","title":"Variables","text":"<p>Works with other systems. Usually begin with a letter([A-za-z]), but also start with an underscore(_). It can include any number of letters, underscores, or numbers.</p> <p>Warning</p> <p>While it's not explictly stated in Ansible's documentation, starting with a variable with an underscore (e.g. _my_variable) is uncommon, but sometimes is used as a way of indicating a 'private' or 'internal use only' variable. For example, maintain prefix variables with an underscore if they're meant for internal role use only, and should not be override by playbooks using the role.</p>"},{"location":"DevOps/Ansible/beyond-the-basic/#playbook-variables","title":"Playbook Variables","text":"<p>Variable file can also be imported conditionally. <pre><code>---\n- hosts: example\n\npre_task:\n  - include_vars: \"{{ item }}\"\n    with_first_found:\n      - \"apache_{{ ansible_os_family  }}.yml\"\n      - \"apache_defuault.yml\"\n\ntasks:\n  - name: Ensure Apache is running.\n    service:\n      name: \"{{ apache_service_name }}\"\n      state: running\n</code></pre></p> <p>Note</p> <p>Add two files in the same fodler as the example playbook, <code>apache_RedHat.yml</code>, and <code>apache_defuault.yml</code>. Define the variable <code>apache_service_name: httpd</code> in the CentOS file, and <code>apache_service_name: apache2</code> in the default file.</p>"},{"location":"DevOps/Ansible/beyond-the-basic/#inventory-variables","title":"Inventory Variables","text":"<p>Define inline <pre><code># Host-specific variables\n[washington]\napp1.example.com proxy_state=present\napp2.example.com proxy_state=absent\n</code></pre></p> <p>Define for the entire group <pre><code>[washington:vars]\ncdn_host=washington.static.example.com\napi_version=3.0.1\n</code></pre></p> <p>For more than a few variables, espcially that apply to more than one or two hosts. Ansible recommends NOT storing variables within the inventory. Instead, use group_vars and host_varsYAML files within a specific path, and Ansible will assign them to individual hosts and groups defined in your inventory. <pre><code># Ansible playbook\n---\n- hosts: all\n  become: true\n\n  vars_files:\n    - vars.yml\n</code></pre> <pre><code># vars.yml\n---\nfoo: bar\nbaz: qux\n</code></pre></p>"},{"location":"DevOps/Ansible/beyond-the-basic/#registered-variables","title":"Registered Variables","text":"<p>If to determine for the return code, stderr, or stdout to run a later task. Ansible allow to use <code>register</code> to store the output of a particular command in a variable at runtime.</p>"},{"location":"DevOps/Ansible/beyond-the-basic/#accessing-variables","title":"Accessing Variables","text":"<p>Define a list variable: <pre><code>foo_list:\n  - one\n  - two\n  - three\n</code></pre></p> <p>Stand Python array syntax <pre><code>foo[0]\n</code></pre></p> <p>Convenient filter provided by Jinja <pre><code>foo|first\n</code></pre></p> <p>For larger and more structured arrays(like IP adress), access any part of the array by drilling through the array keys, either using <code>bracket([])</code> or <code>dot(.)</code> <pre><code>---\n- hosts: all\n  # become: true\n\n  vars:\n    ip_address: \"{{ ansible_enp5s0.ipv4.address }}\"\n    ip_network: \"{{ ansible_enp5s0.ipv4.network }}\"\n\n  tasks:\n    - debug:\n        # var: ip_address\n        msg:\n          - ip_address is {{ ip_address }}.\n          - ip_network is {{ ip_network }}.\n</code></pre></p>"},{"location":"DevOps/Ansible/beyond-the-basic/#host-and-group-variables","title":"Host and Group Variables","text":"<p>Define variables on a per-host or per-group basis within inventory file:</p> <p>Note</p> <p>In following case, Ansible will use the group default variable 'john' for {{ admin_user }}, but for <code>host1</code> and <code>host2</code>, the admin users define alongside the hostname will be used.     <pre><code>[group]\nhost1 admin_user=jane\nhost2 admin_user=jack\nhost3\n\n[group:vars]\nadmin_user=john\n</code></pre></p>"},{"location":"DevOps/Ansible/beyond-the-basic/#automatically-loaded-group_vars-and-host_vars","title":"Automatically-loaded <code>group_vars</code> and <code>host_vars</code>","text":"<p>Ansible will search within the same director as your inventory file( or inside <code>/etc/ansible</code> if using the default inventory file at <code>/etc/ansible/hosts</code>) for two specific dicrecotories: <code>group_vars</code> and <code>host_vars</code></p>"},{"location":"DevOps/Ansible/beyond-the-basic/#factsvairiables-derived-from-system-information","title":"Facts(Vairiables derived from system information)","text":"<p>By default, whenever running an Ansible playbook, firstly it gathers information(\"factc\") about each host in the play. Facts can be extremely helpful when running playbooks. It can use gathered information like <code>host IP address, CPU type, disk space, operating system information, and network interface inforamtion</code> to change when certain tasks are run, or to chagne certain inforamtion used in configuration file. <pre><code>ansible [HOST] -m setup\nrocky8 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"ansible_all_ipv4_addresses\": [\n            \"123.123.123.123\"\n        ],\n        \"ansible_all_ipv6_addresses\": [\n            \"fd42:de7d:cge9:c468:6a0d:4b2f:fai6:d76d\",\n            \"re08::527e:4bec:d2g3:e80m\"\n[...]\n</code></pre></p> <p>Tip</p> <p>If don't need to use facts, and would like to save a few seconds per-host when running playbooks(this can be espcially helpful when running an Ansible playbook angainst dozens or hundreds of servers), you can set <code>gather_facts: no</code> in playbook.</p>"},{"location":"DevOps/Ansible/beyond-the-basic/#local-factsfactsd","title":"Local Facts(Facts.d)","text":"<p>Another way of defining host-specific facts is to place a <code>.fact</code> file in a special directory on remote hosts <code>/etc/ansible/facts.d</code> <pre><code># /etc/ansible/facts.d/settings.fact\n[users]\nadmin=jane,john\nnormal=jim\n</code></pre></p> <pre><code>ansible hostname -m setup -a \"filter=ansible_local\"\nrocky8 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"ansible_local\": {\n            \"settings\": {\n                \"users\": {\n                    \"admin\": \"jane,john\",\n                    \"normal\": \"jim\"\n                }\n            }\n        },\n        \"discovered_interpreter_python\": \"/usr/libexec/platform-python\"\n    },\n    \"changed\": false\n</code></pre>"},{"location":"DevOps/Ansible/beyond-the-basic/#ansible-vault-keeping-secrets-secret","title":"Ansible Vault - Keeping secrets secret","text":"<p>Works much like a real-world vault:</p> <ul> <li>Take any YAML file would normally have in the playbook  e.g. a variables file, host vars, group vars, role default vars, or even task includes!</li> <li>Ansible encrypt the vault('closes the door'), using a key(a password that set).</li> <li>Store the key(vault's password) seperately from the playbook in a location only to control or can access.</li> <li>Use the key to let Ansible decrypt the encrypted vault whenever run the playbook.</li> </ul> <p>Let's see how it works in practice. <pre><code>---\n- hosts: localhost\n  connection: local\n  gather_facts: no\n\n  vars_files:\n    - vars/api_key.yml\n\n  tasks:\n    - name: Echo and API key which was injected into the env.\n      shell: echo $API_KEY\n      environment:\n        API_KEY: \"{{ myapp_api_key }}\"\n      register: echo_result\n\n    - name: Show the result\n      debug: var=echo_result.stdout\n</code></pre></p> <pre><code># vars/api_key.yml\n---\nmyapp_api_key: \"l9bTqfBlbXTQiDaJMqgPJ1VdeFLfId98\"\n</code></pre> <p>To encrypt the file with Ansible Vault. <pre><code>ansible-vault encrypt vars/api_key.yml\n</code></pre></p> <p>Providing the password at playbook runtime works when running a playbook interactively: <pre><code># Use --ask-vault-pass to supply the vault password at runtime\nansible-playbook main.yml --ask-vault-pass\n</code></pre></p> <p>Supply vault passwords via a password file. Set strict permission(e.g.600) so only can be read and write the file. <pre><code># Use --vault-password-file to supply the password via file or script.\nansible-playbook main.yml --vault-password-file PATH/OF/PASSWORD/FILE\n</code></pre></p> <p>Info</p> <p>AES-256 encryption is extremely secure. it would take billions of billions of years to decrypt a single file, even if all of today's fastest supercomupter cluster were all put to the task 24*7.</p>"},{"location":"DevOps/Ansible/beyond-the-basic/#varible-precedence","title":"Varible Precedence","text":"<p>Ansible's documentation provides the following ranking:</p> Parameter Description <code>--extra-vars</code> Passed in via the commandline Task-level vars Task block Block-level vars All tasks in a block <code>[role]/vars/main.yml</code> Role vars <code>include_vars</code> Module <code>set_facts</code> Module <code>register</code> In a task <code>vars_files, vars_prompt, vars</code> individual play-level HOST Host facts <code>host-vars</code> Playbook <code>group-vars</code> Playbook <code>host_vars, group_vars, vars</code> Inventory <code>[role]/vars/main.yml</code> Role default vars <p> Official Ansible: Understanding Variable Preceduce</p>"},{"location":"DevOps/Ansible/beyond-the-basic/#ifthenwhen-conditions","title":"If/then/when - Conditions","text":"<p>It's worthwhile to cover a small part of Jinja(the syntax Ansible uses both for tempaltes and for conditionals). Jinja allows the definition of literals like <code>strings, integers, floats, lists, tuples, dictionaries and bealoons.</code></p> <p> Official Jinja: Templates</p>"},{"location":"DevOps/Ansible/beyond-the-basic/#register","title":"register","text":"<p>\"Register\" a variable, and once registered, the variable will be avaiblable to all subsequent tasks.</p> <p>Tip</p> <p>If want to see the difference properties of a particular registered variable, run a playbook with -v to inspect play output.</p>"},{"location":"DevOps/Ansible/beyond-the-basic/#when","title":"when","text":"<p><code>when</code> is even more powerful if used in conjunction with variables registered by previous tasks. <pre><code>- command: my-app --status\n  register: myapp_result\n\n- command: do-something-to-my-app\n  when: \"'ready' in myapp_result.stdout\"\n</code></pre></p>"},{"location":"DevOps/Ansible/beyond-the-basic/#changed-when-and-failed_when","title":"changed-when and failed_when","text":"<p>If use the <code>command</code> or <code>shell</code> module when using <code>changed_when</code>, Ansible will always report a change. Most modules report whether resulted in changes correctly, but you can override this behavior by invoking <code>changed_when</code>.</p> <p>Example when using PHP Composer as a command to install project dependencies. <pre><code>- name: Install dependencies via Composer\n  command: \"/usr/local/bin/composer global requrie phpunit/phpunit --prefer-dist\"\n  register: composer\n  changed_when: \"'Nothing to install' not in composer.stdout\"\n</code></pre></p> <p>Example to parse the stderr of a Jenkins CLI command to see if it did, in fact, faild to perform the command that requested:</p> <p>Note</p> <p><pre><code>- name: Import a Jenkins job via CLI\n  shell: &gt;\n    java -jar /opt/jenkins-cli.jar -s http://localhost:8000/\n    create-job \"My job\" &lt; /usr/local/my-job.xml\n  register: import\n  failed_when: \"import.stderr and 'exists' not in import.stderr\"\n</code></pre> In this case, only want Ansible to report a failure when the command returns an error, and that error doesn't contain 'exists'. It's debatable whether the command should report a job already exists via stderr, or just print the result to stdout... but it's easy to account for whatever the command does with Ansible!</p>"},{"location":"DevOps/Ansible/beyond-the-basic/#ingore_errors","title":"ingore_errors","text":"<p>When errors don't actually indicate a problem, but it's anonnying. Add <code>ignore_errors: ture</code> to the task. Ansible will remain blissfully unaware of any problems running a particular task.</p>"},{"location":"DevOps/Ansible/beyond-the-basic/#deltegation-local-actions-and-pauses","title":"Deltegation, Local Actions, and Pauses","text":"<p>Ansible allows any task to be delegated to a particular host using <code>delegate_to:</code> <pre><code>- name: Add server to Munin monitoring configuration\n  command: monitor-server webservers {{ inventory_hostname }}\n  deltegate_to: \"{{ monitoring_master }}\"\n</code></pre> Delegation is often used to mamage a server's participation in a load balancer or replication pool; might either run particular command locally(as in the example below), or could be used one of Ansible's build-in load balancer modules and <code>delegate_to</code> a specific load balancer host directly: <pre><code>- name: Remove server from load balancer\n  command: remove-from-lb {{ inventory_hostname }}\n  deltegate_to: 127.0.0.1\n</code></pre> If delegating a task to localhost, Ansible has a convenient shorthand can use, <code>local_action</code>, instead of adding the entire <code>delegate_to</code> line: <pre><code>- name: Remove server from load balancer\n  local_action: command remove-from-lb {{ inventory_hostname }}\n</code></pre></p>"},{"location":"DevOps/Ansible/beyond-the-basic/#pausing-playbook-execution-with-wait_for","title":"Pausing playbook execution with <code>wait_for</code>","text":"<p>You might also use <code>local_action</code> in the middle of a playbook to wait for a freshlybooted server or application to start listening on a particular port: <pre><code>- name: Wait for web server to start\n  local_action:\n    module: wait_for\n    host: \"{{ inventory_hostname }}\"\n    port: \"{{ webserver_port }}\"\n    delay: 10\n    timeout: 300\n    state: started\n</code></pre></p> More things do for <code>wait_for</code> <ul> <li>Using <code>host</code> and <code>port</code>, wait a maximum of <code>timeout</code> seconds of the port to be avaible.</li> <li>Using <code>path</code> (and <code>search_regex</code> if desired), wait a maximum of <code>timeout</code> seconds for the file to be present(or absent).</li> <li>Using <code>host</code> and <code>port</code> and <code>drained</code> for the <code>state</code> parameter, check if given port has drained all it's active connections.</li> <li>Using <code>delay</code>, can simply pause playbook execution for a given amount of time(in seconds).</li> </ul>"},{"location":"DevOps/Ansible/beyond-the-basic/#running-an-entire-playbook-locally","title":"Running an entire playbook locally","text":"<p>To speed up playbook execution by avoiding the SSH connection overhead, use <code>--connection=local</code> Quick example to run the command <code>ansible-play test.yml --connection=local:</code> <pre><code>---\n- hosts: 127.0.0.1\n  gather_facts: no\n\n  tasks:\n    - name: Check the current system date.\n      command: date\n      register: date\n\n    - name: Print the current system date.\n      debug:\n        var: date.stdout\n</code></pre></p>"},{"location":"DevOps/Ansible/beyond-the-basic/#prompts","title":"Prompts","text":"<p>Under rare circumstances, may require the user to enter the value of a variable that will be used in the playbook. If the playbook requires a user's personal login information, or if pormpt for a version or other values that may change depending on who is running the playbook, or where it's being run, and if there's no other way this information can be configured (e.g. using environment variables, inventory variables, etc), use <code>var_prompt</code>. <pre><code>---\n- hosts: all\n\n  vars_prompt:\n    - name: share_user\n      prompt: \"What's your network username?\"\n    - name: share_pass\n      prompt: \"What's your network password?\"\n      private: yes\n</code></pre></p> <p>a few special options add for prompts</p> <ul> <li><code>private</code>: if set to <code>yes</code>, the user's input will be hidden on the command line.</li> <li><code>defaulAt</code>: can set a default value for the prompt, to save time for the end user.</li> <li><code>encrypt / confirm / salt_size</code>: set for password so can verify the entry (the user will have to enter the password twice if confirm is set to yes), and encrypt it using salt (with the specified size and crypt scheme).</li> </ul> <p>avoid unless absoutely necessary</p> <p>It's prefer to use role or playbook variables, inventory variables, or even local environment variables, to maintain complete automation of the playbook run.</p>"},{"location":"DevOps/Ansible/beyond-the-basic/#tags","title":"Tags","text":"<p>Tags allow to run (or exclude) subsets of a playbook's tasks. Tag roles, included files, individual tasks, and even entire plays. <pre><code>---\n# Apply tags to an entire play.\n- hosts: webservers\n  tags: deploy\n\n  roles:\n    # Tags applied to a role will applied to tasks in the role\n    - role: tomcat\n      tags: ['tomcat', 'app']\n\n  tasks:\n    - name: Notify on completion\n      local_action:\n        module: osx_say\n        msg: \"{{inventory_hostname}} is finished!\"\n        voice: Zarvox\n      tags:\n        - notifications\n        - say\n\n    - import_tasks: foo.yml\n      tags: foo\n</code></pre></p> <p>Exclude anything tagged with <code>notifications</code>, can use <code>--skip-tags</code>. <pre><code>ansible-playbook tags.yml --skip-tags \"notifications\"\n</code></pre></p> <p>handy tips</p> <p>Incredibly handy if have a decent tagging structure. When only want to run a particular portion of a playbook, or one play in a series. There is one caveat when adding one or multiple tags using the <code>tags</code> option in a playbook: <pre><code># Shorthand list syntax\ntags: ['one', 'two', 'three']\n\n# Explicit list syntax\ntags:\n  - one\n  - two\n  - three\n\n# Non-working example\ntags: one, two, three\n</code></pre></p>"},{"location":"DevOps/Ansible/beyond-the-basic/#blocks","title":"Blocks","text":"<p>Group related tasks together and apply particular task parameters on the block level. Also allow to handle errors inside the blocks in a way similar to most programming languages' exception handling. <pre><code>---\n- hosts: web\n  tasks:\n    # Install and configure Apache on RHEL/CentOS hosts\n    - block:\n        - yum: name=httpd state=present\n        - template: src=httpd.conf.j2 dest=/etc/httpd/conf/httpd.conf\n        - service: name=httpd state= started enabled=yes\n      when: ansible_os_family == 'RedHat'\n      become: yes\n\n    # Install and configure Apache on Debian/Ubuntu hosts\n    - block:\n        - yum: name=apache2 state=present\n        - template: src=httpd.conf.j2 dest=/etc/apache2/apache2.conf\n        - service: name=apache2 state= started enabled=yes\n      when: ansible_os_family == 'Debian'\n      become: yes\n</code></pre> If want to perform a series of tasks with one set of task parameter (e.g. <code>with_items, when, or become</code>) applied, blocks are quite handdy. Blocks are also useful if want to be able to gracefully handle failures in certain tasks. <pre><code>tasks:\n  - block:\n      - name: Script to connect the app to a monitoring service\n        script: monitoring-connect.sh\n    rescue:\n      - name: This will only run in case of an error in the block\n        debug: msg=\"There was an error in the block\"\n    always:\n      - name: This will always run, no matter what\n        debug: msg=\"This always executes.\"\n</code></pre> Blocks can be very helpful for building reliable playbook, but just like exceptions in programming languages, <code>block/rescue/always</code> failure handling can over-complicate things. <code>failed_when</code> per-task to define acceptable failure conditions, or to structure playbook in a different way is easier to maintain idempotence, rather than to use <code>block/rescure/always</code>.</p>"},{"location":"DevOps/Ansible/playbook-organization/","title":"Playbook Organization - Roles, Includes, and Imports","text":""},{"location":"DevOps/Ansible/playbook-organization/#imports","title":"Imports","text":"<p>Tasks can easily be included in a similar way. In the <code>tasks:</code> section of your playbook, add <code>import_tasks</code> directives like so: <pre><code>tasks:\n  - import_tasks: import_tasks.yml\n</code></pre> Just like with variable include files, tasks are formatted in a flat list in the included file. <code>import_tasks.yml</code> as the example: <pre><code>---\n- name: Add profile info for user.\n  copy:\n    src: example_profile\n    dest: \"/home/{{ usrname }}/.profile\"\n    owner: \"{ username }\"\n    group: \"{ username }\"\n    mode: 0744\n\n- name: Add private keys for user.\n  copy:\n    src: \"{{ item.src }}\"\n    dest: \"/home/{{ username }}/.ssh/{{ item.dest }}\"\n    owner: \"{ username }\"\n    group: \"{ username }\"\n    mode: 0600\n  with_items: \"{{ ssh_private_keys }}\"\n\n- name: Restart example service.\n  service:\n    name: example\n    state: restarted\n</code></pre></p>"},{"location":"DevOps/Ansible/playbook-organization/#includes","title":"Includes","text":"<p><code>import_tasks</code> statically imports the task file as if it were part of the main playbook, once, before the play is executed.</p> <p><code>include_tasks</code> dynamic to do different things depending on how the rest of the playbook runs.</p> <p>case</p> <p><pre><code>---\n- name: Check for existing log files in dynamic log_file_paths variable\n  find:\n    paths: \"{{ item }}\"\n    patterns: '*.log'\n  register: found_log_file_paths\n  with_items: \"{{ log_file_paths }}\"\n</code></pre> the <code>log_file_paths</code> variable is set by a task earlier in playbook, so this include file would't be able to know the value of that variable until the playbook has already partly completed.</p>"},{"location":"DevOps/Ansible/playbook-organization/#dynamic-includes","title":"Dynamic includes","text":"<p>On Ansible 2.0 and later version, <code>includes</code> has been evaluated during playbook exeuction.</p> <p>case</p> <p><pre><code># Include extra tasks file, only if it's present at runtime\n- name: Check if extra_tasks.yml is present\n  stat:\n    path: tasks/extra_tasks.yml\n  register: extra_tasks_file\n  connection: local\n\n- include_tasks: tasks/extra_tasks.yml\n  when: extra_tasks_file.stat.exists\n</code></pre> If the file <code>tasks/extra_tasks_file.yml</code> is not present, Ansible skips the <code>include_tasks</code>. Even use a <code>with_items</code> loop (or any other <code>with_*</code> loop) with includes.</p>"},{"location":"DevOps/Ansible/playbook-organization/#handler-imports-and-includes","title":"Handler imports and includes","text":"<p>Handlers can be imported or included just like tasks, within a playbook's <code>handlers</code> section.</p> <p>Tip</p> <p><pre><code>handlers:\n  - import_tasks: handlers.yml\n</code></pre> This can be helpful in limiting the noise in main playbook, since handlers are usually used for things like restarting services or loading a configuration, and can distract from the playbook's primary purpose.</p>"},{"location":"DevOps/Ansible/playbook-organization/#playbook-imports","title":"Playbook imports","text":"<p>If have two playbooks - one to setup webserver(web.yml), and one to setup database server(db.yml), use the following playbokk to run both at the same time:</p> <p>case</p> <p><pre><code>- hosts: all\n  remote_user: root\n\n  tasks:\n    [...]\n\n- import_playbook: web.yml\n- import_playbook: db.yml\n</code></pre> In infrastructure, create master playbook that includes each of the individual playbooks. When want to initialize the infrastructure, make changes across entire fleet of servers, or check to make sure the configuration matches playbook definition.</p>"},{"location":"DevOps/Ansible/playbook-organization/#complete-includes-example","title":"Complete includes example","text":""},{"location":"DevOps/Ansible/playbooks/","title":"Ansible Playbooks","text":""},{"location":"DevOps/Ansible/playbooks/#power-plays","title":"Power plays","text":"<p>Playbooks are written in YAML, a simple human-readable syntax popular for defining configuration. Playbooks may be included within other playbooks, and certain metadata and options cause diffreent plays or playbooks to be run in different scnarios on different servers.</p> <p>Ad-hoc command alone make Ansible a powerful tool.</p> <p>Playbooks turn Ansible into a top-notch server provisioning and configuration management tool.</p> <p>Attractive fact that easy to convert shell script (or one-off shell comamnds) directly into Ansible plays as followingscript:</p>"},{"location":"DevOps/Ansible/playbooks/#shell-script","title":"Shell Script","text":"<p><pre><code># Install Apache.\nyum install --quiet -y httpd httpd-devel\n# Copy configuration files.\ncp httpd.conf /etc/httpd/conf/httpd.conf\ncp httpd-vhosts.conf /etc/httpd/conf/httpd-vhosts.conf\n# Start Apache and configure it to run at boot.\nservice httpd start\nchkconfig httpd on\n</code></pre> <pre><code># (From the same directory in which the shell script reside).\n$ ./shell-script.sh\n</code></pre></p>"},{"location":"DevOps/Ansible/playbooks/#ansible-playbooks_1","title":"Ansible Playbooks","text":"<p><pre><code>---\n- hosts: all\n  tasks:\n    - name: Install Apache.\n      command: yum install --quiet -y httpd httpd-devel\n    - name: Copy configuration files.\n      command: &gt;\n        cp httpd.conf /etc/httpd/conf/httpd.conf\n      command: &gt;\n        cp httpd-vhosts.conf /etc/httpd/conf/httpd-vhosts.conf\n    - name: Start Apache and configure it to run at boot.\n      command: service httpd start\n      command: chkconfig httpd on\n</code></pre> <pre><code># (From the same directory in which the playbook resides).\n$ ansible-playbook playbook.yml\n</code></pre></p>"},{"location":"DevOps/Ansible/playbooks/#revised-ansible-playbook-now-with-idempotence","title":"Revised Ansible Playbook - Now with idempotence!","text":"<p> idempotence</p> Idempoetence in Ansible <p>An idempotent task is one that can be executed multiple times without changing the system beyond the initial application, assuming no other changes were made to the system.</p> <p>Benefits:</p> <ul> <li>Consistency: Idempotent tasks ensure that your deployment will behave the same way every time it's run, which is critical for continuous integration and delivery pipelines.</li> <li>Safety: They can be rerun without fear of causing unexpected side effects or failures.</li> <li>Efficiency: Idempotent operations often check the current state before acting, which can prevent unnecessary changes and reduce execution time.</li> </ul> <pre><code>---\n- hosts: all\n  become: yes\n\n  tasks:\n    - name: Install Apache.\n      yum:\n        name:\n          - httpd\n          - httpd-devel\n        state: present\n\n    - name: Copy configuration files.\n      copy:\n        src: \"{{ item.src }}\"\n        dest: \"{{ item.dest }}\"\n        owner: root\n        group: root\n        mode: 0644\n      with_items:\n        - src: httpd.conf\n          dest: /etc/httpd/conf/httpd.conf\n        - src: httpd-vhosts.conf\n          dest: /etc/httpd/conf/httpd-vhosts.conf\n\n    - name: Make sure Apache is started now and at boot.\n      service:\n        name: httpd\n        state: started\n        enable: yes\n</code></pre>"},{"location":"DevOps/Ansible/playbooks/#running-playbooks-with-ansible-playbook","title":"Running Playbooks with <code>ansible-playbook</code>","text":""},{"location":"DevOps/Ansible/playbooks/#limiting-playbooks-to-particular-hosts-and-groups","title":"Limiting playbooks to particular hosts and groups","text":"<p>Limit the hosts on which the playbook is run: <pre><code># group `webservers` in case only even if set to `hosts:all` in playbook\n$ ansible-playbook playbook.yml --limit webservers\n</code></pre></p> <p>Limit on one particular host: <pre><code>$ ansible-playbook playbook.yml --limit xyz.example.com\n</code></pre></p> <p>To see a list of <code>hosts</code> that affected by playbook before actually run: <pre><code>$ ansible-playbook playbook.yml --list-hosts\n</code></pre></p>"},{"location":"DevOps/Ansible/playbooks/#setting-user-and-sudo-options-with-ansible-playbook","title":"Setting user and sudo options with <code>ansible-playbook</code>","text":"<p>If no <code>remote_user</code> is defined alongside the <code>hosts</code> in a playbook, Ansible assumes you'll connect as the user defined in your inventory file for a particular host, and then will fall back to your local user account name. Explicitly define a remote user to use for remote plays using the --user (-u) option.</p> <p>Define a remote user to run playbook: <pre><code>$ ansible-playbook playbook.yml --user=johndoe\n</code></pre></p> <p>Tip</p> <p>remote_user option can be set in the <code>hosts</code> file or with the upon command line options:</p> <ul> <li>-k, --ask-pass: ask for connection password</li> <li>--private-key=PRIVATE_KEY_FILE, --key-file=PRIVATE_KEY_FILE: use this file to authenticate the connection</li> <li>-u REMOTE_USER, --user=REMOTE_USER: connect as this user (default=None)</li> </ul> <p>become_user option can be set per playbook, task, or via the following connection options:</p> <ul> <li>-b, --become: run operations with become (does not imply password prompting)</li> <li>--become-method=BECOME_METHOD: privilege escalation method to use (default=sudo), valid choices: [ sudo | su | pbrun | pfexec | doas | dzdo | ksu | runas | pmrun ]</li> <li>--become-user=BECOME_USER: run operations as this user (default=root)</li> <li>-K, --ask-become-pass: ask for privilege escalation password</li> </ul> <p>Prefer setting remote_user in the <code>hosts</code> file and become_user in the <code>command line</code> options because it's easier to change hosts files then playbooks.</p> <ul> <li> <p> <code>become_user</code>:</p> <ul> <li>At the task level overrides at the playbook level overrides at the command switch level</li> <li>After initial development, don't specify at the playbook level- this will let you set it at the command option level (different servers need different ones for the same playbook) or at the task level (which, for most purposes, should only need <code>root</code>)</li> <li>If need to use <code>root</code> for a task, always use become: true and become_user: root. This will overrides whatever default become_user is set.</li> </ul> <p> Ansible</p> </li> </ul> sudo password <ul> <li>--ask-become-pass (-K) option: perform commands via sudo.</li> <li>--become (-b): explicitly force all taks in a playbook to use sudo.</li> <li>--become-user: define the sudo user for tasks run via sudo (the default is root)</li> </ul>"},{"location":"DevOps/Ansible/playbooks/#real-world-playbook-nodejs-app-server","title":"Real-World playbook: Node.js app server","text":"playbook javascript <pre><code>---\n- hosts: all\n  become: yes\n\n  vars:\n    node_apps_location: /usr/local/opt/node\n\n    # - name: Ensure firewalld is stopped (since this is a test server).\n      # service: name=firewalld state=stopped\n\n    - name: Install Node.js and npm.\n      dnf: name=npm state=present\n\n    - name: Install pm2 (to run our Node.js app).\n      npm: name=pm2 global=yes state=present\n\n    - name: Ensure Node.js app folder exists.\n      file: \"path={{ node_apps_location }} state=directory\"\n\n    - name: Copy example Node.js app to server.\n      copy: \"src=app dest={{ node_apps_location }}\"\n\n    - name: Check list of running Node.js apps.\n      command: /usr/local/bin/pm2 list\n      register: pm2_list\n      changed_when: false\n\n    - name: Start example Node.js app.\n      command: \"/usr/local/bin/pm2 start {{ node_apps_location }}/app/app.js\"\n</code></pre> <p>Note</p> <ol> <li><code>register</code> creates a new variable, <code>forever_list</code>, to be used in the next play to determine when to run the play.<code>register</code> stashes the output(stdout,stderr) of the defined command in the variable name passed to it.</li> </ol> <pre><code>var http = require(\"http\");\nvar server = http.createServer(function (req, res) {\nres.writeHead(200);\nres.end(\"Hello world!\");\n});\nserver.listen(3000);\n</code></pre>"},{"location":"DevOps/Ansible/playbooks/#be-able-to-visit-at-the-browser","title":"Be able to visit at the browser","text":"<p><code>http://HOST_NAME</code></p>"},{"location":"DevOps/Ansible/playbooks/#real-world-playbook-ubuntu-lamp-server-with-drupal","title":"Real-World playbook: Ubuntu LAMP server with Drupal","text":"playbook var template host.ini <pre><code>---\n- hosts: all\n  become: yes\n\n  vars_files:\n    - vars.yml\n\n  pre_tasks:\n    - name: Update apt cache if needed.\n      apt: update_cache=yes cache_valid_time=3600\n\n  handlers:\n    - name: restart apache\n      service: name=apache2 state=restarted\n\n  tasks:\n    - name: Get software for apt repository management.\n      apt:\n        state: present\n        name:\n          - python3-apt\n          - python3-pycurl\n\n    - name: Add ondrej repository for later versions of PHP.\n      apt_repository: repo='ppa:ondrej/php' update_cache=yes\n\n    - name: \"Install Apache, MySQL, PHP, and other dependencies.\"\n      apt:\n        state: present\n        name:\n          - acl\n          - git\n          - curl\n          - unzip\n          - sendmail\n          - apache2\n          - php8.3-common\n          - php8.3-cli\n          - php8.3-dev\n          - php8.3-gd\n          - php8.3-curl\n          - php8.3-opcache\n          - php8.3-xml\n          - php8.3-mbstring\n          - php8.3-pdo\n          - php8.3-mysql\n          - php8.3-apcu\n          - libpcre3-dev\n          - libapache2-mod-php8.3\n          - python3-mysqldb\n          - mysql-server\n\n    - name: Disable the firewall (since this is for local dev only).\n      service: name=ufw state=stopped\n\n    - name: \"Start Apache, MySQL, and PHP.\"\n      service: \"name={{ item }} state=started enabled=yes\"\n      with_items:\n        - apache2\n        - mysql\n\n    - name: Enable Apache rewrite module (required for Drupal).\n      apache2_module: name=rewrite state=present\n      notify: restart apache\n\n    - name: Add Apache virtualhost for Drupal.\n      template:\n        src: \"templates/drupal.test.conf.j2\"\n        dest: \"/etc/apache2/sites-available/{{ domain }}.test.conf\"\n        owner: root\n        group: root\n        mode: 0644\n      notify: restart apache\n\n    - name: Enable the Drupal site.\n      command: &gt;\n        a2ensite {{ domain }}.test\n        creates=/etc/apache2/sites-enabled/{{ domain }}.test.conf\n      notify: restart apache\n\n    - name: Disable the default site.\n      command: &gt;\n        a2dissite 000-default\n        removes=/etc/apache2/sites-enabled/000-default.conf\n      notify: restart apache\n\n    - name: Adjust OpCache memory setting.\n      lineinfile:\n        dest: \"/etc/php/8.3/apache2/conf.d/10-opcache.ini\"\n        regexp: \"^opcache.memory_consumption\"\n        line: \"opcache.memory_consumption = 96\"\n        state: present\n      notify: restart apache\n\n    - name: Create a MySQL database for Drupal.\n      mysql_db: \"db={{ domain }} state=present\"\n\n    - name: Create a MySQL user for Drupal.\n      mysql_user:\n        name: \"{{ domain }}\"\n        password: \"1234\"\n        priv: \"{{ domain }}.*:ALL\"\n        host: localhost\n        state: present\n\n    - name: Download Composer installer.\n      get_url:\n        url: https://getcomposer.org/installer\n        dest: /tmp/composer-installer.php\n        mode: 0755\n\n    - name: Run Composer installer.\n      command: &gt;\n        php composer-installer.php\n        chdir=/tmp\n        creates=/usr/local/bin/composer\n\n    - name: Move Composer into globally-accessible location.\n      command: &gt;\n        mv /tmp/composer.phar /usr/local/bin/composer\n        creates=/usr/local/bin/composer\n\n    - name: Ensure Drupal directory exists.\n      file:\n        path: \"{{ drupal_core_path }}\"\n        state: directory\n        owner: www-data\n        group: www-data\n\n    - name: Check if Drupal project already exists.\n      stat:\n        path: \"{{ drupal_core_path }}/composer.json\"\n      register: drupal_composer_json\n\n    - name: Create Drupal project.\n      composer:\n        command: create-project\n        arguments: drupal/recommended-project \"{{ drupal_core_path }}\"\n        working_dir: \"{{ drupal_core_path }}\"\n        no_dev: true\n      become_user: www-data\n      when: not drupal_composer_json.stat.exists\n\n    - name: Add drush to the Drupal site with Composer.\n      composer:\n        command: require\n        arguments: drush/drush*\n        working_dir: \"{{ drupal_core_path }}\"\n      become_user: www-data\n      when: not drupal_composer_json.stat.exists\n\n    - name: Install Drupal.\n      command: &gt;\n        vendor/bin/drush si -y --site-name=\"{{ drupal_site_name }}\"\n        --account-name=admin\n        --account-pass=admin\n        --db-url=mysql://{{ domain }}:1234@localhost/{{ domain }}\n        --root={{ drupal_core_path }}/web\n        chdir={{ drupal_core_path }}\n        creates={{ drupal_core_path }}/web/sites/default/settings.php\n      notify: restart apache\n      become_user: www-data\n</code></pre> <p>highlight</p> <ul> <li>pre_tasks: let tasks to run before or after the main tasks(defined in <code>tasks:</code>) or roles(defined in roles:) respectively.</li> <li>handlers: special kids of tasks to run at the end of a play by adding the <code>notify</code> option to any of the tasks in that group.</li> <li>notify: restart apache: call the handlers from a tasks.</li> <li>python3-apt and python3-pycurl: required for <code>apt_repository</code>, which allow Python to manage apt more precisely.</li> <li>apache2_module:  To remedy that situation when Apache may not have mod_rewrite enabled.</li> </ul> <p>handlers</p> <p>By default, Ansible will stop all playbook execution when a task fails, and won't notify any handlers that may need to be triggered. In some cases, this leads to unintended side effects. If want to make sure handlers always run after a tasks uses <code>notify</code> to call the handler, even in case of playbook failure, add <code>--force-handlers</code> to ansible-playbook command.</p> <p>firewall</p> <p>On production server or any server exposed to the Internet, ports <code>22,80,443</code> are must be allowed through restrictive firewall.</p> <p>apache2_module</p> <p>Apache may not have mod_rewrite enabled. To remedythat situation, use the command <code>sudo a2enmod rewrite</code>. <code>apache2_module</code> module will do the samething with idempotent.</p> <p>a2ensite</p> <pre><code>a2ensite, a2dissite - enable or disable an apache2 site / virtual host\n</code></pre> <p>notify</p> <p><pre><code>- name: Enable the Drupal site.\n  command: &gt;\n    a2ensite {{ domain }}.test\n    creates=/etc/apache2/sites-enabled/{{ domain }}.test.conf\n  notify: restart apache\n\n- name: Disable the default site.\n  command: &gt;\n    a2dissite 000-default\n    removes=/etc/apache2/sites-enabled/000-default.conf\n  notify: restart apache\n</code></pre> Two tasks(lines 76-86) enable the VirtualHost, and remove the default VirtualHost definition, which no longer need. But start the server, Apache will likely throw an error since VitrualHost that defined which does not exist (<code>there's no directory at {{ drupal_core_path }}/web yest!</code>).</p> <p>That's why using notify is important - instead of adding a task after three steps to restart Apache (which will fail the first time running the playbook).</p> <p>notify will wait until after finished all the other steps in main group of tasks (holding time to finish setting up the server), then restart Aphache.</p> <p>Install Drupal</p> <p><pre><code>vendor/bin/drush si -y --site=name=\"{{ drupal_site_name }}\"\n</code></pre> si: short for site-install</p> <pre><code>---\n# The path where Drupal will be downloaded and installed.\ndrupal_core_path: \"/var/www/drupal\"\n\n# The resulting domain will be [domain].test (with .test appended).\ndomain: \"drupal\"\n\n# Your Drupal site name.\ndrupal_site_name: \"Drupal Test\"\n</code></pre> <pre><code>&lt;VirtualHost *:80&gt;\n    ServerAdmin webmaster@localhost\n    ServerName {{ domain }}.test\n    ServerAlias www.{{ domain }}.test\n    DocumentRoot {{ drupal_core_path }}/web\n    &lt;Directory \"{{ drupal_core_path }}/web\"&gt;\n        Options FollowSymLinks Indexes\n        AllowOverride All\n    &lt;/Directory&gt;\n&lt;/VirtualHost&gt;\n</code></pre> <pre><code>[app]\nubuncld\n\n[box:children]\napp\n\n[box:vars]\nansible_user=johndoe\nansible_ssh_private_key_file=/PATH/TO/PUBKEY\n</code></pre>"},{"location":"DevOps/Ansible/playbooks/#be-able-to-visit-at-the-browser_1","title":"Be able to visit at the browser","text":"<p><code>http://HOST_NAME</code></p>"},{"location":"DevOps/Ansible/playbooks/#ansible-command-module","title":"Ansible command module","text":"<p>Ansible command module &gt; shell &gt; raw</p> <p>Ansible's <code>command</code> module is the preferred option for running commands on a host. But doesn't working on options, like <code>&lt;,&gt;,|,&amp;</code>, and local env variables like <code>$HOME</code> as well as pip output to other command, etc.</p> <p> Composer Drush Drupal</p>"},{"location":"DevOps/Ansible/playbooks/#real-world-playbook-ubuntu-server-with-solr","title":"Real-World playbook: Ubuntu server with Solr","text":"playbook <pre><code>---\n- hosts: all\n  become: true\n\n  vars_files:\n    - vars.yml\n\n  pre_tasks:\n    - name: Update apt cache if needed.\n      apt: update_cache=true cache_valid_time=3600\n\n  tasks:\n    - name: Install Java.\n      apt: name=openjdk-11-jdk state=present\n\n    - name: Download Solr.\n      get_url:\n        url: \"https://archive.apache.org/dist/lucene/solr/{{ solr_version }}/solr-{{ solr_version }}.tgz\"\n        url: \"https://archive.apache.org/dist/solr/solr/{{ solr_version }}/solr-{{ solr_version }}.tgz\"\n        dest: \"{{ download_dir }}/solr-{{ solr_version }}.tgz\"\n        checksum: \"{{ solr_checksum }}\"\n\n    - name: Expand Solr.\n      unarchive:\n        src: \"{{ download_dir }}/solr-{{ solr_version }}.tgz\"\n        dest: \"{{ download_dir }}\"\n        remote_src: true\n        creates: \"{{ download_dir }}/solr-{{ solr_version }}/README.txt\"\n\n    - name: Run Solr installation script.\n      command: &gt;\n        {{ download_dir }}/solr-{{ solr_version }}/bin/install_solr_service.sh\n        {{ download_dir }}/solr-{{ solr_version }}.tgz\n        -i /opt\n        -d /var/solr\n        -u solr\n        -s solr\n        -p 8983\n        creates={{ solr_dir }}/bin/solr\n\n    - name: Adjust setting for Solr public access\n      lineinfile:\n        path: /etc/default/solr.in.sh\n        regexp: '^SOLR_JETTY_HOST'\n        insertafter: '^#SOLR_JETTY_HOST'\n        line: SOLR_JETTY_HOST=\"0.0.0.0\"\n        state: present\n\n    - name: Restart Solr Service\n      service: name=solr state=restarted\n</code></pre> <p>Warning</p> <p>If pass a directory to the <code>dest</code> parameter, Ansible will place the file inside, but will always re-download the file on subsequent runs of the playbook (and overwrite the existing download if it has changed).</p> <p>To avoid this extra overhead, give the full path to the download file.</p>  var host.ini solr collection <pre><code>---\n# The directory into which Solr will be downloaded for setup.\ndownload_dir: /tmp\n\n# The directory inside which Solr will be installed.\nsolr_dir: /opt/solr\n\n# Solr version and download information.\n# solr_version: 8.11.3\n# solr_checksum: sha512:10f09b163bd9c31b2a8fdf889cf624114648e76881d69a4c096d473272c47b3cbe37ec9f9bd1980fd90967352c4052477065e165127eccb2c49a60c8d9860afc\nsolr_version: 9.6.1\nsolr_checksum: sha512:7e16aa71fc01f9d9b05e5514e35798104a18253a211426aa669aa3b91225d110a4fa1c78c9ec86b7e1909e2aae63696deffd877536790303cd0638eb7f1a8c63\n</code></pre> <pre><code>[app]\nubuncld\n\n[box:children]\napp\n\n[box:vars]\nansible_user=johndoe\nansible_ssh_private_key_file=/PATH/TO/PUBKEY\n</code></pre> <pre><code># Create a New Solr Collection - mycol1\nsudo su - solr -c \"/opt/solr/bin/solr create -c mycol1 -n data_driven_schema_configs\"\n</code></pre>"},{"location":"DevOps/Ansible/playbooks/#be-able-to-access-the-solr-admin-interface-on-the-browser","title":"Be able to access the Solr admin interface on the browser","text":"<p><code>http://HOST_NAME:8983</code></p>"},{"location":"DevOps/Ansible/playbooks/#check-solr-service-port","title":"Check Solr Service Port","text":"<pre><code>sudo ss -tnlp | grep 8983\n</code></pre>"},{"location":"DevOps/Container/cgroups/","title":"cgoups","text":"<p>A control group (cgroup) is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, and so on) of a collection of processes.</p> <p>Cgroups provide the following features:</p> <ul> <li> <p>Resource limits \u2013 You can configure a cgroup to limit how much of a particular resource (memory or CPU, for example) a process can use.</p> </li> <li> <p>Prioritization \u2013 You can control how much of a resource (CPU, disk, or network) a process can use compared to processes in another cgroup when there is resource contention.</p> </li> <li> <p>Accounting \u2013 Resource limits are monitored and reported at the cgroup level.</p> </li> <li> <p>Control \u2013 You can change the status (frozen, stopped, or restarted) of all processes in a cgroup with a single command.</p> </li> </ul> <p>Basically, you use cgroups to control how much of a given key resource (CPU, memory, network, and disk I/O) can be accessed or used by a process or set of processes. Cgroups are a key component of containers because there are often multiple processes running in a container that you need to control together. In a Kubernetes environment, cgroups can be used to implement resource requests and limits and corresponding QoS classes at the pod level.</p> <p>The following diagram illustrates how when you allocate a particular percentage of available system resources to a cgroup (in this case cgroup\u20111), the remaining percentage is available to other cgroups (and individual processes) on the system.</p> <p></p>"},{"location":"DevOps/Container/cgroups/#cgroup-versions","title":"Cgroup Versions","text":"<p>According to Wikipedia, the first version of cgroups was merged into the Linux kernel mainline in late 2007 or early 2008, and \u201cthe documentation of cgroups\u2011v2 first appeared in [the] Linux kernel \u2026 [in] 2016\u201d. Among the many changes in version 2, the big ones are a much simplified tree architecture, new features and interfaces in the cgroup hierarchy, and better accommodation of \u201crootless\u201d containers (with non\u2011zero UIDs).</p> <p>The new interface in v2 is for pressure stall information (PSI). It provides insight into per\u2011process memory use and allocation in a much more granular way than was previously possible (this is beyond the scope of this blog, but is a very cool topic)."},{"location":"DevOps/Container/cgroups/#conclusion","title":"Conclusion","text":"<p>Namespaces and cgroups are the building blocks for containers and modern applications. Having an understanding of how they work is important as we refactor applications to more modern architectures.</p> <p>Namespaces provide isolation of system resources, and cgroups allow for fine\u2011grained control and enforcement of limits for those resources.</p> <p>Containers are not the only way that you can use namespaces and cgroups. Namespaces and cgroup interfaces are built into the Linux kernel, which means that other applications can use them to provide separation and resource constraints.</p>"},{"location":"DevOps/Container/containerd/","title":"containerd","text":"<p>Containerd \u2013 An abstraction of kernel features that provides a relatively high level container interface. Other software projects can use this to run containers and manage container images.</p> <p>Ecosystem </p>"},{"location":"DevOps/Container/containerd/#container-runtime-interface-cri","title":"Container Runtime Interface (CRI)","text":"<p>CRI is the API that Kubernetes uses to control the different runtimes that create and manage containers.</p> <p>CRI makes it easier for Kubernetes to use different container runtimes. Instead of the Kubernetes project having to manually add support for each runtime, the CRI API describes how Kubernetes interacts with each runtime. So it\u2019s up to the runtime how to actually manage containers, as long as it obeys the CRI API. </p> <p>So if you prefer to use containerd to run your containers, you can. Or, if you prefer to use CRI-O, then you can. This is because both of these runtimes implement the CRI spec.</p> <p>If you\u2019re an end user, the implementation mostly shouldn\u2019t matter. These CRI implementations are intended to be pluggable and seamlessly changeable.</p> <p>Your choice of runtime might be important if you pay to get support (security, bug fixes etc) from a vendor. For example, Red Hat\u2019s OpenShift uses CRI-O, and offers support for it. Docker provides support for their own containerd.</p> How to check your container runtime in Kubernetes? <p>In Kubernetes architecture, the kubelet (the agent that runs on each node) is responsible for sending instructions to the container runtime to start and run containers.</p> <p>You can check which container runtime you\u2019re using by looking at the kubelet parameters on each node. There\u2019s an option --container-runtime and --container-runtime-endpoint which are used to configure which runtime to use.</p>"},{"location":"DevOps/Container/containerd/#containerd_1","title":"Containerd","text":"<p>containerd is a high-level container runtime that came from Docker, and implements the CRI spec. It pulls images from registries, manages them and then hands over to a lower-level runtime, which actually creates and runs the container processes.</p> <p>containerd was separated out of the Docker project, to make Docker more modular.  So Docker uses containerd internally itself. When you install Docker, it will also install containerd. containerd implements the Kubernetes Container Runtime Interface (CRI), via its cri plugin.</p>"},{"location":"DevOps/Container/containerd/#cri-o","title":"CRI-O","text":"<p>CRI-O is another high-level container runtime which implements the Container Runtime Interface (CRI). It\u2019s an alternative to containerd. It pulls container images from registries, manages them on disk, and launches a lower-level runtime to run container processes.</p> <p>Yes, CRI-O is another container runtime. It was born out of Red Hat, IBM, Intel, SUSE and others.</p> <p>It was specifically created from the ground up as a container runtime for Kubernetes. It provides the ability to start, stop and restart containers, just like containerd.</p>"},{"location":"DevOps/Container/containerd/#open-container-initiative-oci","title":"Open Container Initiative (OCI)","text":"<p>The OCI is a group of tech companies who maintain a specification for the container image format, and how containers should be run.</p> <p>The idea behind the OCI is that you can choose between different runtimes which conform to the spec. Each of these runtimes have different lower-level implementations.</p> <p>For example, you might have one OCI-compliant runtime for your Linux hosts, and one for your Windows hosts.</p> <p>This is the benefit of having one standard that can be implemented by many different projects. This same \u201cone standard, many implementations\u201d approach is in use everywhere, from Bluetooth devices to Java APIs.</p>"},{"location":"DevOps/Container/containerd/#runc","title":"runc","text":"<p>runc is an OCI-compatible container runtime. It implements the OCI specification and runs the container processes.</p> <p>runc is called the reference implementation of OCI.</p> What is a reference implementation? <p>A reference implementation is a piece of software that has implemented all the requirements of a specification or standard.</p> <p>It\u2019s usually the first piece of software which is developed from the specification.</p> <p>In the case of OCI, runc provides all the features expected of an OCI-compliant runtime, although anyone can implement their own OCI runtime if they like.</p> <p>runc provides all of the low-level functionality for containers, interacting with existing low-level Linux features, like namespaces and control groups. It uses these features to create and run container processes.</p> <p>A couple of alternatives to runc are:</p> <ul> <li> <p>crun a container runtime written in C (by contrast, runc is written in Go.)</p> </li> <li> <p>kata-runtime from the Katacontainers project, which implements the OCI specification as individual lightweight VMs (hardware virtualisation</p> </li> <li> <p>gVisor from Google, which creates containers that have their own kernel. It implements OCI in its runtime called runsc.</p> </li> </ul> What's the equivalent of runc on Windows? <p>runc is a tool for running containers on Linux. So that means it runs on Linux, on bare metal or inside a VM.</p> <p>On Windows, it\u2019s slightly different. The equivalent of runc is Microsoft\u2019s Host Compute Service (HCS). It includes a tool called runhcs, which itself is a fork of runc, and also implements the Open Container Initiative specification.</p>"},{"location":"DevOps/Container/namespace/","title":"Namespace","text":"<p>Namespace are a feature of the Linux kernel that partitions kernel resources such that one set of processes sees one set of resources while another set of processes sees a different set of resources.</p> <p></p>"},{"location":"DevOps/Container/namespace/#types-of-namespaces","title":"Types of Namespaces","text":"<p>Within the Linux kernel, there are different types of namespaces. Each namespace has its own unique properties:</p> <ul> <li> <p>user namespace has its own set of user IDs and group IDs for assignment to processes. In particular, this means that a process can have root privilege within its user namespace without having it in other user namespaces.</p> </li> <li> <p>process ID (PID) namespace assigns a set of PIDs to processes that are independent from the set of PIDs in other namespaces. The first process created in a new namespace has PID 1 and child processes are assigned subsequent PIDs. If a child process is created with its own PID namespace, it has PID 1 in that namespace as well as its PID in the parent process\u2019 namespace. See below for an example.</p> </li> <li> <p>network namespace has an independent network stack: its own private routing table, set of IP addresses, socket listing, connection tracking table, firewall, and other network\u2011related resources.</p> </li> <li> <p>mount namespace has an independent list of mount points seen by the processes in the namespace. This means that you can mount and unmount filesystems in a mount namespace without affecting the host filesystem.</p> </li> <li> <p>An interprocess communication (IPC) namespace has its own IPC resources, for example POSIX message queues.</p> </li> <li> <p>A UNIX Time\u2011Sharing (UTS) namespace allows a single system to appear to have different host and domain names to different processes.</p> </li> </ul>"},{"location":"DevOps/Container/namespace/#an-example-of-parent-and-child-pid-namespaces","title":"An Example of Parent and Child PID Namespaces","text":"<p>In the diagram above, there are three PID namespaces \u2013 a parent namespace and two child namespaces. Within the parent namespace, there are four processes, named PID1 through PID4. These are normal processes which can all see each other and share resources.</p> <p>The child processes with PID2 and PID3 in the parent namespace also belong to their own PID namespaces in which their PID is 1. From within a child namespace, the PID1 process cannot see anything outside. For example, PID1 in both child namespaces cannot see PID4 in the parent namespace.</p> <p>This provides isolation between (in this case) processes within different namespaces.</p>"},{"location":"DevOps/Container/namespace/#conclusion","title":"Conclusion","text":"<p>Namespaces and cgroups are the building blocks for containers and modern applications. Having an understanding of how they work is important as we refactor applications to more modern architectures.</p> <p>Namespaces provide isolation of system resources, and cgroups allow for fine\u2011grained control and enforcement of limits for those resources.</p> <p>Containers are not the only way that you can use namespaces and cgroups. Namespaces and cgroup interfaces are built into the Linux kernel, which means that other applications can use them to provide separation and resource constraints.</p>"},{"location":"DevOps/Container/nerdctl/","title":"Nerdctl","text":"<p>Nerdctl: Docker-compatible CLI for containerd</p>"},{"location":"DevOps/Container/nerdctl/#why-another-cli","title":"Why another CLI?","text":"<p>containerd already has its own CLI called ctr . However, ctr was made only for work very low-level functionality of containerd, and hence its CLI design is not friendly to humans. So we had to create another CLI with high-level functionalities and with human-friendly UI/UX. Notably, ctr lacks the equivalents of the following Docker CLI commands:</p> <ul> <li>docker run -p  <li>docker run --restart=always</li> <li>docker pull with ~/.docker/config.json and credential helper binaries such as docker-credential-ecr-login</li> <li>docker logs</li>"},{"location":"DevOps/Container/nerdctl/#getting-started-with-nerdctl","title":"Getting started with nerdctl","text":"<p>The latest binary release of nerdctl can be downloaded.</p> <p>Two types of distributions are available:</p> <ul> <li>nerdctl--linux-amd64.tar.gz : nerdctl only. Should be extracted under /usr/local/bin . <li>nerdctl-full--linux-amd64.tar.gz : nerdctl with dependencies (containerd, runc, CNI, \u2026). Should be extracted under /usr/local . <p>If you already have containerd, you should use the former one. Otherwise the latter one is the best choice.</p>"},{"location":"DevOps/Container/nerdctl/#installation","title":"Installation","text":"<p><pre><code>\u279c wget https://download.fastgit.org/containerd/nerdctl/releases/download/v0.12.1/nerdctl-0.12.1-linux-amd64.tar.gz\n\u279c mkdir -p /usr/local/containerd/bin/ &amp;&amp; tar -zxvf nerdctl-0.12.1-linux-amd64.tar.gz nerdctl &amp;&amp; mv nerdctl /usr/local/containerd/bin/\n\u279c ln -s /usr/local/containerd/bin/nerdctl /usr/local/bin/nerdctl\n</code></pre> <pre><code>\u279c nerdctl run -d -p 80:80 --name=nginx --restart=always nginx:alpine\n\u279c nerdctl exec -it nginx /bin/sh\n/ # date\nThu Aug 19 06:43:19 UTC 2021\n/ #\n</code></pre></p>"},{"location":"DevOps/Container/nerdctl/#nerdctl_1","title":"Nerdctl","text":"<ol> <li> <p>nerdctl inspect: Return low-level information on objects.</p> </li> <li> <p>nerdctl tag: Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE <pre><code>\u279c nerdctl images\nREPOSITORY    TAG                  IMAGE ID        CREATED           SIZE\nbusybox       latest               0f354ec1728d    6 minutes ago     1.3 MiB\nnginx         alpine               bead42240255    41 minutes ago    16.0 KiB\n\u279c nerdctl tag nginx:alpine harbor.k8s.local/course/nginx:alpine\n\u279c nerdctl images\nREPOSITORY                       TAG                  IMAGE ID        CREATED           SIZE\nbusybox                          latest               0f354ec1728d    7 minutes ago     1.3 MiB\nnginx                            alpine               bead42240255    41 minutes ago    16.0 KiB\nharbor.k8s.local/course/nginx    alpine               bead42240255    2 seconds ago     16.0 KiB\n</code></pre></p> </li> <li> <p>nerdctl save: Save one or more images to a tar archive (streamed to STDOUT by default) <pre><code>\u279c nerdctl save -o busybox.tar.gz busybox:latest\n\u279c ls -lh busybox.tar.gz\n-rw-r--r-- 1 root root 761K Aug 19 15:19 busybox.tar.gz\n</code></pre></p> </li> <li> <p>nerdctl load: Load an image from a tar archive or STDIN <pre><code>\u279c nerdctl load -i busybox.tar.gz\nunpacking docker.io/library/busybox:latest (sha256:0f354ec1728d9ff32edcd7d1b8bbdfc798277ad36120dc3dc683be44524c8b60)...done\n</code></pre></p> </li> <li> <p>nerdctl build: Build an image from a Dockerfile. Needs buildkitd to be running.</p> </li> </ol> <p>Tip</p> <p>BuildKit is turns a Dockerfile into a Docker image. And it doesn\u2019t just build Docker images; it can build OCI images and several other output formats.</p> <p>BuildKit is composed of the buildkitd daemon and the buildctl client. While the buildctl client is available for Linux, macOS, and Windows, the buildkitd daemon is only available for Linux currently.</p> <p>The buildkitd daemon requires the following components to be installed:</p> <ul> <li>runc or crun</li> <li>containerd (if you want to use containerd worker)</li> </ul> <p>Github</p> <pre><code>\u279c nerdctl build -t nginx:nerdctl -f Dockerfile .\nFATA[0000] `buildctl` needs to be installed and `buildkitd` needs to be running, see https://github.com/moby/buildkit: exec: \"buildctl\": executable file not found in $PATH\n</code></pre> <p> BuildKit <pre><code>\u279c wget https://github.com/moby/buildkit/releases/download/v0.9.1/buildkit-v0.9.1.linux-amd64.tar.gz\n# download.fastgit.org for speedup\n# wget https://download.fastgit.org/moby/buildkit/releases/download/v0.9.1/buildkit-v0.9.1.linux-amd64.tar.gz\n\u279c tar -zxvf buildkit-v0.9.1.linux-amd64.tar.gz -C /usr/local/containerd/\nbin/\nbin/buildctl\nbin/buildkit-qemu-aarch64\nbin/buildkit-qemu-arm\nbin/buildkit-qemu-i386\nbin/buildkit-qemu-mips64\nbin/buildkit-qemu-mips64el\nbin/buildkit-qemu-ppc64le\nbin/buildkit-qemu-riscv64\nbin/buildkit-qemu-s390x\nbin/buildkit-runc\nbin/buildkitd\n\u279c ln -s /usr/local/containerd/bin/buildkitd /usr/local/bin/buildkitd\n\u279c ln -s /usr/local/containerd/bin/buildctl /usr/local/bin/buildctl\n</code></pre></p> <p> Systemd <pre><code>\u279c cat /etc/systemd/system/buildkit.service\n[Unit]\nDescription=BuildKit\nDocumentation=https://github.com/moby/buildkit\n\n[Service]\nExecStart=/usr/local/bin/buildkitd --oci-worker=false --containerd-worker=true\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></p> <pre><code>\u279c systemctl daemon-reload\n\u279c systemctl enable buildkit --now\nCreated symlink /etc/systemd/system/multi-user.target.wants/buildkit.service \u2192 /etc/systemd/system/buildkit.service.\n\u279c systemctl status buildkit\n\u25cf buildkit.service - BuildKit\n     Loaded: loaded (/etc/systemd/system/buildkit.service; enabled; vendor preset: enabled)\n     Memory: 8.6M\n     CGroup: /system.slice/buildkit.service\n             \u2514\u25005779 /usr/local/bin/buildkitd --oci-worker=false --containerd-worker=true\n\nAug 19 16:03:10 ydzsio systemd[1]: Started BuildKit.\nAug 19 16:03:10 ydzsio buildkitd[5779]: time=\"2021-08-19T16:03:10+08:00\" level=warning msg=\"using host network as the default\"\nAug 19 16:03:10 ydzsio buildkitd[5779]: time=\"2021-08-19T16:03:10+08:00\" level=info msg=\"found worker \\\"euznuelxhxb689bc5of7pxmbc\\\", labels&gt;\nAug 19 16:03:10 ydzsio buildkitd[5779]: time=\"2021-08-19T16:03:10+08:00\" level=info msg=\"found 1 workers, default=\\\"euznuelxhxb689bc5of7pxm&gt;\nAug 19 16:03:10 ydzsio buildkitd[5779]: time=\"2021-08-19T16:03:10+08:00\" level=warning msg=\"currently, only the default worker can be used.\"\nAug 19 16:03:10 ydzsio buildkitd[5779]: time=\"2021-08-19T16:03:10+08:00\" level=info msg=\"running server on /run/buildkit/buildkitd.sock\"\n~\n</code></pre>"},{"location":"DevOps/Container/nerdctl/#nerdctl-compose","title":"Nerdctl Compose","text":"<ol> <li>nerdctl compose up: Create and start containers</li> <li>nerdctl compose build: Build or rebuild services</li> <li>nerdctl compose config: Validate and view the Compose file</li> </ol>"},{"location":"DevOps/Docker/docker/","title":"Docker","text":"<p>Docker \u2013 A developer-oriented software with a high level interface that lets you easily build and run containers from your terminal. It now uses containerd as its container runtime.</p> <p>We have to start with Docker because it\u2019s the most popular developer tool for working with containers. And for a lot of people, the name \u201cDocker\u201d itself is synonymous with the word \u201ccontainer\u201d.</p> <p>Docker kick-started this whole revolution. Docker created a very ergonomic (nice-to-use) tool for working with containers \u2013 also calleddocker.</p>"},{"location":"DevOps/Docker/docker/#container-in-docker","title":"Container in Docker","text":"<p>docker is designed to be installed on a workstation or server and comes with a bunch of tools to make it easy to build and run containers as a developer, or DevOps person.</p> <p>The docker command line tool can build container images, pull them from registries, create, start and manage containers.</p> <p>To make all of this happen, the experience you know as docker is now comprised of these projects (there are others, but these are the main ones):</p> <ul> <li> <p>docker-cli: This is the command-line utility that you interact with using docker ... commands.</p> </li> <li> <p>containerd: This is a daemon process that manages and runs containers. It pushes and pulls images, manages storage and networking, and supervises the running of containers.</p> </li> <li> <p>runc: This is the low-level container runtime (the thing that actually creates and runs containers). It includes libcontainer, a native Go-based implementation for creating containers.</p> </li> </ul> <p>In reality, when you run a container with docker, you\u2019re actually running it through the Docker daemon, containerd, and then runc.</p>"},{"location":"DevOps/Docker/docker/#dockershim-docker-in-kubernetes","title":"Dockershim: Docker in Kubernetes","text":"<p>Kubernetes includes a component called dockershim, which allows it to support Docker.</p> <p>Kubernetes prefers to run containers through any container runtime which supports its Container Runtime Interface (CRI).</p> <p>But Docker, being older than Kubernetes, doesn\u2019t implement CRI. So that\u2019s why the dockershim exists, to basically bolt Docker onto Kubernetes. Or Kubernetes onto Docker, whichever way round you prefer to think of it.</p> What is a shim? <p>In tech terms, a shim is a component in a software system, which acts as a bridge between different APIs, or as a compatibility layer. A shim is sometimes added when you want to use a third-party component, but you need a little bit of glue code to make it work.</p> <p>Going forward, Kubernetes will remove support for Docker directly, and prefer to use only container runtimes that implement its Container Runtime Interface. This probably means using containerd or CRI-O.</p> <p>But this doesn\u2019t mean that Kubernetes won\u2019t be able to run Docker-formatted containers. Both containerd and CRI-O can run Docker-formatted (actually OCI-formatted) images, they just do it without having to use the docker command or the Docker daemon.</p> <p>Phew. Hope that cleared that up.</p>"},{"location":"DevOps/Docker/docker/#docker-images","title":"Docker images","text":"<p>What many people refer to as Docker images, are actually images packaged in the Open Container Initiative (OCI) format.</p> <p>So if you pull an image from Docker Hub, or another registry, you should be able to use it with the docker command, or on a Kubernetes cluster, or with the podman utility, or any other tool that supports the OCI image format spec.</p>"},{"location":"DevOps/Kubernetes/architecture/","title":"Architecture","text":""},{"location":"DevOps/Kubernetes/architecture/#k8s-architecture","title":"K8s architecture","text":"<p>A daemon for Linux and Windows. It manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments and beyond.</p>"},{"location":"DevOps/Kubernetes/architecture/#architecture","title":"Architecture","text":""},{"location":"DevOps/Kubernetes/kubernetes/","title":"K8s","text":"<p>Kubernetes \u2013 A container orchestrator that works with multiple container runtimes, including containerd. Kubernetes is focused on deploying containers in aggregate across one or more physical \u201cnodes.\u201d Historically, Kubernetes was tied to Docker.</p>"},{"location":"DevOps/Kubernetes/kubernetes/#architecture","title":"Architecture","text":""},{"location":"DevOps/Kubernetes/kubernetes/#the-control-plane","title":"The Control plane","text":"<p>On the left, you\u2019ve got the control plane, which lives across multiple nodes.</p> <p>\u201cControl plane\u201d is a pretty broad term to describe the components that manage the cluster. It includes things like the Kubernetes API Server (which you interact with when you use kubectl), and Etcd (which is the data store that holds the desired state of the cluster).</p>"},{"location":"DevOps/Kubernetes/kubernetes/#the-nodes","title":"The nodes","text":"<p>The nodes are the worker machines which run your Pods and their containers. On each node, the Kubelet is a clever daemon that manages the containers running on that node, and reports back their status to the control plane.</p> <p>You want to learn more about Kubernetes, check out the components page on the official Kubernetes on the website.</p>"},{"location":"DevOps/Rancher/K3s/","title":"K3s","text":""},{"location":"DevOps/Rancher/K3s/#abstract","title":"Abstract","text":"<p>Lightweight Kubernetes. Production ready, easy to install, half the memory, all in a binary less than 100 MB. Great for:</p> <ul> <li> Edge</li> <li> IoT</li> <li> CI</li> <li> Development</li> <li> ARM</li> <li> Embedding k8s</li> <li> Situations where a PhD in k8s clusterology is infeasible</li> </ul>"},{"location":"DevOps/Rancher/K3s/#architecture","title":"Architecture","text":"<p>How it works </p>"},{"location":"DevOps/Rancher/K3s/#alpine-linux","title":"alpine Linux:","text":"<p>mini requirement can be used by virtual systems which optimized by slimmed down the kernel.</p> <p>Warning</p> <p>In order to set up Alpine Linux, you have to go through the following preparation: Update /etc/update-extlinux.conf by adding:</p> <p><pre><code>default_kernel_opts=\"...  cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory\"\n</code></pre> <pre><code>update-extlinux\nreboot\n</code></pre></p> <pre><code>curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=\"644\" sh -s\n</code></pre>"},{"location":"DevOps/Terraform/KVM/","title":"K3s cluster on Ubuntu using terraform and libvirt","text":"<p>K3s is a lightweight Kubernetes deployment by Rancher that is fully compliant, yet also compact enough to run on development boxes and edge devices.</p> <p>In this article, I will show you how to deploy a  three-node K3s cluster on Ubuntu nodes that are created using Terraform and a local KVM libvirt provider.</p> <p></p>"},{"location":"DevOps/Terraform/KVM/#creating-node-vms","title":"Creating node VMs","text":"<p>We will deploy this K3s cluster on three independent guests running Ubuntu.</p> <p>These Ubuntu VMs could actually be created using any hypervisor or  hyperscaler, but for this article we will use Terraform and the local <code>KVM libvirt provider</code> to create guests named: <code>k3s-1, k3s-2, k3s-3</code>.   We will place them on the standard KVM default network, <code>192.168.122.0/24</code>.</p> <p>Install Terraform, its libvirt provider, and KVM as described in a previous article.  Then use my github project to create the three Ubuntu guest OS.</p> <pre><code># required packages\nsudo apt install make git curl -y\n\n# github project with terraform to create guest OS\ngit clone https://github.com/fabianlee/k3s-cluster-kvm.git\ncd k3s-cluster-kvm\n\n# run terraform init and apply\ncd tf-libvirt\n\n# make ssh keypair for login as 'ubuntu'\nssh-keygen -t rsa -b 4096 -f id_rsa -C tf-libvirt -N \"\" -q\n\n# initialize terraform and its plugins\nterraform init\n\n# do creation\nterraform apply -auto-approve\n</code></pre> <p>The KVM guests can now be listed using virsh.  I have embedded the IP address in the libvirt domain name to make the address obvious.</p> <pre><code># should show three running k3s VMs\n$ virsh list\nId Name State\n--------------------------------------------\n...\n10 k3s-1-192.168.122.213 running\n11 k3s-2-192.168.122.214 running\n12 k3s-1-192.168.122.215 running\n</code></pre> <p>cloud-init has been used to give the \u2018ubuntu\u2019 user an ssh keypair for login, which allows us to validate the login for each host using the command below.</p> <pre><code># accept key as known_hosts\nfor octet in $(seq 213 215); do ssh-keyscan -H 192.168.122.$octet &gt;&gt; ~/.ssh/known_hosts; done\n\n# test ssh into remote host\nfor octet in $(seq 213 215); do ssh -i id_rsa ubuntu@192.168.122.$octet \"hostname -f; uptime\"; done\n</code></pre>"},{"location":"DevOps/Terraform/KVM/#updating-hosts-files-for-k3s-cluster","title":"Updating hosts files for K3s cluster","text":"<p>To allow all the VMs participating in the K3s cluster to see each other, add entries to <code>/etc/hosts</code>.  Do this either manually like below:</p> <pre><code># login to each host manually\n# ssh -i id_rsa ubuntu@192.168.122.[213-215]\n\n# add these entries to each /etc/hosts\nk3s-1 192.168.122.213\nk3s-2 192.168.122.214\nk3s-3 192.168.122.215\n</code></pre> <p>Or you can use this single command.</p> <pre><code>for octet in $(seq 213 215); do ssh -i id_rsa ubuntu@192.168.122.$octet 'echo -e \"k3s-1 192.168.122.213\\nk3s-2 192.168.122.214\\nk3s-3 192.168.122.215\" | sudo tee -a /etc/hosts'; done\n</code></pre>"},{"location":"DevOps/Terraform/KVM/#install-master-k3s-node","title":"Install master K3s node","text":"<p>Login manually to the K3s master node (.213) and run these commands.</p> <pre><code># login to master\n$ ssh -i id_rsa ubuntu@192.168.122.213\n\n# install\n$ sudo curl -sfL https://get.k3s.io | sh -\n\n[INFO]  Finding release for channel stable\n[INFO]  Using v1.21.4+k3s1 as release\n[INFO]  Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.21.4+k3s1/sha256sum-amd64.txt\n[INFO]  Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.21.4+k3s1/k3s\n[INFO]  Verifying binary download\n[INFO]  Installing k3s to /usr/local/bin/k3s\n[INFO]  Creating /usr/local/bin/kubectl symlink to k3s\n[INFO]  Creating /usr/local/bin/crictl symlink to k3s\n[INFO]  Creating /usr/local/bin/ctr symlink to k3s\n[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh\n[INFO]  Creating uninstall script /usr/local/bin/k3s-uninstall.sh\n[INFO]  env: Creating environment file /etc/systemd/system/k3s.service.env\n[INFO]  systemd: Creating service file /etc/systemd/system/k3s.service\n[INFO]  systemd: Enabling k3s unit\nCreated symlink /etc/systemd/system/multi-user.target.wants/k3s.service \u2192 /etc/systemd/system/k3s.service.\n[INFO]  systemd: Starting k3s\n</code></pre> <p>And then do a validation to ensure the service is started and kubectl reports back a single master node.</p> <pre><code># validate service, status should say 'active (running)'\n$ sudo systemctl status k3s --no-pager | head -n5\nk3s.service - Lightweight Kubernetes\n     Loaded: loaded (/etc/systemd/system/k3s.service; enabled; vendor preset: enabled)\n     Active: active (running) since Sun 2021-09-12 20:39:32 UTC; 5min ago\n       Docs: https://k3s.io\n    Process: 3050 ExecStartPre=/bin/sh -xc ! /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service (code=exited, status=0/SUCCESS)\n\n# validate that kubectl returns single master\n$ sudo kubectl get nodes -o wide\nNAME    STATUS   ROLES                  AGE    VERSION        INTERNAL-IP       EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\nk3s-1   Ready    control-plane,master   7m3s   v1.21.4+k3s1   192.168.122.213           Ubuntu 20.04.3 LTS   5.4.0-84-generic   containerd://1.4.9-k3s1\n</code></pre> <p>The master node has a file <code>\u201c/var/lib/rancher/k3s/server/node-token\u201d</code> which we will use in the next section to add nodes to the cluster.</p>"},{"location":"DevOps/Terraform/KVM/#join-k3s-nodes-to-the-cluster","title":"Join K3s nodes to the cluster","text":"<p>In order to add K3s nodes to the master, we need to use the key found on the master at {++ \u201c/var/lib/rancher/k3s/server/node-token\u201d++}.  So let\u2019s capture that value.</p> <pre><code>k3s_master=192.168.122.213\n\n# grab node token from master\nnode_token=$(ssh -i id_rsa ubuntu@$k3s_master \"sudo cat /var/lib/rancher/k3s/server/node-token\")\n\necho node_token from master is $node_token\n</code></pre> <p>To create a cluster run the get.k3s.io script just like the master, but this time add parameters for the master node IP and node token.</p> <pre><code># install on k3s-2, join cluster\n$ ssh -i id_rsa ubuntu@192.168.122.214 \"sudo curl -sfL http://get.k3s.io | K3S_URL=https://${k3s_master}:6443 K3S_TOKEN=${node_token} sh -\"\n[INFO]  Finding release for channel stable\n[INFO]  Using v1.21.4+k3s1 as release\n[INFO]  Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.21.4+k3s1/sha256sum-amd64.txt\n[INFO]  Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.21.4+k3s1/k3s\n[INFO]  Verifying binary download\n[INFO]  Installing k3s to /usr/local/bin/k3s\n[INFO]  Creating /usr/local/bin/kubectl symlink to k3s\n[INFO]  Creating /usr/local/bin/crictl symlink to k3s\n[INFO]  Creating /usr/local/bin/ctr symlink to k3s\n[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh\n[INFO]  Creating uninstall script /usr/local/bin/k3s-agent-uninstall.sh\n[INFO]  env: Creating environment file /etc/systemd/system/k3s-agent.service.env\n[INFO]  systemd: Creating service file /etc/systemd/system/k3s-agent.service\n[INFO]  systemd: Enabling k3s-agent unit\nCreated symlink /etc/systemd/system/multi-user.target.wants/k3s-agent.service \u2192 /etc/systemd/system/k3s-agent.service.\n[INFO]  systemd: Starting k3s-agent\n</code></pre> <p>In the same way, join the k3s-3 node.</p> <pre><code># install on k3s-3, join cluster\n$ ssh -i id_rsa ubuntu@192.168.122.214 \"sudo curl -sfL http://get.k3s.io | K3S_URL=https://${k3s_master}:6443 K3S_TOKEN=${node_token} sh -\"\n</code></pre>"},{"location":"DevOps/Terraform/KVM/#validate-kubernetes-cluster","title":"Validate Kubernetes Cluster","text":"<p>A call to kubectl should now show all 3 nodes participating in the Kubernetes cluster.</p> <pre><code>$ ssh -i id_rsa ubuntu@192.168.122.213 \"sudo kubectl get nodes\"\n\nNAME    STATUS   ROLES                  AGE    VERSION\nk3s-2   Ready                     10m    v1.21.4+k3s1\nk3s-3   Ready                     4m7s   v1.21.4+k3s1\nk3s-1   Ready    control-plane,master   32m    v1.21.4+k3s1\n</code></pre>"},{"location":"DevOps/Terraform/KVM/#validate-app-deployment-to-cluster","title":"Validate app deployment to cluster","text":"<p>As a quick test of the Kubernetes cluster, create a test deployment of the whoami app which is exposed using the default <code>K3s Traefik ingress</code>.</p> <pre><code># copy deployment manifest to master\nscp -i id_rsa whoami_traefik_example.yml ubuntu@192.168.122.213:.\n\n# login to master\nssh -i id_rsa ubuntu@192.168.122.213\n\n# apply manifest\n$ sudo kubectl apply -f whoami_traefik_example.yml\ndeployment.apps/whoami-ds created\nservice/whoami-service created\ningressroute.traefik.containo.us/whoami-ingressroute created\n\n# exposed services should now have 'whoami-service'\n$ sudo kubectl get services\nNAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE\nkubernetes       ClusterIP   10.43.0.1              443/TCP   49m\nwhoami-service   ClusterIP   10.43.210.80           80/TCP    40s\n\n# put ClusterIP of whoami service into variable\ncluster_ip=$(sudo kubectl get service/whoami-service -o=jsonpath=\"{.spec.clusterIP}\")\n\n# curl to service ClusterIP\n$ curl http://$cluster_ip/whoami\n\nHostname: whoami-ds-78447d957f-jnfrw\nIP: 127.0.0.1\nIP: ::1\nIP: 10.42.1.3\nIP: fe80::18:13ff:febc:c0fb\nRemoteAddr: 10.42.0.0:18326\nGET /whoami HTTP/1.1\nHost: 10.43.210.80\nUser-Agent: curl/7.68.0\nAccept: */*\n\n# also exposed by Traefik\n$ sudo kubectl get service traefik -n kube-system\nNAME      TYPE           CLUSTER-IP      EXTERNAL-IP                                       PORT(S)                      AGE\ntraefik   LoadBalancer   10.43.214.196   192.168.122.213,192.168.122.214,192.168.122.215   80:30180/TCP,443:31528/TCP   59m\n\n# available here from master node\n$ curl http://localhost/whoami\n$ curl http://k3s-1.local/whoami\n\n# leave ssh session\nexit\n</code></pre> <p>And Traefik also exposes this service externally.</p> <pre><code># available also outside cluster\n$ curl http://192.168.122.213/whoami\nHostname: whoami-ds-78447d957f-jnfrw\nIP: 127.0.0.1\nIP: ::1\nIP: 10.42.1.3\nIP: fe80::18:13ff:febc:c0fb\nRemoteAddr: 10.42.0.8:35890\nGET /whoami HTTP/1.1\nHost: 192.168.122.213\nUser-Agent: curl/7.68.0\nAccept: */*\nAccept-Encoding: gzip\nX-Forwarded-For: 10.42.0.1\nX-Forwarded-Host: 192.168.122.213\nX-Forwarded-Port: 80\nX-Forwarded-Proto: http\nX-Forwarded-Server: traefik-97b44b794-k6bbx\nX-Real-Ip: 10.42.0.1\n\n# available at each cluster node\n$ curl http://192.168.122.214/whoami\n$ curl http://192.168.122.215/whoami\n</code></pre>"},{"location":"DevOps/Terraform/KVM/#view-traefik-dashboard","title":"View traefik dashboard","text":"<pre><code># http://127.0.0.1:9000/dashboard\nkubectl port-forward -n kube-system pod/traefik-97b44b794-4mpx5 9000:9000\n</code></pre> <p>k3s config.toml for configuration of containerd</p> <pre><code>sudo vi /var/lib/rancher/k3s/agent/etc/containerd/config.toml\n</code></pre>"},{"location":"DevOps/Terraform/Kubernetes/","title":"Deploying Kubernetes VMs in Proxmox with Terraform","text":""},{"location":"DevOps/Terraform/Kubernetes/#kubernetes-proxmox-terraform-template","title":"Kubernetes Proxmox Terraform Template","text":"<p>Without further ado, below is the template I used to create my virtual machines. The main LAN network is 10.98.1.0/24, and the Kube internal network (on its own bridge) is 10.17.0.0/24.</p> <p>This template creates a Kube server, two agents, and a storage server.</p> <pre><code>terraform {\n  required_providers {\n    proxmox = {\n      source = \"telmate/proxmox\"\n      version = \"2.7.4\"\n    }\n  }\n}\n\nprovider \"proxmox\" {\n  pm_api_url = \"https://prox-1u.home.fluffnet.net:8006/api2/json\"\n  pm_api_token_id = [secret]\n  pm_api_token_secret = [secret]\n  pm_tls_insecure = true\n}\n\nresource \"proxmox_vm_qemu\" \"kube-server\" {\n  count = 1\n  name = \"kube-server-0${count.index + 1}\"\n  target_node = \"prox-1u\"\n\n  clone = \"ubuntu-2004-cloudinit-template\"\n\n  agent = 1\n  os_type = \"cloud-init\"\n  cores = 2\n  sockets = 1\n  cpu = \"host\"\n  memory = 4096\n  scsihw = \"virtio-scsi-pci\"\n  bootdisk = \"scsi0\"\n\n  disk {\n    slot = 0\n    size = \"10G\"\n    type = \"scsi\"\n    storage = \"local-zfs\"\n    #storage_type = \"zfspool\"\n    iothread = 1\n  }\n\n  network {\n    model = \"virtio\"\n    bridge = \"vmbr0\"\n  }\n\n  network {\n    model = \"virtio\"\n    bridge = \"vmbr17\"\n  }\n\n  lifecycle {\n    ignore_changes = [\n      network,\n    ]\n  }\n\n  ipconfig0 = \"ip=10.98.1.4${count.index + 1}/24,gw=10.98.1.1\"\n  ipconfig1 = \"ip=10.17.0.4${count.index + 1}/24\"\n  sshkeys = &lt;&lt;EOF\n  ${var.ssh_key}\n  EOF\n}\n\nresource \"proxmox_vm_qemu\" \"kube-agent\" {\n  count = 2\n  name = \"kube-agent-0${count.index + 1}\"\n  target_node = \"prox-1u\"\n\n  clone = \"ubuntu-2004-cloudinit-template\"\n\n  agent = 1\n  os_type = \"cloud-init\"\n  cores = 2\n  sockets = 1\n  cpu = \"host\"\n  memory = 4096\n  scsihw = \"virtio-scsi-pci\"\n  bootdisk = \"scsi0\"\n\n  disk {\n    slot = 0\n    size = \"10G\"\n    type = \"scsi\"\n    storage = \"local-zfs\"\n    #storage_type = \"zfspool\"\n    iothread = 1\n  }\n\n  network {\n    model = \"virtio\"\n    bridge = \"vmbr0\"\n  }\n\n  network {\n    model = \"virtio\"\n    bridge = \"vmbr17\"\n  }\n\n  lifecycle {\n    ignore_changes = [\n      network,\n    ]\n  }\n\n  ipconfig0 = \"ip=10.98.1.5${count.index + 1}/24,gw=10.98.1.1\"\n  ipconfig1 = \"ip=10.17.0.5${count.index + 1}/24\"\n  sshkeys = &lt;&lt;EOF\n  ${var.ssh_key}\n  EOF\n}\n\nresource \"proxmox_vm_qemu\" \"kube-storage\" {\n  count = 1\n  name = \"kube-storage-0${count.index + 1}\"\n  target_node = \"prox-1u\"\n\n  clone = \"ubuntu-2004-cloudinit-template\"\n\n  agent = 1\n  os_type = \"cloud-init\"\n  cores = 2\n  sockets = 1\n  cpu = \"host\"\n  memory = 4096\n  scsihw = \"virtio-scsi-pci\"\n  bootdisk = \"scsi0\"\n\n  disk {\n    slot = 0\n    size = \"20G\"\n    type = \"scsi\"\n    storage = \"local-zfs\"\n    #storage_type = \"zfspool\"\n    iothread = 1\n  }\n\n  network {\n    model = \"virtio\"\n    bridge = \"vmbr0\"\n  }\n\n  network {\n    model = \"virtio\"\n    bridge = \"vmbr17\"\n  }\n\n  lifecycle {\n    ignore_changes = [\n      network,\n    ]\n  }\n\n  ipconfig0 = \"ip=10.98.1.6${count.index + 1}/24,gw=10.98.1.1\"\n  ipconfig1 = \"ip=10.17.0.6${count.index + 1}/24\"\n  sshkeys = &lt;&lt;EOF\n  ${var.ssh_key}\n  EOF\n}\n</code></pre> <p>After running Terraform plan and apply, you should have 4 new VMs in your Proxmox cluster:</p> <p></p>"},{"location":"DevOps/Terraform/Kubernetes/#conclusion","title":"Conclusion","text":"<p>You now have 4 VMs ready for Kubernetes installation. The next post will show how to install Kubernetes with Ansible.</p>"},{"location":"DevOps/Terraform/Proxmox/","title":"How to deploy VMs in Proxmox with Terraform","text":""},{"location":"DevOps/Terraform/Proxmox/#background","title":"Background","text":"<p>I\u2019d like to learn Kubernetes and DevOps. A Kubernetes cluster requires at least 3 VMs/bare metal machines. In my last post, I wrote about how to create a Ubuntu cloud-init template for Proxmox. In this post, we\u2019ll take that template and use it to deploy a couple VMs via automation using Terraform. If you don\u2019t have a template, you need one before proceeding.</p>"},{"location":"DevOps/Terraform/Proxmox/#overview","title":"Overview","text":"<ol> <li>Install Terraform</li> <li>Determine authentication method for Terraform to interact with Proxmox (user/pass vs API keys)</li> <li>Terraform basic initialization and provider installation</li> <li>Develop Terraform plan</li> <li>Terraform plan</li> <li>Run Terraform plan and watch the VMs appear!</li> </ol>"},{"location":"DevOps/Terraform/Proxmox/#install-terraform","title":"Install Terraform","text":"<pre><code>curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -\nsudo apt-add-repository \"deb [arch=$(dpkg --print-architecture)] https://apt.releases.hashicorp.com $(lsb_release -cs) main\"\nsudo apt update\nsudo apt install terraform\n</code></pre>"},{"location":"DevOps/Terraform/Proxmox/#determine-authentication-method-use-api-keys","title":"Determine Authentication Method (use API keys)","text":"<p>You have two options here:</p> <ul> <li>Username/password \u2013 you can use the existing default root user and root password here to make things easy\u2026 or</li> <li>API keys \u2013 this involves setting up a new user, giving that new user the required permissions, and then setting up API keys so that user doesn\u2019t have to type in a password to perform actions</li> </ul> <p>I went with the API key method since it is not desirable to have your root password sitting in Terraform files (even as an environment variable isn\u2019t a great idea). I didn\u2019t really know what I was doing and I basically gave the new user full admin permissions anyways. Should I lock it down? Surely. Do I know what the minimum required permissions are to do so? Nope. If someone in the comments or on Reddit could enlighten me, I\u2019d really appreciate it!</p> <p>So we need to create a new user. We\u2019ll name it \u2018blog_example\u2019. To add a new user go to Datacenter in the left tab, then Permissions -&gt; Users -&gt; Click add, name the user and click add.</p> <p> <code>Adding \u2018blog_example\u2019 user to my proxmox datacenter (cluster)</code></p> <p>Next, we need to add API tokens. Click API tokens below users in the permissions category and click add. Select the user you just created and give the token an ID, and uncheck privilege separation (which means we want the token to have the same permissions as the user):</p> <p> <code>Adding a new API token for user \u2018blog_example\u2019</code></p> <p>When you click Add it will show you the key. Save this key. It will never be displayed again!</p> <p> <code>Super secret API key secret</code></p> <p>Next we need to add a role to the new user. Permissions -&gt; Add -&gt; Path = \u2018/\u2019, User is the one you just made, role = \u2018PVEVMAdmin\u2019. This gives the user (and associated API token!) rights to all nodes (the / for path) to do VMAdmin activities:</p> <p></p> <p>You also need to add permissions to the storage used by the VMs you want to deploy (both from and to), for me this is /storage/local-zfs (might be /storage/local-lvm for you). Add that too in the path section. Use Admin for the role here because the user also needs the ability to allocate space in the datastore (you could use PVEVMAdmin + a datastore role but I haven\u2019t dove into which one yet):</p> <p></p> <p>At this point we are done with the permissions:</p> <p></p>"},{"location":"DevOps/Terraform/Proxmox/#terraform-basic-information-and-provider-installation","title":"Terraform basic information and provider installation","text":"<p>Terraform has three main stages: init, plan, and apply. We will start with describing the plans, which can be thought of a a type of configuration file for what you want to do. Plans are files stored in directories. Make a new directory (terraform-blog), and create two files: main.tf and vars.tf:</p> <pre><code>cd ~\nmkdir terraform-blog &amp;&amp; cd terraform-blog\ntouch main.tf vars.tf\n</code></pre> <p>The two files are hopefully reasonably named. The main content will be in main.tf and we will put a few variables in vars.tf. Everything could go in main.tf but it is a good practice to start splitting things out early. I actually don\u2019t have as much in vars.tf as I should but we all gotta start somewhere</p> <p>Ok so in main.tf let\u2019s add the bare minimum. We need to tell Terraform to use a provider, which is the term they use for the connector to the entity Terraform will be interacting with. Since we are using Proxmox, we need to use a Proxmox provider. This is actually super easy \u2013 we just need to specify the name and version and Terraform goes out and grabs it from github and installs it. I used the Telmate Proxmox provider.</p> <p>main.tf:</p> <pre><code>terraform {\n  required_providers {\n    proxmox = {\n      source = \"telmate/proxmox\"\n      version = \"2.7.4\"\n    }\n  }\n}\n</code></pre> <p>Save the file. Now we\u2019ll initialize Terraform with our barebones plan (terraform init), which will force it to go out and grab the provider. If all goes well, we will be informed that the provider was installed and that Terraform has been initialized. Terraform is also really nice in that it tells you the next step towards the bottom of the output (\u201ctry running \u2018terraform plan\u2019 next\u201d).</p> <pre><code>austin@EARTH:/mnt/c/Users/Austin/terraform-blog$ terraform init\n\nInitializing the backend...\n\nInitializing provider plugins...\n- Finding telmate/proxmox versions matching \"2.7.4\"...\n- Installing telmate/proxmox v2.7.4...\n- Installed telmate/proxmox v2.7.4 (self-signed, key ID A9EBBE091B35AFCE)\n\nPartner and community providers are signed by their developers.\nIf you'd like to know more about provider signing, you can read about it here:\nhttps://www.terraform.io/docs/cli/plugins/signing.html\n\nTerraform has created a lock file .terraform.lock.hcl to record the provider\nselections it made above. Include this file in your version control repository\nso that Terraform can guarantee to make the same selections by default when\nyou run \"terraform init\" in the future.\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n</code></pre>"},{"location":"DevOps/Terraform/Proxmox/#develop-terraform-plan","title":"Develop Terraform plan","text":"<p>Alright with the provider installed, it is time to use it to deploy a VM. We will use the template as below. Alter your main.tf file to be the following. I break it down inside the file with comments</p> <pre><code>terraform {\n  required_providers {\n    proxmox = {\n      source = \"telmate/proxmox\"\n      version = \"2.7.4\"\n    }\n  }\n}\n\nprovider \"proxmox\" {\n  # url is the hostname (FQDN if you have one) for the proxmox host you'd like to connect to to issue the commands. my proxmox host is 'prox-1u'. Add /api2/json at the end for the API\n  pm_api_url = \"https://prox-1u:8006/api2/json\"\n\n  # api token id is in the form of: &lt;username&gt;@pam!&lt;tokenId&gt;\n  pm_api_token_id = \"blog_example@pam!new_token_id\"\n\n  # this is the full secret wrapped in quotes. don't worry, I've already deleted this from my proxmox cluster by the time you read this post\n  pm_api_token_secret = \"9ec8e608-d834-4ce5-91d2-15dd59f9a8c1\"\n\n  # leave tls_insecure set to true unless you have your proxmox SSL certificate situation fully sorted out (if you do, you will know)\n  pm_tls_insecure = true\n}\n\n# resource is formatted to be \"[type]\" \"[entity_name]\" so in this case\n# we are looking to create a proxmox_vm_qemu entity named test_server\nresource \"proxmox_vm_qemu\" \"test_server\" {\n  count = 1 # just want 1 for now, set to 0 and apply to destroy VM\n  name = \"test-vm-${count.index + 1}\" #count.index starts at 0, so + 1 means this VM will be named test-vm-1 in proxmox\n\n  # this now reaches out to the vars file. I could've also used this var above in the pm_api_url setting but wanted to spell it out up there. target_node is different than api_url. target_node is which node hosts the template and thus also which node will host the new VM. it can be different than the host you use to communicate with the API. the variable contains the contents \"prox-1u\"\n  target_node = var.proxmox_host\n\n  # another variable with contents \"ubuntu-2004-cloudinit-template\"\n  clone = var.template_name\n\n  # basic VM settings here. agent refers to guest agent\n  agent = 1\n  os_type = \"cloud-init\"\n  cores = 2\n  sockets = 1\n  cpu = \"host\"\n  memory = 2048\n  scsihw = \"virtio-scsi-pci\"\n  bootdisk = \"scsi0\"\n\n  disk {\n    slot = 0\n    # set disk size here. leave it small for testing because expanding the disk takes time.\n    size = \"10G\"\n    type = \"scsi\"\n    storage = \"local-zfs\"\n    iothread = 1\n  }\n\n  # if you want two NICs, just copy this whole network section and duplicate it\n  network {\n    model = \"virtio\"\n    bridge = \"vmbr0\"\n  }\n\n  # not sure exactly what this is for. presumably something about MAC addresses and ignore network changes during the life of the VM\n  lifecycle {\n    ignore_changes = [\n      network,\n    ]\n  }\n\n  # the ${count.index + 1} thing appends text to the end of the ip address\n  # in this case, since we are only adding a single VM, the IP will\n  # be 10.98.1.91 since count.index starts at 0. this is how you can create\n  # multiple VMs and have an IP assigned to each (.91, .92, .93, etc.)\n\n  ipconfig0 = \"ip=10.98.1.9${count.index + 1}/24,gw=10.98.1.1\"\n\n  # sshkeys set using variables. the variable contains the text of the key.\n  sshkeys = &lt;&lt;EOF\n  ${var.ssh_key}\n  EOF\n}\n</code></pre> <p>There is a good amount going on in here. Hopefully the embedded comments explain everything. If not, let me know in the comments or on Reddit (u/Nerdy-Austin).</p> <p>Now for the vars.tf file. This is a bit easier to understand. Just declare a variable, give it a name, and a default value. That\u2019s all I know at this point and it works.</p> <pre><code>variable \"ssh_key\" {\n  default = \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDcwZAOfqf6E6p8IkrurF2vR3NccPbMlXFPaFe2+Eh/8QnQCJVTL6PKduXjXynuLziC9cubXIDzQA+4OpFYUV2u0fAkXLOXRIwgEmOrnsGAqJTqIsMC3XwGRhR9M84c4XPAX5sYpOsvZX/qwFE95GAdExCUkS3H39rpmSCnZG9AY4nPsVRlIIDP+/6YSy9KWp2YVYe5bDaMKRtwKSq3EOUhl3Mm8Ykzd35Z0Cysgm2hR2poN+EB7GD67fyi+6ohpdJHVhinHi7cQI4DUp+37nVZG4ofYFL9yRdULlHcFa9MocESvFVlVW0FCvwFKXDty6askpg9yf4FnM0OSbhgqXzD austin@EARTH\"\n}\n\nvariable \"proxmox_host\" {\n    default = \"prox-1u\"\n}\n\nvariable \"template_name\" {\n    default = \"ubuntu-2004-cloudinit-template\"\n}\n</code></pre>"},{"location":"DevOps/Terraform/Proxmox/#terraform-plan-official-term-for-what-will-terraform-do-next","title":"Terraform plan (official term for \u201cwhat will Terraform do next\u201d)","text":"<p>Now with the .tf files completed, we can run the plan (terraform plan). We defined a count=1 resource, so we would expect Terraform to create a single VM. Let\u2019s have Terraform run through the plan and tell us what it intends to do. It tells us a lot.</p> <pre><code>austin@EARTH:/mnt/c/Users/Austin/terraform-blog$ terraform plan\n\nTerraform used the selected providers to generate the following execution plan. Resource actions\nare indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # proxmox_vm_qemu.test_server[0] will be created\n  + resource \"proxmox_vm_qemu\" \"test_server\" {\n      + additional_wait           = 15\n      + agent                     = 1\n      + balloon                   = 0\n      + bios                      = \"seabios\"\n      + boot                      = \"cdn\"\n      + bootdisk                  = \"scsi0\"\n      + clone                     = \"ubuntu-2004-cloudinit-template\"\n      + clone_wait                = 15\n      + cores                     = 2\n      + cpu                       = \"host\"\n      + default_ipv4_address      = (known after apply)\n      + define_connection_info    = true\n      + force_create              = false\n      + full_clone                = true\n      + guest_agent_ready_timeout = 600\n      + hotplug                   = \"network,disk,usb\"\n      + id                        = (known after apply)\n      + ipconfig0                 = \"ip=10.98.1.91/24,gw=10.98.1.1\"\n      + kvm                       = true\n      + memory                    = 2048\n      + name                      = \"test-vm-1\"\n      + nameserver                = (known after apply)\n      + numa                      = false\n      + onboot                    = true\n      + os_type                   = \"cloud-init\"\n      + preprovision              = true\n      + reboot_required           = (known after apply)\n      + scsihw                    = \"virtio-scsi-pci\"\n      + searchdomain              = (known after apply)\n      + sockets                   = 1\n      + ssh_host                  = (known after apply)\n      + ssh_port                  = (known after apply)\n      + sshkeys                   = &lt;&lt;-EOT\n              ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDcwZAOfqf6E6p8IkrurF2vR3NccPbMlXFPaFe2+Eh/8QnQCJVTL6PKduXjXynuLziC9cubXIDzQA+4OpFYUV2u0fAkXLOXRIwgEmOrnsGAqJTqIsMC3XwGRhR9M84c4XPAX5sYpOsvZX/qwFE95GAdExCUkS3H39rpmSCnZG9AY4nPsVRlIIDP+/6YSy9KWp2YVYe5bDaMKRtwKSq3EOUhl3Mm8Ykzd35Z0Cysgm2hR2poN+EB7GD67fyi+6ohpdJHVhinHi7cQI4DUp+37nVZG4ofYFL9yRdULlHcFa9MocESvFVlVW0FCvwFKXDty6askpg9yf4FnM0OSbhgqXzD austin@EARTH\n        EOT\n      + target_node               = \"prox-1u\"\n      + unused_disk               = (known after apply)\n      + vcpus                     = 0\n      + vlan                      = -1\n      + vmid                      = (known after apply)\n\n      + disk {\n          + backup       = 0\n          + cache        = \"none\"\n          + file         = (known after apply)\n          + format       = (known after apply)\n          + iothread     = 1\n          + mbps         = 0\n          + mbps_rd      = 0\n          + mbps_rd_max  = 0\n          + mbps_wr      = 0\n          + mbps_wr_max  = 0\n          + media        = (known after apply)\n          + replicate    = 0\n          + size         = \"10G\"\n          + slot         = 0\n          + ssd          = 0\n          + storage      = \"local-zfs\"\n          + storage_type = (known after apply)\n          + type         = \"scsi\"\n          + volume       = (known after apply)\n        }\n\n      + network {\n          + bridge    = \"vmbr0\"\n          + firewall  = false\n          + link_down = false\n          + macaddr   = (known after apply)\n          + model     = \"virtio\"\n          + queues    = (known after apply)\n          + rate      = (known after apply)\n          + tag       = -1\n        }\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nNote: You didn't use the -out option to save this plan, so Terraform can't guarantee to take\nexactly these actions if you run \"terraform apply\" now.\n</code></pre> <p>You can see the output of the planning phase of Terraform. It is telling us it will create proxmox_vm_qemu.test_server[0] with a list of parameters. You can double-check the IP address here, as well as the rest of the basic settings. At the bottom is the summary \u2013 \u201cPlan: 1 to add, 0 to change, 0 to destroy.\u201d Also note that it tells us again what step to run next \u2013 \u201cterraform apply\u201d.</p>"},{"location":"DevOps/Terraform/Proxmox/#execute-the-terraform-plan-and-watch-the-vms-appear","title":"Execute the Terraform plan and watch the VMs appear!","text":"<p>With the summary stating what we want, we can now apply the plan (terraform apply). Note that it prompts you to type in \u2018yes\u2019 to apply the changes after it determines what the changes are. It typically takes 1m15s \u00b1 15s for my VMs to get created.</p> <p>If all goes well, you will be informed that 1 resource was added!</p> <p></p> <p>Command and full output:</p> <pre><code>austin@EARTH:/mnt/c/Users/Austin/terraform-blog$ terraform apply\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # proxmox_vm_qemu.test_server[0] will be created\n  + resource \"proxmox_vm_qemu\" \"test_server\" {\n      + additional_wait           = 15\n      + agent                     = 1\n      + balloon                   = 0\n      + bios                      = \"seabios\"\n      + boot                      = \"cdn\"\n      + bootdisk                  = \"scsi0\"\n      + clone                     = \"ubuntu-2004-cloudinit-template\"\n      + clone_wait                = 15\n      + cores                     = 2\n      + cpu                       = \"host\"\n      + default_ipv4_address      = (known after apply)\n      + define_connection_info    = true\n      + force_create              = false\n      + full_clone                = true\n      + guest_agent_ready_timeout = 600\n      + hotplug                   = \"network,disk,usb\"\n      + id                        = (known after apply)\n      + ipconfig0                 = \"ip=10.98.1.91/24,gw=10.98.1.1\"\n      + kvm                       = true\n      + memory                    = 2048\n      + name                      = \"test-vm-1\"\n      + nameserver                = (known after apply)\n      + numa                      = false\n      + onboot                    = true\n      + os_type                   = \"cloud-init\"\n      + preprovision              = true\n      + reboot_required           = (known after apply)\n      + scsihw                    = \"virtio-scsi-pci\"\n      + searchdomain              = (known after apply)\n      + sockets                   = 1\n      + ssh_host                  = (known after apply)\n      + ssh_port                  = (known after apply)\n      + sshkeys                   = &lt;&lt;-EOT\n              ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDcwZAOfqf6E6p8IkrurF2vR3NccPbMlXFPaFe2+Eh/8QnQCJVTL6PKduXjXynuLziC9cubXIDzQA+4OpFYUV2u0fAkXLOXRIwgEmOrnsGAqJTqIsMC3XwGRhR9M84c4XPAX5sYpOsvZX/qwFE95GAdExCUkS3H39rpmSCnZG9AY4nPsVRlIIDP+/6YSy9KWp2YVYe5bDaMKRtwKSq3EOUhl3Mm8Ykzd35Z0Cysgm2hR2poN+EB7GD67fyi+6ohpdJHVhinHi7cQI4DUp+37nVZG4ofYFL9yRdULlHcFa9MocESvFVlVW0FCvwFKXDty6askpg9yf4FnM0OSbhgqXzD austin@EARTH\n        EOT\n      + target_node               = \"prox-1u\"\n      + unused_disk               = (known after apply)\n      + vcpus                     = 0\n      + vlan                      = -1\n      + vmid                      = (known after apply)\n\n      + disk {\n          + backup       = 0\n          + cache        = \"none\"\n          + file         = (known after apply)\n          + format       = (known after apply)\n          + iothread     = 1\n          + mbps         = 0\n          + mbps_rd      = 0\n          + mbps_rd_max  = 0\n          + mbps_wr      = 0\n          + mbps_wr_max  = 0\n          + media        = (known after apply)\n          + replicate    = 0\n          + size         = \"10G\"\n          + slot         = 0\n          + ssd          = 0\n          + storage      = \"local-zfs\"\n          + storage_type = (known after apply)\n          + type         = \"scsi\"\n          + volume       = (known after apply)\n        }\n\n      + network {\n          + bridge    = \"vmbr0\"\n          + firewall  = false\n          + link_down = false\n          + macaddr   = (known after apply)\n          + model     = \"virtio\"\n          + queues    = (known after apply)\n          + rate      = (known after apply)\n          + tag       = -1\n        }\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n\n  Enter a value: yes\n\nproxmox_vm_qemu.test_server[0]: Creating...\nproxmox_vm_qemu.test_server[0]: Still creating... [10s elapsed]\nproxmox_vm_qemu.test_server[0]: Still creating... [20s elapsed]\nproxmox_vm_qemu.test_server[0]: Still creating... [30s elapsed]\nproxmox_vm_qemu.test_server[0]: Still creating... [40s elapsed]\nproxmox_vm_qemu.test_server[0]: Still creating... [50s elapsed]\nproxmox_vm_qemu.test_server[0]: Still creating... [1m0s elapsed]\nproxmox_vm_qemu.test_server[0]: Creation complete after 1m9s [id=prox-1u/qemu/142]\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n</code></pre> <p>Now go check Proxmox and see if your VM was created:</p> <p></p> <p>Success! You should now be able to SSH into the new VM with the key you already provided (note: the username will be \u2018ubuntu\u2019, not whatever you had set in your key).</p>"},{"location":"DevOps/Terraform/Proxmox/#removing-the-test-vm","title":"Removing the test VM","text":"<p>I just set the count to 0 for the resource in the main.tf file and apply and the VM is stopped and destroyed.</p> <pre><code>resource \"proxmox_vm_qemu\" \"test_server\" {\n    count = # just want 1 for now, set to 0 and apply to destroy\n}\n</code></pre>"},{"location":"DevOps/Terraform/Proxmox/#conclusion","title":"Conclusion","text":"<p>This felt like a quick-n-dirty tutorial for how to use Terraform to deploy virtual machines in Proxmox but looking back, there is a decent amount of detail. It took me quite a while to work through permission issues, hostnames being invalid (turns out you can\u2019t have underscores (_) in hostnames, duh, that took an hour to find), assigning roles to users vs the associated API keys, etc. but I\u2019m glad I worked through everything and can pass it along. Check back soon for my next post on using Terraform to deploy a full set of Kubernetes machines to a Proxmox cluster (and thrilling sequel to that post, Using Ansible to bootstrap a Kubernetes Cluster)!</p>"},{"location":"DevOps/Terraform/Proxmox/#references","title":"References","text":"<ul> <li>https://austinsnerdythings.com/2021/09/01/how-to-deploy-vms-in-proxmox-with-terraform/</li> <li>https://registry.terraform.io/providers/Telmate/proxmox/latest/docs</li> <li>https://vectops.com/2020/05/provision-proxmox-vms-with-terraform-quick-and-easy/</li> <li>https://norocketscience.at/provision-proxmox-virtual-machines-with-terraform/</li> </ul>"},{"location":"Linux/Gentoo/Base%20system/","title":"Installing base system","text":""},{"location":"Linux/Gentoo/Base%20system/#selecting-mirrors","title":"Selecting mirrors","text":"<p>In order to download source code quickly it is recommended to select a fast mirror. Portage will look in the make.conf file for the GENTOO_MIRRORS variable and use the mirrors listed therein. <pre><code>root #mirrorselect -i -o &gt;&gt; /mnt/gentoo/etc/portage/make.conf\n</code></pre></p>"},{"location":"Linux/Gentoo/Base%20system/#gentoo-ebuild-repository","title":"Gentoo ebuild repository","text":"<p>A second important step in selecting mirrors is to configure the Gentoo ebuild repository via the /etc/portage/repos.conf/gentoo.conf file. This file contains the sync information needed to update the package repository (the collection of ebuilds and related files containing all the information Portage needs to download and install software packages). <pre><code>root #mkdir --parents /mnt/gentoo/etc/portage/repos.conf\nroot #cp /mnt/gentoo/usr/share/portage/config/repos.conf /mnt/gentoo/etc/portage/repos.conf/gentoo.conf\n</code></pre></p> <pre><code># /mnt/gentoo/etc/portage/repos.conf/gentoo.conf\n[DEFAULT]\nmain-repo = gentoo\n\n[gentoo]\nlocation = /var/db/repos/gentoo\nsync-type = rsync\nsync-uri = rsync://rsync.gentoo.org/gentoo-portage\nauto-sync = yes\nsync-rsync-verify-jobs = 1\nsync-rsync-verify-metamanifest = yes\nsync-rsync-verify-max-age = 24\nsync-openpgp-key-path = /usr/share/openpgp-keys/gentoo-release.asc\nsync-openpgp-key-refresh-retry-count = 40\nsync-openpgp-key-refresh-retry-overall-timeout = 1200\nsync-openpgp-key-refresh-retry-delay-exp-base = 2\nsync-openpgp-key-refresh-retry-delay-max = 60\nsync-openpgp-key-refresh-retry-delay-mult = 4\n</code></pre>"},{"location":"Linux/Gentoo/Base%20system/#dns-info","title":"DNS info","text":"<pre><code>root #cp --dereference /etc/resolv.conf /mnt/gentoo/etc/\n</code></pre>"},{"location":"Linux/Gentoo/Base%20system/#mounting-the-necessary-filesystmes","title":"Mounting the necessary filesystmes","text":"<p>In a few moments, the Linux root will be changed towards the new location.</p> <p>The filesystems that need to be made available are:</p> Filesystem Description /proc/ a pseudo-filesystem. It looks like regular files, but is generated on-the-fly by the Linux kernel /sys/ is a pseudo-filesystem, like /proc/ which it was once meant to replace, and is more structured than /proc/ /dev/ is a regular file system which contains all device. It is partially managed by the Linux device manager (usually udev) /run/ is a temporary file system used for files generated at runtime, such as PID files or locks <p>The /proc/ location will be mounted on /mnt/gentoo/proc/ whereas the others are bind-mounted. The latter means that, for instance, /mnt/gentoo/sys/ will actually be /sys/ (it is just a second entry point to the same filesystem) whereas /mnt/gentoo/proc/ is a new mount (instance so to speak) of the filesystem.</p> <pre><code>root #mount --types proc /proc /mnt/gentoo/proc\nroot #mount --rbind /sys /mnt/gentoo/sys\nroot #mount --make-rslave /mnt/gentoo/sys\nroot #mount --rbind /dev /mnt/gentoo/dev\nroot #mount --make-rslave /mnt/gentoo/dev\nroot #mount --bind /run /mnt/gentoo/run\nroot #mount --make-slave /mnt/gentoo/run\n</code></pre> <p>The --make-rslave operations are needed for systemd support later in the installation.</p>"},{"location":"Linux/Gentoo/Base%20system/#entering-the-new-environment","title":"Entering the new environment","text":"<p>Now that all partitions are initialized and the base environment installed, it is time to enter the new installation environment by chrooting into it. This means that the session will change its root (most top-level location that can be accessed) from the current installation environment (installation CD or other installation medium) to the installation system (namely the initialized partitions). Hence the name, change root or chroot.</p> <p>This chrooting is done in three steps:</p> <ol> <li>The root location is changed from / (on the installation medium) to /mnt/gentoo/ (on the partitions) using chroot</li> <li>Some settings (those in /etc/profile) are reloaded in memory using the source command</li> <li>The primary prompt is changed to help us remember that this session is inside a chroot environment.</li> </ol> <pre><code>root #chroot /mnt/gentoo /bin/bash\nroot #source /etc/profile\nroot #export PS1=\"(chroot) ${PS1}\"\n</code></pre>"},{"location":"Linux/Gentoo/Base%20system/#mounting-the-boot-partition","title":"Mounting the boot partition","text":"<pre><code>root #mount /dev/sda1 /boot\n</code></pre>"},{"location":"Linux/Gentoo/Base%20system/#congiuring-portage","title":"Congiuring Portage","text":"<p>Installing a Gentoo ebuild repository snapshot from the web</p>"},{"location":"Linux/Gentoo/Bootloader/","title":"Configuring the bootloader","text":""},{"location":"Linux/Gentoo/Bootloader/#selecting-a-boot-loader","title":"Selecting a boot loader","text":"<p>With the Linux kernel configured, system tools installed and configuration files edited, it is time to install the last important piece of a Linux installation: the boot loader.</p> <p>The boot loader is responsible for firing up the Linux kernel upon boot - without it, the system would not know how to proceed when the power button has been pressed.</p> <p>For amd64, we document how to configure either GRUB or LILO for BIOS based systems, and GRUB or efibootmgr for UEFI systems.</p> <p>In this section of the Handbook a delineation has been made between emerging the boot loader's package and installing a boot loader to a system disk. Here the term emerge will be used to ask Portage to make the software package available to the system. The term install will signify the boot loader copying files or physically modifying appropriate sections of the system's disk drive in order to render the boot loader activated and ready to operate on the next power cycle.</p>"},{"location":"Linux/Gentoo/Bootloader/#default-grub","title":"Default: GRUB","text":"<p>By default, the majority of Gentoo systems now rely upon GRUB (found in the sys-boot/grub package), which is the direct successor to GRUB Legacy. With no additional configuration, GRUB gladly supports older BIOS (\"pc\") systems. With a small amount of configuration, necessary before build time, GRUB can support more than a half a dozen additional platforms. For more information, consult the Prerequisites section of the GRUB article.</p>"},{"location":"Linux/Gentoo/Bootloader/#emerge","title":"Emerge","text":"<p>Warning</p> <p>When using an older BIOS system supporting only MBR partition tables, no additional configuration is needed in order to emerge GRUB:</p> <pre><code>root #emerge --ask --verbose sys-boot/grub\n</code></pre> <p>Tip</p> <p>UEFI users: running the above command will output the enabled GRUB_PLATFORMS values before emerging. When using UEFI capable systems, users will need to ensure GRUB_PLATFORMS=\"efi-64\" is enabled (as it is the case by default). If that is not the case for the setup, GRUB_PLATFORMS=\"efi-64\" will need to be added to the /etc/portage/make.conf file before emerging GRUB so that the package will be built with EFI functionality:</p> <pre><code>root #echo 'GRUB_PLATFORMS=\"efi-64\"' &gt;&gt; /etc/portage/make.conf\nroot #emerge --ask sys-boot/grub\n</code></pre> <p>If GRUB was somehow emerged without enabling GRUB_PLATFORMS=\"efi-64\", the line (as shown above) can be added to make.conf and then dependencies for the world package set can be re-calculated by passing the --update --newuse options to emerge: <pre><code>root #emerge --ask --update --newuse --verbose sys-boot/grub\n</code></pre></p>"},{"location":"Linux/Gentoo/Bootloader/#install","title":"Install","text":"<p>Next, install the necessary GRUB files to the /boot/grub/ directory via the grub-install command. Presuming the first disk (the one where the system boots from) is /dev/sda, one of the following commands will do:</p> <ul> <li> <p>BIOS: <pre><code>root #grub-install /dev/sda\n</code></pre></p> </li> <li> <p>UEFI:</p> </li> </ul> <p>Make sure the EFI system partition has been mounted before running grub-install. It is possible for grub-install to install the GRUB EFI file (grubx64.efi) into the wrong directory without providing any indication the wrong directory was used. <pre><code>root #grub-install --target=x86_64-efi --efi-directory=/boot\n</code></pre></p> <p>Modify the --efi-directory option to the root of the EFI System Partition. This is necessary if the /boot partition was not formatted as a FAT variant.</p> <p>Warning</p> <p>If grub-install returns an error like Could not prepare Boot variable: Read-only file system, it may be necessary to remount the efivars special mount as read-write in order to succeed: <pre><code>root #mount -o remount,rw,nosuid,nodev,noexec --types efivarfs efivarfs /sys/firmware/efi/efivars\n</code></pre></p> <p>Some motherboard manufacturers seem to only support the /efi/boot/ directory location for the .EFI file in the EFI System Partition (ESP). The GRUB installer can perform this operation automatically with the --removable option. Verify the ESP is mounted before running the following commands. Presuming the ESP is mounted at /boot (as suggested earlier), execute: <pre><code>root #grub-install --target=x86_64-efi --efi-directory=/boot --removable\n</code></pre> This creates the default directory defined by the UEFI specification, and then copies the grubx64.efi file to the 'default' EFI file location defined by the same specification.</p>"},{"location":"Linux/Gentoo/Bootloader/#configure","title":"Configure","text":"<p>Next, generate the GRUB configuration based on the user configuration specified in the /etc/default/grub file and /etc/grub.d scripts. In most cases, no configuration is needed by users as GRUB will automatically detect which kernel to boot (the highest one available in /boot/) and what the root file system is. It is also possible to append kernel parameters in /etc/default/grub using the GRUB_CMDLINE_LINUX variable.</p> <p>To generate the final GRUB configuration, run the grub-mkconfig command: <pre><code>root #grub-mkconfig -o /boot/grub/grub.cfg\n\nGenerating grub.cfg ...\nFound linux image: /boot/vmlinuz-5.15.52-gentoo\nFound initrd image: /boot/initramfs-genkernel-amd64-5.15.52-gentoo\ndone\n</code></pre></p> <p>The output of the command must mention that at least one Linux image is found, as those are needed to boot the system. If an initramfs is used or genkernel was used to build the kernel, the correct initrd image should be detected as well. If this is not the case, go to /boot/ and check the contents using the ls command. If the files are indeed missing, go back to the kernel configuration and installation instructions.</p> <p>Tip</p> <p>The os-prober utility can be used in conjunction with GRUB to detect other operating systems from attached drives. Windows 7, 8.1, 10, and other distributions of Linux are detectable. Those desiring dual boot systems should emerge the sys-boot/os-prober package then re-run the grub-mkconfig command (as seen above). If detection problems are encountered be sure to read the GRUB article in its entirety before asking the Gentoo community for support.</p>"},{"location":"Linux/Gentoo/Bootloader/#efibootmgr","title":"efibootmgr","text":"<p>On UEFI based systems, the UEFI firmware on the system (in other words the primary bootloader), can be directly manipulated to look for UEFI boot entries. Such systems do not need to have additional (also known as secondary) bootloaders like GRUB in order to help boot the system. With that being said, the reason EFI-based bootloaders such as GRUB exist is to extend the functionality of UEFI systems during the boot process. Using efibootmgr is really for those who desire to take a minimalist (although more rigid) approach to booting their system; using GRUB (see above) is easier for the majority of users because it offers a flexible approach when booting UEFI systems.</p> <p>Remember sys-boot/efibootmgr application is not a bootloader; it is a tool to interact with the UEFI firmware and update its settings, so that the Linux kernel that was previously installed can be booted with additional options (if necessary), or to allow multiple boot entries. This interaction is done through the EFI variables (hence the need for kernel support of EFI vars).</p> <p>Be sure to read through the EFI stub kernel article before continuing. The kernel must have specific options enabled to be directly bootable by the system's UEFI firmware. It might be necessary to recompile the kernel. It is also a good idea to take a look at the efibootmgr article.</p> <p>To reiterate, efibootmgr is not a requirement to boot an UEFI system. The Linux kernel itself can be booted immediately, and additional kernel command-line options can be built-in to the Linux kernel (there is a kernel configuration option called CONFIG_CMDLINE that allows the user to specify boot parameters as command-line options. Even an initramfs can be 'built-in' to the kernel.</p> <pre><code>root #emerge --ask sys-boot/efibootmgr\n</code></pre> <p>Then, create the /boot/efi/boot/ location, and then copy the kernel into this location, calling it bootx64.efi: <pre><code>root #mkdir -p /boot/efi/boot\nroot #cp /boot/vmlinuz-* /boot/efi/boot/bootx64.efi\n</code></pre></p> <p>Next, tell the UEFI firmware that a boot entry called \"Gentoo\" is to be created, which has the freshly compiled EFI stub kernel: <pre><code>root #efibootmgr --create --disk /dev/sda --part 2 --label \"Gentoo\" --loader \"\\efi\\boot\\bootx64.efi\"\n</code></pre></p> <p>If an initial RAM file system (initramfs) is used, add the proper boot option to it: <pre><code>root #efibootmgr -c -d /dev/sda -p 2 -L \"Gentoo\" -l \"\\efi\\boot\\bootx64.efi\" initrd='\\initramfs-genkernel-amd64-5.15.52-gentoo'\n</code></pre></p> <p>The use of  as directory separator is mandatory when using UEFI definitions.</p>"},{"location":"Linux/Gentoo/Bootloader/#rebooting-the-system","title":"Rebooting the system","text":"<p>Exit the chrooted environment and unmount all mounted partitions. Then type in that one magical command that initiates the final, true test: reboot <pre><code>root #exit\ncdimage ~#cd\ncdimage ~#umount -l /mnt/gentoo/dev{/shm,/pts,}\ncdimage ~#umount -R /mnt/gentoo\ncdimage ~#reboot\n</code></pre></p>"},{"location":"Linux/Gentoo/Finalizing/","title":"Finalizing","text":""},{"location":"Linux/Gentoo/Finalizing/#user-administration","title":"User administration","text":""},{"location":"Linux/Gentoo/Finalizing/#adding-a-user-for-daily-use","title":"Adding a user for daily use","text":"Group Description audio Be able to access the audio devices. cdrom Be able to directly access optical devices. floppy Be able to directly access floppy devices. games Be able to play games. portage Be able to access portage restricted resources. usb Be able to access USB devices. video Be able to access video capturing hardware and doing hardware acceleration. wheel Be able to use su. <p>For instance, to create a user called larry who is member of the wheel, users, and audio groups, log in as root first (only root can create users) and run useradd: <pre><code>Login:root\n\nPassword: (Enter the root password)\n</code></pre></p> <pre><code>root #useradd -m -G users,wheel,audio -s /bin/bash larry\nroot #passwd larry\n\nPassword: (Enter the password for larry)\nRe-enter password: (Re-enter the password to verify)\n</code></pre>"},{"location":"Linux/Gentoo/Finalizing/#disk-cleanup","title":"Disk cleanup","text":""},{"location":"Linux/Gentoo/Finalizing/#removing-tarballs","title":"Removing tarballs","text":"<p>With the Gentoo installation finished and the system rebooted, if everything has gone well, we can now remove the downloaded stage3 tarball from the hard disk. Remember that they were downloaded to the / directory. <pre><code>root #rm /stage3-*.tar.*\n</code></pre></p>"},{"location":"Linux/Gentoo/Introduction/","title":"How to install Gentoo with EFI boot loader and OpenRC sysControl","text":""},{"location":"Linux/Gentoo/Introduction/#amd64-handbook","title":"AMD64 Handbook","text":""},{"location":"Linux/Gentoo/Introduction/#how-the-installation-is-structured","title":"How the installation is structured","text":"<p>The Gentoo Installation can be seen as a 10-step procedure. Each step results in a certain state: Step</p> Step Result 1 The user is in a working environment ready to install Gentoo. 2 The Internet connection is ready to install Gentoo. 3 The hard disks are initialized to host the Gentoo installation. 4 The installation environment is prepared and the user is ready to chroot into the new environment. 5 Core packages, which are the same on all Gentoo installations, are installed. 6 The Linux kernel is installed. 7 Most of the Gentoo system configuration files are created. 8 The necessary system tools are installed. 9 The proper boot loader has been installed and configured. 10 The freshly installed Gentoo Linux environment is ready to be explored."},{"location":"Linux/Gentoo/Kernel/","title":"Configuring the kernel","text":""},{"location":"Linux/Gentoo/Kernel/#installing-firmware-andor-microcode","title":"Installing firmware and/or microcode","text":""},{"location":"Linux/Gentoo/Kernel/#firmware","title":"Firmware","text":"<p>Before getting to configuring kernel sections, it is beneficial to be aware that some hardware devices require additional, sometimes non-FOSS compliant, firmware to be installed on the system before they will operate correctly. This is often the case for wireless network interfaces commonly found in both desktop and laptop computers. Modern video chips from vendors like AMD, Nvidia, and Intel, often also require external firmware files to be fully functional. Most firmware for modern hardware devices can be found within the sys-kernel/linux-firmware package. This is the recommendation to have to be installed before the initial system reboot.</p> <pre><code>root #emerge --ask sys-kernel/linux-firmware\n</code></pre> <p>Installing certain firmware packages often requires accepting the associated firmware licenses. If necessary, visit the license handling section of the Handbook for help on accepting licenses.</p>"},{"location":"Linux/Gentoo/Kernel/#microcode","title":"Microcode","text":"<p>In addition to discrete graphics hardware and network interfaces, CPUs also can require firmware updates. Typically this kind of firmware is referred to as microcode. Newer revisions of microcode are sometimes necessary to patch instability, security concerns, or other miscellaneous bugs in CPU hardware.</p> <p>Microcode updates for AMD CPUs are distributed within the aforementioned sys-kernel/linux-firmware package. Microcode for Intel CPUs can be found within the sys-firmware/intel-microcode package, which will need to be installed separately. See the {++Microcode article++} for more information on how to apply microcode updates.</p>"},{"location":"Linux/Gentoo/Kernel/#kernel-configuration-and-compilation","title":"Kernel configuration and compilation","text":"<ul> <li>Full automation approach: Distribution kernels</li> <li>Hybrid approach: Genkernel</li> <li>Full manual approach</li> </ul>"},{"location":"Linux/Gentoo/Kernel/#distribution-kernels-distribution-kernels","title":"Distribution kernels Distribution kernels","text":"<p>Distribution Kernels are ebuilds that cover the complete process of unpacking, configuring, compiling, and installing the kernel. The primary advantage of this method is that the kernels are updated to new versions by the package manager as part of @world upgrade. This requires no more involvement than running an emerge command. Distribution kernels default to a configuration supporting the majority of hardware, however two mechanisms are offered for customization: savedconfig and config snippets. See the project page for more details on configuration.</p>"},{"location":"Linux/Gentoo/Kernel/#installing-the-correct-installkernel-package","title":"Installing the correct installkernel package","text":"<pre><code># systemd-boot as the bootloader\nroot #emerge --ask sys-kernel/installkernel-systemd-boot\n</code></pre> <pre><code># Grub,LILO,etc traditional layout\nroot #emerge --ask sys-kernel/installkernel-gentoo\n</code></pre>"},{"location":"Linux/Gentoo/Kernel/#install-a-distribution-kernel","title":"Install a distribution kernel","text":"<pre><code># build a kernel with Gentoo patches form source\nroot #emerge --ask sys-kernel/gentoo-kernel\n</code></pre>"},{"location":"Linux/Gentoo/Kernel/#upgrading-and-cleaning-up","title":"Upgrading and cleaning up","text":"<p>Once the kernel is installed, the package manager will automatically update it to newer versions. The previous versions will be kept until the package manager is requested to clean up stale packages. To reclaim disk space, stale packages can be trimmed by periodically running emerge with the --depclean option: <pre><code>root #emerge --depclean\n\n# clean up old kernel versions as specifically\nemerge --prune sys-kernel/gentoo-kernel sys-kernel/gentoo-kernel-bin\n</code></pre></p>"},{"location":"Linux/Gentoo/Kernel/#post-installupgrade-tasks","title":"Post-install/upgrade tasks","text":"<p>Distribution kernels are capable of rebuilding kernel modules installed by other packages. linux-mod.eclass provides the dist-kernel USE flag which controls a subslot dependency on virtual/dist-kernel.</p> <p>Enabling this USE flag on packages like sys-fs/zfs and sys-fs/zfs-kmod allows them to automatically be rebuilt against a newly updated kernel and, if applicable, will re-generate the initramfs accordingly. Manually rebuilding the initramfs</p> <p>If required, manually trigger such rebuilds by, after a kernel upgrade, executing: <pre><code>root #emerge --ask @module-rebuild\n</code></pre></p> <p>If any kernel modules (e.g. ZFS) are needed at early boot, rebuild the initramfs afterward via: <pre><code>root #emerge --config sys-kernel/gentoo-kernel\nroot #emerge --config sys-kernel/gentoo-kernel-bin\n</code></pre></p>"},{"location":"Linux/Gentoo/Kernel/#installing-the-kernel-sources","title":"Installing the kernel sources","text":"<p>This section is only relevant when using the following genkernel(hybrid) or manual kernel management approach.</p> <p>When installing and compiling the kernel for amd64-based systems, Gentoo recommends the sys-kernel/gentoo-sources package.</p> <p>Choose an appropriate kernel source and install it using emerge: <pre><code>root #emerge --ask sys-kernel/gentoo-sources\n</code></pre></p> <p>This will install the Linux kernel sources in /usr/src/ using the specific kernel version in the path. It will not create a symbolic link by itself without USE=symlink being enabled on the chosen kernel sources package.</p> <p>It is conventional for a /usr/src/linux symlink to be maintained, such that it refers to whichever sources correspond with the currently running kernel. However, this symbolic link will not be created by default. An easy way to create the symbolic link is to utilize eselect's kernel module.</p> <p>For further information regarding the purpose of the symlink, and how to manage it, please refer to Kernel/Upgrade.</p> <p>First, list all installed kernels: <pre><code>root #eselect kernel list\n\nAvailable kernel symlink targets:\n  [1]   linux-5.15.52-gentoo\n</code></pre></p> <p>In order to create a symbolic link called linux, use: <pre><code>eselect kernel set 1\n</code></pre></p> <pre><code>root #ls -l /usr/src/linux\n\nlrwxrwxrwx    1 root   root    12 Oct 13 11:04 /usr/src/linux -&gt; linux-5.15.52-gentoo\n</code></pre>"},{"location":"Linux/Gentoo/Kernel/#genkernel","title":"Genkernel","text":"<p>If an entirely manual configuration looks too daunting, system administrators should consider using genkernel as a hybrid approach to kernel maintenance.</p> <p>{++Genkernel provides a generic kernel configuration file, automatically generates the kernel, initramfs, and associated modules, and then installs the resulting binaries to the appropriate locations.++} {++This results in minimal and generic hardware support for the system's first boot, and allows for additional update control and customization of the kernel's configuration in the future.++}</p> Info <p>While using genkernel to maintain the kernel provides system administrators with more update control over the system's kernel, initramfs, and other options, it will require a time and effort commitment to perform future kernel updates as new sources are released. Those looking for a hands-off approach to kernel maintenance should use a distribution kernel.</p> <p>Misconception to believe genkernel automatically generates a custom kernel configuration for the hardware on which it is run;</p> <p>It uses a predetermined kernel configuration that supports most generic hardware and automatically handles the make commands necessary to assemble and install the kernel, the associate modules, and the initramfs file.</p> <p>As a prerequisite, due to the firwmare USE flag being enabled by default for the sys-kernel/genkernel package, the package manager will also attempt to pull in the sys-kernel/linux-firmware package. The binary redistributable software licenses are required to be accepted before the linux-firmware will install.</p> <p>This license group can be accepted system-wide for any package by adding the @BINARY-REDISTRIBUTABLE as an ACCEPT_LICENSE value in the /etc/portage/make.conf file. It can be exclusively accepted for the linux-firmware package by adding a specific inclusion via a /etc/portage/package.license/linux-firmware file.</p> <p>If necessary, review the methods of accepting software licenses available in the Installing the base system chapter of the handbook, then make some changes for acceptable software licenses.</p> <p>If in analysis paralysis, the following will do the trick: <pre><code>root #mkdir /etc/portage/package.license\n</code></pre></p> <p>/etc/portage/package.license/linux-firmware</p> <p>Accept binary redistributable licenses for the linux-firmware package <pre><code>sys-kernel/linux-firmware @BINARY-REDISTRIBUTABLE\n</code></pre></p>"},{"location":"Linux/Gentoo/Kernel/#installation","title":"Installation","text":"<p>Explanations and prerequisites aside, install the sys-kernel/genkernel package:</p>"},{"location":"Linux/Gentoo/Kernel/#generation","title":"Generation","text":"<p>Compile the kernel sources by running genkernel all. Be aware though, as genkernel compiles a kernel that supports a wide array of hardware for differing computer architectures, this compilation may take quite a while to finish.</p> <p>If the root partition/volume uses a filesystem other than ext4, it may be necessary to manually configure the kernel using genkernel --menuconfig all to add built-in kernel support for the particular filesystem(s) (i.e. not building the filesystem as a module).</p> <p>Users of LVM2 should add --lvm as an argument to the genkernel command below.</p> <pre><code>root #genkernel --mountboot --install all\n</code></pre> <p>Once genkernel completes, a kernel and an initial ram filesystem (initramfs) will be generated and installed into the /boot directory. Associated modules will be installed into the /lib/modules directory. The initramfs will be started immediately after loading the kernel to perform hardware auto-detection (just like in the live disk image environments).</p> <pre><code>root #ls /boot/vmlinu* /boot/initramfs*\nroot #ls /lib/modules\n</code></pre>"},{"location":"Linux/Gentoo/Kernel/#manual-configuration","title":"Manual configuration","text":""},{"location":"Linux/Gentoo/Kernel/#introduction","title":"Introduction","text":"<p>Manually configuring a kernel is often seen as the most difficult procedure a Linux user ever has to perform. Nothing is less true - after configuring a couple of kernels no one remembers that it was difficult!</p> <p>However, one thing is true: it is vital to know the system when a kernel is configured manually. Most information can be gathered by emerging sys-apps/pciutils which contains the lspci command: <pre><code>root #emerge --ask sys-apps/pciutils\n</code></pre></p> <p>Inside the chroot, it is safe to ignore any pcilib warnings (like pcilib: cannot open /sys/bus/pci/devices) that lspci might throw out.</p> <p>Another source of system information is to run lsmod to see what kernel modules the installation CD uses as it might provide a nice hint on what to enable.</p> <p>Now go to the kernel source directory and execute make menuconfig. This will fire up menu-driven configuration screen.</p> <pre><code>root #cd /usr/src/linux\nroot #make menuconfig\n</code></pre> <p>Gentoo kernel configuration guide</p> <p>The Linux kernel configuration has many, many sections. Let's first list some options that must be activated (otherwise Gentoo will not function, or not function properly without additional tweaks). We also have a Gentoo kernel configuration guide on the Gentoo wiki that might help out further.</p>"},{"location":"Linux/Gentoo/Kernel/#enabling-required-options","title":"Enabling required options","text":"<p>When using sys-kernel/gentoo-sources, it is strongly recommend the Gentoo-specific configuration options be enabled. These ensure that a minimum of kernel features required for proper functioning is available:</p> <p>Enable Gentoo-specific options</p> <p>KERNEL</p> <pre><code>Gentoo Linux ---&gt;\n  Generic Driver Options ---&gt;\n    [*] Gentoo Linux support\n    [*]   Linux dynamic and persistent device naming (userspace devfs) support\n    [*]   Select options required by Portage features\n        Support for init systems, system and service manager ---&gt;\n          [*] OpenRC, runit and other script based systems and managers.\n          [*] systemd\n</code></pre> <p>Naturally the choice in the last two lines depends on the selected init system (OpenRC vs. systemd). It does not hurt to have support for both init systems enabled.</p>"},{"location":"Linux/Gentoo/Kernel/#enabling-support-for-typical-system-components","title":"Enabling support for typical system components","text":"<p>Make sure that every driver that is vital to the booting of the system (such as SATA controllers, NVMe block device support, filesystem support, etc.) is compiled in the kernel and not as a module, otherwise the system may not be able to boot completely.</p> <p>Next select the exact processor type. It is also recommended to enable MCE features (if available) so that users are able to be notified of any hardware problems. On some architectures (such as x86_64), these errors are not printed to dmesg, but to /dev/mcelog. This requires the app-admin/mcelog package.</p> <p>Also select Maintain a devtmpfs file system to mount at /dev so that critical device files are already available early in the boot process (CONFIG_DEVTMPFS and CONFIG_DEVTMPFS_MOUNT):</p> <p>Enable devtmpfs support(_CONFIG_DEVTMPFS)</p> <p>KERNEL</p> <pre><code>Device Driver ---&gt;\n  Generic Driver Options ---&gt;\n    [*] Maintain a devtmpfs filesystem to mount at /dev\n    [*]   Automount devtmpfs at /dev, after the kernel mounted the rootfs\n</code></pre> <p>Verify SCSI disk support has been activated \u0013(CONFIG_BLK_DEV_SD):</p> <p>Enabling SCSI disk support (CONFIG_SCSI, CONFIG_BLK_DEV_SD)</p> <p>KERNEL</p> <pre><code>Device Driver ---&gt;\n  SCSI device support ---&gt;\n    &lt;*&gt; SCSI device support\n    &lt;*&gt; SCSI disk support\n</code></pre> <p>Enabling basic SATA and PATA support (CONFIG_ATA_ACPI, CONFIG_SATA_PMP, CONFIG_SATA_AHCI, CONFIG_ATA_BMDMA, CONFIG_ATA_SFF, CONFIG_ATA_PIIX)</p> <p>KERNEL</p> <pre><code>Device Driver ---&gt;\n  &lt;*&gt; Serial ATA and Parallel ATA drivers (libata) ---&gt;\n    [*] ATA ACPI Support\n    [*] SATA Poart Multiplier support\n    &lt;*&gt; AHCI SATA support (achi)\n    [*] ATA BMDMA support\n    [*] ATA SFF support (for legacy IDE and PATA)\n    &lt;*&gt; Intel ESB, ICH, PIIX3, PIIX4 PATA/SATA support (ata_piix)\n</code></pre> <p>Verify basic NVMe support has been enabled:</p> <p>Enable basic NVMe support for Linux 4.4.x (CONFIG_BLK_DEV_NVME)</p> <p>KERNEL</p> <pre><code>Device Driver ---&gt;\n  &lt;*&gt; NVM Express block device\n</code></pre> <p>Enable basic NVMe support for Linux 5.x.x (CONFIG_DEVTMPFS)</p> <p>KERNEL</p> <pre><code>Device Driver ---&gt;\n  NVME support ---&gt;\n    &lt;*&gt; NVM Express block device\n</code></pre> <p>It does not hurt to enable the following additional NVMe support:</p> <p>Enabling additional NVMe support (CONFIG_NVME_MULTIPATH, CONFIG_NVME_MULTIPATH, CONFIG_NVME_HWMON, CONFIG_NVME_FC, CONFIG_NVME_TCP, CONFIG_NVME_TARGET, CONFIG_NVME_TARGET_PASSTHRU, CONFIG_NVME_TARGET_LOOP, CONFIG_NVME_TARGET_FC, CONFIG_NVME_TARGET_FCLOOP, CONFIG_NVME_TARGET_TCP</p> <p>KERNEL</p> <pre><code>[*] NVMe multipath support\n[*] NVME hardware monitoring\n&lt;M&gt; NVM Express over Fabrics FC host driver\n&lt;M&gt; NVM Express over Fabrics TCP host driver\n&lt;M&gt; NVM Traget support\n  [*]   NVMe Target Passthrough support\n  &lt;M&gt;   NVMe loopback device support\n  &lt;M&gt;   NVMe over Fabrics FC target driver\n  &lt; &gt;     NVMe over Fabrics FC Transport Loopback Test driver (NEW)\n  &lt;M&gt;   NVMe over Fabrics TCP target driver\n</code></pre> <p>Now go to File Systems and select support for the filesystems that will be used by the system. Do not compile the file system that is used for the root filesystem as module, otherwise the system may not be able to mount the partition. Also select Virtual memory and /proc file system. Select one or more of the following options as needed by the system:</p> <p>Enable file system support (CONFIG_EXT2_FS, CONFIG_EXT3_FS, CONFIG_EXT4_FS, CONFIG_BTRFS_FS, CONFIG_MSDOS_FS, CONFIG_VFAT_FS, CONFIG_PROC_FS, and CONFIG_TMPFS)</p> <p>KERNEL</p> <pre><code>File systems ---&gt;\n  &lt;*&gt; Second extended fs support\n  &lt;*&gt; The Extended 3 (ext3) filesystem\n  &lt;*&gt; The Extended 4 (ext3) filesystem\n  &lt;*&gt; Btrfs filesystem support\n  DOS/FAT/NT Filesystems ---&gt;\n    &lt;*&gt; MSDOS fs support\n    &lt;*&gt; VFAT (Windows-95) fs support\n  Pseudo Filesystems ---&gt;\n    [*] /porc file system support\n    [*] Tmpfs virtual memory file system support (former shm fs)\n</code></pre> <p>Most systems also have multiple cores at their disposal, so it is important to activate Symmetric multi-processing support (CONFIG_SMP):</p> <p>Activating SMP support (CONFIG_SMP)</p> <p>KERNEL</p> <pre><code>Processor type and features ---&gt;\n  [*] Symmetric multi-processing support\n</code></pre> <p>If USB input devices (like keyboard or mouse) or other USB devices will be used, do not forget to enable those as well:</p> <p>Enable USB and human input device support (CONFIG_HID_GENERIC, CONFIG_USB_HID, CONFIG_USB_SUPPORT, CONFIG_USB_XHCI_HCD, CONFIG_USB_EHCI_HCD, CONFIG_USB_OHCI_HCD, (CONFIG_HID_GENERIC, CONFIG_USB_HID, CONFIG_USB_SUPPORT, CONFIG_USB_XHCI_HCD, CONFIG_USB_EHCI_HCD, CONFIG_USB_OHCI_HCD, CONFIG_USB4)</p> <p>KERNEL</p> <pre><code>Device Drivers ---&gt;\n  HID support  ---&gt;\n    -*- HID bus support\n    &lt;*&gt;   Generic HID driver\n    [*]   Battery level reporting for HID devices\n      USB HID suport ---&gt;\n        &lt;*&gt; USB HID transport layer\n[*] USB support ---&gt;\n  &lt;*&gt;       xHCI HCD (USB 3.0) support\n  &lt;*&gt;       EHCI HCD (USB 2.0) support\n  &lt;*&gt;       OHCI HCD (USB 1.1) support\n&lt;*&gt; Unified support for USB4 and Thunderbolt ---&gt;\n</code></pre> <p>If PPPoE is used to connect to the Internet, or a dial-up modem, then enable the following options (CONFIG_PPP, CONFIG_PPP_ASYNC, and CONFIG_PPP_SYNC_TTY):</p> <p>Enabling PPPoE support (PPPoE, CONFIG_PPPOE, CONFIG_PPP_ASYNC, CONFIG_PPP_SYNC_TTY</p> <p>KERNEL</p> <pre><code>Device Driver ---&gt;\n  Network device support ---&gt;\n    &lt;*&gt; PPP (point-to-point protocol) support\n    &lt;*&gt; PPP over Ethernet\n    &lt;*&gt; PPP support for async serial ports\n    &lt;*&gt; PPP support for sync tty point\n</code></pre>"},{"location":"Linux/Gentoo/Kernel/#architecture-specific-kernel-configuration","title":"Architecture specific kernel configuration","text":"<p>Enable GPT partition label support if that was used previously when partitioning the disk (CONFIG_PARTITION_ADVANCED and CONFIG_EFI_PARTITION):</p> <p>Enable support for GPT</p> <p>KERNEL</p> <pre><code>_*_ Enable the block layer ---&gt;\n   Partition Types ---&gt;\n    [*] Advanced partition selection\n    [*] EFI GUID Partition support\n</code></pre> <p>Enable EFI stub support, EFI variables and EFI Framebuffer in the Linux kernel if UEFI is used to boot the system (CONFIG_EFI, CONFIG_EFI_STUB, CONFIG_EFI_MIXED, CONFIG_EFI_VARS, and CONFIG_FB_EFI):</p> <p>Enable support for UEFI</p> <p>KERNEL</p> <pre><code>Processor type and features ---&gt;\n   [*] EFI runtime service support\n   [*]   EFI stub support\n   [*]     EFI mixed-mode support\n\nDevice Drivers\n    Firmware Drivers   ---&gt;\n        EFI (Extension Firmware Interface) Support  ---&gt;\n            &lt;*&gt; EFI Veriable Support via sysfs\n    Graphics support   ---&gt;\n        Frame buffer Devices  ---&gt;\n            &lt;*&gt; Support for frame buffer devices  ---&gt;\n                [*]     EFI-based Framebuffer Support\n</code></pre>"},{"location":"Linux/Gentoo/Kernel/#compiling-and-installing","title":"Compiling and installing","text":"<p>With the configuration now done, it is time to compile and install the kernel. Exit the configuration and start the compilation process: <pre><code>root #make &amp;&amp; make modules_install\n</code></pre></p> Note <p>It is possible to enable parallel builds using make -jX with X being an integer number of parallel tasks that the build process is allowed to launch. This is similar to the instructions about /etc/portage/make.conf earlier, with the MAKEOPTS variable.</p> <p>When the kernel has finished compiling, copy the kernel image to /boot/. This is handled by the make install command: <pre><code>root #make install\n</code></pre></p>"},{"location":"Linux/Gentoo/Kernel/#optional-building-an-initramfs","title":"Optional: Building an initramfs","text":"<p>In certain cases it is necessary to build an initramfs - an initial ram-based file system. The most common reason is when important file system locations (like /usr/ or /var/) are on separate partitions. With an initramfs, these partitions can be mounted using the tools available inside the initramfs.</p> <p>Without an initramfs, there is a risk that the system will not boot properly as the tools that are responsible for mounting the file systems require information that resides on unmounted file systems. An initramfs will pull in the necessary files into an archive which is used right after the kernel boots, but before the control is handed over to the init tool. Scripts on the initramfs will then make sure that the partitions are properly mounted before the system continues booting.</p> <p>Important</p> <p>If using genkernel, it should be used for both building the kernel and the initramfs. When using genkernel only for generating an initramfs, it is crucial to pass {++--kernel-config=/path/to/kernel.config++} to genkernel or the generated initramfs may not work with a manually built kernel. Note that manually built kernels go beyond the scope of support for the handbook. See the kernel configuration article for more information.</p> <p>To install an initramfs, install sys-kernel/dracut first, then have it generate an initramfs:</p> <pre><code>root #emerge --ask sys-kernel/dracut\nroot #dracut --kver=5.15.52-gentoo\n</code></pre> <p>The initramfs will be stored in /boot/. The resulting file can be found by simply listing the files starting with initramfs:</p> <pre><code>root #ls /boot/initramfs*\n</code></pre>"},{"location":"Linux/Gentoo/Kernel/#kernel-modules","title":"Kernel modules","text":""},{"location":"Linux/Gentoo/Kernel/#listing-available-kernel-modules","title":"Listing available kernel modules","text":"<p>Hardware modules are optional to be listed manually. udev will normally load all hardware modules that are detected to be connected in most cases. However, it is not harmful for modules that will be automatically loaded to be listed. Modules cannot be loaded twice; they are either loaded or unloaded. Sometimes exotic hardware requires help to load their drivers.</p> <p>The modules that need to be loaded during each boot in can be added to /etc/modules-load.d/.conf files in the format of one module per line. When extra options are needed for the modules, they should be set in /etc/modprobe.d/.conf files instead.</p> <p>To view all modules available for a specific kernel version, issue the following find command. Do not forget to substitute \"\" with the appropriate version of the kernel to search: <pre><code>root #find /lib/modules/&lt;kernel version&gt;/ -type f -iname '*.o' -or -iname '*.ko' | less\n</code></pre>"},{"location":"Linux/Gentoo/Kernel/#force-loading-particular-kernel-modules","title":"Force loading particular kernel modules","text":"<p>To force load the kernel to load the 3c59x.ko module (which is the driver for a specific 3Com network card family), edit the /etc/modules-load.d/network.conf file and enter the module name within it.</p> <pre><code>root #mkdir -p /etc/modules-load.d\nroot #nano -w /etc/modules-load.d/network.conf\n</code></pre> <p>Note that the module's .ko file suffix is insignificant to the loading mechanism and left out of the configuration file:</p> <p>Force loading 3c59x module</p> <p>FILE /etc/modules-load.d/network.conf</p> <p>3c59x</p>"},{"location":"Linux/Gentoo/Media/","title":"Choosing the media","text":""},{"location":"Linux/Gentoo/Media/#hardware-requirements","title":"Hardware requirements","text":"<p>AMD64 livedisk hardware requirements</p> Minimal CD LiveDVD CPU Any x86-64 CPU, both AMD64 and Intel 64 Memory 2 GB Disk space 8 GB(excluding swap space) Swap space At least 2 GB"},{"location":"Linux/Gentoo/Media/#gentoo-linux-installation-media","title":"Gentoo Linux installation Media","text":""},{"location":"Linux/Gentoo/Media/#minimal-installation","title":"Minimal installation","text":"<p>The Gentoo minimal installation CD is a bootable image: a self-contained Gentoo environment. It allows the user to boot Linux from the CD or other installation media. During the boot process the hardware is detected and the appropriate drivers are loaded. The image is maintained by Gentoo developers and allows anyone to install Gentoo if an active Internet connection is available.</p> <p>The Minimal Installation CD is called install-amd64-minimal-.iso."},{"location":"Linux/Gentoo/Media/#the-occasional-gentoo-livedvd","title":"The occasional Gentoo LiveDVD","text":"<p>Occasionally, a special DVD image is crafted which can be used to install Gentoo. The instructions in this chapter target the Minimal Installation CD, so things might be a bit different when booting from the LiveDVD. However, the LiveDVD (or any other official Gentoo Linux environment) supports getting a root prompt by just invoking sudo su - or sudo -i in a terminal.</p>"},{"location":"Linux/Gentoo/Media/#what-are-stages-then","title":"What are stages then?","text":"<p>A stage3 tarball is an archive containing a profile specific minimal Gentoo environment. Stage3 tarballs are suitable to continue the Gentoo installation using the instructions in this handbook. Previously, the handbook described the installation using one of three stage tarballs. Gentoo does not offer stage1 and stage2 tarballs for download any more since these are mostly for internal use and for bootstrapping Gentoo on new architectures.</p> <p>Stage3 tarballs can be downloaded from releases/amd64/autobuilds/ on any of the official Gentoo mirrors. Stage files update frequently and are not included in official installation images.</p> <ul> <li>A .CONTENTS file which is a text file listing all files available on the installation media. This file can be useful to verify if particular firmware or drivers are available on the installation media before downloading it.</li> <li>A .DIGESTS file which contains the hash of the ISO file itself, in various hashing formats/algorithms. This file can be used to verify if the downloaded ISO file is corrupt or not.</li> <li>A .asc file which is a cryptographic signature of the ISO file. This can be used to both verify if the downloaded ISO file is corrupt or not, as well as verify that the download is indeed provided by the Gentoo Release Engineering team and has not been tampered with.</li> </ul>"},{"location":"Linux/Gentoo/Media/#using-dd-to-write-the-iso-image-to-a-usb-driver","title":"Using dd to write the ISO image to a USB driver","text":"Warning <p>The dd command will wipe all data from the destination drive. Always backup all important data.</p> <pre><code>root #dd if=/path/to/image.iso of=/dev/sdc bs=8192k; sync\n</code></pre>"},{"location":"Linux/Gentoo/Media/#creating-bootable-liveusb-drives-from-linux-systems","title":"Creating bootable LiveUSB drives from Linux systems","text":""},{"location":"Linux/Gentoo/Media/#automatic-drive-wide-installation-script","title":"Automatic drive-wide installation script","text":"Warning <p>This script will erase all data from the USB drive. Make sure to backup any pre-existing data first, and always backup all important data.</p> <p>Note</p> <pre><code>#!/bin/bash\nset -e\nimage=${1:?Supply the .iso image of a Gentoo installation medium}\ntarget=${2:?Supply the target device}\n\necho Checking for the necessary tools presence...\nwhich syslinux\nwhich sfdisk\nwhich mkfs.vfat\n\necho Mounting Gentoo CD image...\ncdmountpoint=/mnt/gentoo-cd\nmkdir -p \"$cdmountpoint\"\ntrap 'echo Unmounting Gentoo CD image...; umount \"$cdmountpoint\"' EXIT\nmount -o loop,ro \"$image\" \"$cdmountpoint\"\n\necho Creating a disk-wide EFI FAT partition on \"$target\"...\necho ',,U,*' | sfdisk --wipe always \"$target\"\n\necho Installing syslinux MBR on \"$target\"...\ndd if=/usr/share/syslinux/mbr.bin of=\"$target\"\nsleep 1\n\necho Creating file system on \"$target\"1...\nmkfs.vfat \"$target\"1 -n GENTOO\n\necho Mounting file system...\nmountpoint=/mnt/gentoo-usb\nmkdir -p \"$mountpoint\"\nmount \"$target\"1 \"$mountpoint\"\n\necho Copying files...\ncp -r \"$cdmountpoint\"/* \"$mountpoint\"/\nmv \"$mountpoint\"/isolinux/* \"$mountpoint\"\nmv \"$mountpoint\"/isolinux.cfg \"$mountpoint\"/syslinux.cfg\nrm -rf \"$mountpoint\"/isolinux*\nmv \"$mountpoint\"/memtest86 \"$mountpoint\"/memtest\nsed -i -e \"s:cdroot:cdroot slowusb:\" -e \"s:kernel memtest86:kernel memtest:\" \"$mountpoint\"/syslinux.cfg\n\necho Unmounting file system...\numount \"$mountpoint\"\n\necho Installing syslinux on \"$target\"1\nsyslinux \"$target\"1\n\necho Syncing...\nsync\n\necho 'Done!'\n</code></pre>"},{"location":"Linux/Gentoo/Network/","title":"Network","text":""},{"location":"Linux/Gentoo/Network/#determine-interface-names","title":"Determine interface names","text":"<pre><code>root #ifconfig\n\neth0      Link encap:Ethernet  HWaddr 00:50:BA:8F:61:7A\n          inet addr:192.168.0.2  Bcast:192.168.0.255  Mask:255.255.255.0\n          inet6 addr: fe80::50:ba8f:617a/10 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:1498792 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:1284980 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:1984 txqueuelen:100\n          RX bytes:485691215 (463.1 Mb)  TX bytes:123951388 (118.2 Mb)\n          Interrupt:11 Base address:0xe800\n</code></pre>"},{"location":"Linux/Gentoo/Network/#configure-the-proxies","title":"Configure the proxies","text":"<pre><code>#http_proxy\nroot #export http_proxy=\"http://proxy.gentoo.org:8080\"\n#ftp_proxy\nroot #export ftp_proxy=\"ftp://proxy.gentoo.org:8080\"\n#rsync_proxy\nroot #export RSYNC_PROXY=\"proxy.gentoo.org:8080\"\n</code></pre>"},{"location":"Linux/Gentoo/Network/#netowork-automatic-config-tool","title":"Netowork automatic config Tool","text":"<pre><code>root #net-setup eth0\n</code></pre>"},{"location":"Linux/Gentoo/Network/#dhcp","title":"DHCP","text":"<pre><code>root #dhcpcd eth0\n</code></pre>"},{"location":"Linux/Gentoo/Network/#preparing-the-wireless-access","title":"Preparing the wireless access","text":"<p><pre><code>root #iw dev wlp9s0 info\nInterface wlp9s0\n  ifindex 3\n  wdev 0x1\n  addr 00:00:00:00:00:00\n  type managed\n  wiphy 0\n  channel 11 (2462 MHz), width: 20 MHz (no HT), center1: 2462 MHz\n  txpower 30.00 dBm\n</code></pre> <pre><code>root #iw dev wlp9s0 link\nNot connected.\n</code></pre> <pre><code>root #ip link set dev wlp9s0 up\n</code></pre></p> <p>If the wireless network is set up with WPA or WPA2, then wpa_supplicant needs to be used. For more information on configuring wireless networking in Gentoo Linux, please read the Wireless networking chapter in the Gentoo Handbook.</p>"},{"location":"Linux/Gentoo/Prepare%20the%20disks/","title":"Prepare the disks","text":"<p>GUID Partition Table - he GUID Partition Table (GPT) setup (also called GPT disklabel) uses 64-bit identifiers for the partitions. The location in which it stores the partition information is much bigger than the 512 bytes of the MBR partition table (DOS disklabel), which means there is practically no limit on the amount of partitions for a GPT disk. Also the size of a partition is bounded by a much greater limit (almost 8 ZiB - yes, zebibytes).</p>"},{"location":"Linux/Gentoo/Prepare%20the%20disks/#default-partitioning-scheme","title":"Default partitioning scheme","text":"Partition Filesystem Size Description <code>/dev/sda1</code> fat32 (UEFI) or ext4 (BIOS - aka Legacy boot) 256M Boot/EFI system partition <code>/dev/sda2</code> swap RAM size *2 Swap partition <code>/dev/sda3</code> ext4 Rest of the disk Root partition"},{"location":"Linux/Gentoo/Prepare%20the%20disks/#filesystems","title":"Filesystems","text":"<p>btrfs</p> <p>A next generation filesystem that provides many advanced features such as snapshotting, self-healing through checksums, transparent compression, subvolumes, and integrated RAID. Kernels prior to 5.4.y are not guaranteed to be safe to use with btrfs in production because fixes for serious issues are only present in the more recent releases of the LTS kernel branches. Filesystem corruption issues are common on older kernel branches, with anything older than 4.4.y being especially unsafe and prone to corruption. Corruption is more likely on older kernels (than 5.4.y) when compression is enabled. RAID \u215a and quota groups unsafe on all versions of btrfs. Furthermore, btrfs can counter-intuitively fail filesystem operations with ENOSPC when df reports free space due to internal fragmentation (free space pinned by DATA + SYSTEM chunks, but needed in METADATA chunks). Additionally, a single 4K reference to a 128M extent inside btrfs can cause free space to be present, but unavailable for allocations. This can also cause btrfs to return ENOSPC when free space is reported by df. Installing sys-fs/btrfsmaintenance and configuring the scripts to run periodically can help to reduce the possibility of ENOSPC issues by rebalancing btrfs, but it will not eliminate the risk of ENOSPC when free space is present. Some workloads will never hit ENOSPC while others will. If the risk of ENOSPC in production is unacceptable, you should use something else. If using btrfs, be certain to avoid configurations known to have issues. With the exception of ENOSPC, information on the issues present in btrfs in the latest kernel branches is available at the btrfs status page.</p> Filesystem Creation command On minimal CD\uff1f Package btrfs mkfs.btrfs Yes sys-fs/btrfs-progs ext4 mkfs.ext4 Yes sys-fs/e2fsprogs <ul> <li>EFI system partition partition (/dev/sda1) as FAT32 and the root partition (/dev/sda3) as ext4 as used in the example partition structure <pre><code>root #mkfs.vfat -F 32 /dev/sda1\nroot #mkfs.ext4 /dev/sda3\nroot #mkswap /dev/sda2\n</code></pre></li> <li> <p>To activate the swap partition <pre><code>root #swapon /dev/sda2\n</code></pre></p> </li> <li> <p>Mounting the root partition <pre><code>root #mkdir --parents /mnt/gentoo\nroot #mount /dev/sda3 /mnt/gentoo\n</code></pre></p> </li> </ul>"},{"location":"Linux/Gentoo/Stage3/","title":"Installing stage3","text":""},{"location":"Linux/Gentoo/Stage3/#installing-a-stage-tarball","title":"Installing a stage tarball","text":""},{"location":"Linux/Gentoo/Stage3/#verify-the-current-date-and-time-by-running-the-date-command","title":"Verify the current date and time by running the date command:","text":"<pre><code>root #date\n\nMon Oct  3 13:16:22 PDT 2021\n</code></pre> <p>Official Gentoo live environments include the chronyd command (available through the net-misc/chrony package) and a configuration file pointing to ntp.org time servers. It can be used to automatically sync the system clock to UTC time using a time server. Using this method requires a working network configuration and may not be available on all architectures. <pre><code>root #chronyd -q\n</code></pre></p>"},{"location":"Linux/Gentoo/Stage3/#choosing-a-stage-tarball","title":"Choosing a stage tarball","text":"<p>Multilib (32 and 64-bit)</p> <p>Multilib</p> <p>Choosing a base tarball for the system can save a considerable amount of time later on in the installation process, specifically when it is time to choose a system profile. The selection of a stage tarball will directly impact future system configuration and can save a headache or two later on down the line. The multilib tarball uses 64-bit libraries when possible, and only falls back to the 32-bit versions when necessary for compatibility. This is an excellent option for the majority of installations because it provides a great amount of flexibility for customization in the future. Those who desire their systems to be capable of easily switching profiles should download the multilib tarball option for their respective processor architecture.</p> <p>Most users should not use the 'advanced' tarballs options; they are for specific software or hardware configurations.</p> <p>No-multilib(pure 64-bit)</p> <p>No-Multilib</p> <p>Selecting a no-multilib tarball to be the base of the system provides a complete 64-bit operating system environment. This effectively renders the ability to switch to multilib profiles improbable, although still technically possible.</p> Warning <p>Readers who are just starting out with Gentoo should not choose a no-multilib tarball unless it is absolutely necessary. Migrating from a no-multilib to a multilib system requires an extremely well-working knowledge of Gentoo and the lower-level toolchain (it may even cause our Toolchain developers to shudder a little). It is not for the faint of heart and is beyond the scope of this guide.</p>"},{"location":"Linux/Gentoo/Stage3/#openrc","title":"OpenRC","text":"<p>OpenRC is a dependency-based init system (responsible for starting up system services once the kernel has booted) that maintains compatibility with the system provided init program, normally located in /sbin/init. It is Gentoo's native and original init system, but is also deployed by a few other Linux distributions and BSD systems.</p> <p>OpenRC does not function as a replacement for the /sbin/init file by default and is 100% compatible with Gentoo init scripts. This means a solution can be found to run the dozens of daemons in the Gentoo ebuild repository.</p>"},{"location":"Linux/Gentoo/Stage3/#systemd","title":"systemd","text":"<p>systemd is a modern SysV-style init and rc replacement for Linux systems. It is used as the primary init system by a majority of Linux distributions. systemd is fully supported in Gentoo and works for its intended purpose. If something seems lacking in the Handbook for a systemd install path, review the systemd article before asking for support.</p>"},{"location":"Linux/Gentoo/Stage3/#donwloading-the-stage-tarball","title":"Donwloading the stage tarball","text":"<ul> <li> <p>Graphical browsers <pre><code>root #cd /mnt/gentoo\nroot #wget &lt;PASTED_STAGE_URL&gt;\n</code></pre></p> </li> <li> <p>Command-line browsers <pre><code>root #links https://www.gentoo.org/downloads/mirrors/\nroot #links -http-proxy proxy.server.com:8080 https://www.gentoo.org/downloads/mirrors/\nroot #lynx https://www.gentoo.org/downloads/mirrors/\n</code></pre></p> </li> <li>Verifying and validating  <pre><code>root #openssl dgst -r -sha512 stage3-amd64-&lt;release&gt;-&lt;init&gt;.tar.xz\nroot #sha512sum stage3-amd64-&lt;release&gt;-&lt;init&gt;.tar.xz\nroot #openssl dgst -r -whirlpool stage3-amd64-&lt;release&gt;-&lt;init&gt;.tar.xz\n</code></pre></li> </ul>"},{"location":"Linux/Gentoo/Stage3/#unpacking-the-stage-tarball","title":"Unpacking the stage tarball","text":"<pre><code>root #tar xpvf stage3-*.tar.xz --xattrs-include='*.*' --numeric-owner\n</code></pre>"},{"location":"Linux/Gentoo/Stage3/#configuring-compile-options","title":"Configuring compile options","text":"<p>To optimize the system, it is possible to set variables which impact the behavior of Portage, Gentoo's officially supported package manager. All those variables can be set as environment variables (using export) but setting via export is not permanent. <pre><code>root #nano -w /mnt/gentoo/etc/portage/make.conf\n</code></pre></p>"},{"location":"Linux/Gentoo/Stage3/#cflags-and-cxxflags","title":"CFLAGS and CXXFLAGS","text":"<p>The CFLAGS and CXXFLAGS variables define the optimization flags for GCC C and C++ compilers respectively. Although those are defined generally here, for maximum performance one would need to optimize these flags for each program separately. The reason for this is because every program is different. However, this is not manageable, hence the definition of these flags in the make.conf file.</p> <p>In make.conf one should define the optimization flags that will make the system the most responsive generally. Don't place experimental settings in this variable; too much optimization can make programs misbehave (crash, or even worse, malfunction).</p> <p>To understand them all, read the GNU Online Manual(s) or the gcc info page (info gcc - only works on a working Linux system). The make.conf.example file itself also contains lots of examples and information; don't forget to read it too.</p> <p>A first setting is the -march= or -mtune= flag, which specifies the name of the target architecture. Possible options are described in the make.conf.example file (as comments). A commonly used value is native as that tells the compiler to select the target architecture of the current system (the one users are installing Gentoo on).</p> <p>A second one is the -O flag (that is a capital O, not a zero), which specifies the gcc optimization class flag. Possible classes are s (for size-optimized), 0 (zero - for no optimizations), 1, 2 or even 3 for more speed-optimization flags (every class has the same flags as the one before, plus some extras). -O2 is the recommended default. -O3 is known to cause problems when used system-wide, so we recommend to stick to -O2.</p> <p>Another popular optimization flag is -pipe (use pipes rather than temporary files for communication between the various stages of compilation). It has no impact on the generated code, but uses more memory. On systems with low memory, gcc might get killed. In that case, do not use this flag.</p> <p>Using -fomit-frame-pointer (which doesn't keep the frame pointer in a register for functions that don't need one) might have serious repercussions on the debugging of applications.</p> <p>When the CFLAGS and CXXFLAGS variables are defined, combine the several optimization flags in one string. The default values contained in the stage3 archive that is unpacked should be good enough. The following one is just an example: <pre><code># Compiler flags to set for all languages\nCOMMON_FLAGS=\"-march=native -O2 -pipe\"\n# Use the same settings for both variables\nCFLAGS=\"${COMMON_FLAGS}\"\nCXXFLAGS=\"${COMMON_FLAGS}\"\n</code></pre></p> <p>Although the GCC optimization article has more information on how the various compilation options can affect a system, the Safe CFLAGS article may be a more practical place for beginners to start optimizing their systems.</p>"},{"location":"Linux/Gentoo/Stage3/#makeopts","title":"MAKEOPTS","text":"<p>The MAKEOPTS variable defines how many parallel compilations should occur when installing a package. As of Portage version 3.0.31<sup>1</sup>, if left undefined, Portage's default behavior is to set the MAKEOPTS value to the same number of threads returned by nproc.</p> <p>A good choice is the smaller of: the number of threads the CPU has, or the total amount of system RAM divided by 2 GiB.</p> Warning <p>Using a large number of jobs can significantly impact memory consumption. A good recommendation is to have at least 2 GiB of RAM for every job specified (so, e.g. -j6 requires at least 12 GiB). To avoid running out of memory, lower the number of jobs to fit the available memory.</p> <p>When using parallel emerges (--jobs), the effective number of jobs run can grow exponentially (up to make jobs multiplied by emerge jobs). This can be worked around by running a localhost-only distcc configuration that will limit the number of compiler instances per host.:warning</p> <p>/etc/portage/make.conf</p> <p>If left undefined, Portage's default behavior is to set the MAKEOPTS value to the same number of threads returned by <code>nproc</code> <pre><code># Example MAKEOPTS declaration\nMAKEOPTS=\"-j4\"\n</code></pre></p> <ol> <li> <p>https://gitweb.gentoo.org/proj/portage.git/commit/?id=5d2af567772bb12b073f1671daea6263055cbdc2 \u21a9</p> </li> </ol>"},{"location":"Linux/Gentoo/System/","title":"Configuring the system","text":""},{"location":"Linux/Gentoo/System/#filesystem-information","title":"Filesystem information","text":""},{"location":"Linux/Gentoo/System/#creating-the-fstab-file","title":"Creating the fstab file","text":"<p>The /etc/fstab file uses a table-like syntax. Every line consists of six fields, separated by whitespace (space(s), tabs, or a mixture of the two). Each field has its own meaning:</p> <ol> <li>The first field shows the block special device or remote filesystem to be mounted. Several kinds of device identifiers are available for block special device nodes, including paths to device files, filesystem labels and UUIDs, and partition labels and UUIDs.</li> <li>The second field shows the mount point at which the partition should be mounted.</li> <li>The third field shows the type of filesystem used by the partition.</li> <li>The fourth field shows the mount options used by mount when it wants to mount the partition. As every filesystem has its own mount options, so system admins are encouraged to read the mount man page (man mount) for a full listing. Multiple mount options are comma-separated.</li> <li>The fifth field is used by dump to determine if the partition needs to be dumped or not. This can generally be left as 0 (zero).</li> <li>The sixth field is used by fsck to determine the order in which filesystems should be checked if the system wasn't shut down properly. The root filesystem should have 1 while the rest should have 2 (or 0 if a filesystem check is not necessary).</li> </ol> <pre><code>root #nano /etc/fstab\n</code></pre>"},{"location":"Linux/Gentoo/System/#filesystem-labels-and-uuids","title":"Filesystem labels and UUIDs","text":"<p>Both MBR (BIOS) and GPT include support for filesystem labels and filesystem UUIDs. These attributes can be defined in /etc/fstab as alternatives for the mount command to use when attempting to find and mount block devices. Filesystem labels and UUIDs are identified by the LABEL and UUID prefix and can be viewed with the blkid command:</p> <pre><code>root #blkid\n</code></pre> <p>Danger</p> <p>If the filesystem inside a partition is wiped, then the filesystem label and the UUID values will be subsequently altered or removed.</p> <p>Because of uniqueness, readers that are using an MBR-style partition table are recommended to use UUIDs over labels to define mountable volumes in /etc/fstab.</p> <p>Info</p> <p>UUIDs of the filesystem on a LVM volume and its LVM snapshots are identical, therefore using UUIDs to mount LVM volumes should be avoided.</p>"},{"location":"Linux/Gentoo/System/#partition-labels-and-uuids","title":"Partition labels and UUIDs","text":"<p>Users who have gone the GPT route have a couple more 'robust' options available to define partitions in /etc/fstab. Partition labels and partition UUIDs can be used to identify the block device's individual partition(s), regardless of what filesystem has been chosen for the partition itself. Partition labels and UUIDs are identified by the PARTLABEL and PARTUUID prefixes respectively and can be viewed nicely in the terminal by running the blkid command:</p> <pre><code>root #blkid\n</code></pre> <p>Below is a more elaborate example of an /etc/fstab file:</p> <p>A full /etc/fstab example</p> <p>FILE /etc/fstab</p> <p>Adjust any formatting difference and additional partitions created from the Preparing the disks step</p> filesystem dir type options jump pass /dev/sda1 /boot vfat defaults,noatime 0 2 /dev/sda2 none swap sw 0 0 /dev/sda3 / ext4 noatime 0 1 /dev/cdrom /mnt/cdrom auto noauto,user 0 0 <p>When {++auto++} is used in the third field, it makes the mount command guess what the filesystem would be. This is recommended for removable media as they can be created with one of many filesystems. The {++user++} option in the fourth field makes it possible for non-root users to mount the CD.</p> <p>To improve performance, most users would want to add the {++noatime++} mount option, which results in a faster system since access times are not registered (those are not needed generally anyway). This is also recommended for systems with solid state drives (SSDs).</p> Tip <p>Due to degradation in performance, defining the discard mount option in /etc/fstab is not recommended. It is generally better to schedule block discards on a periodic basis using a job scheduler such as cron or a timer (systemd). See Periodic fstrim jobs for more information.</p>"},{"location":"Linux/Gentoo/System/#networking-information","title":"Networking information","text":"<p>For systems running OpenRC, a more detailed reference for network setup is available in the advanced network configuration section, which is covered near the end of the handbook. Systems with more specific network needs may need to skip ahead, then return here to continue with the rest of the installation.</p> <p>For more specific systemd network setup, please review see the networking portion of the systemd article.</p>"},{"location":"Linux/Gentoo/System/#hostname","title":"Hostname","text":""},{"location":"Linux/Gentoo/System/#set-the-hostname-openrc-or-systemd","title":"Set the hostname (OpenRC or systemd)","text":"<pre><code>root #echo tux &gt; /etc/hostname\n</code></pre>"},{"location":"Linux/Gentoo/System/#systemd","title":"systemd","text":"<pre><code>root #hostnamectl hostname tux\n</code></pre>"},{"location":"Linux/Gentoo/System/#network","title":"Network","text":""},{"location":"Linux/Gentoo/System/#dhcp-via-dhcpcd-any-init-system","title":"DHCP via dhcpcd (any init system)","text":"<pre><code>root #emerge --ask net-misc/dhcpcd\n</code></pre> <p>To enable and then start the service on OpenRC systems:</p> <pre><code>root #rc-update add dhcpcd default\nroot #rc-service dhcpcd start\n</code></pre> <p>To enable and start the service on systmd system:</p> <pre><code>root #systemctl enable --now dhcpcd\n</code></pre> <p>All networking information is gathered in /etc/conf.d/net. It uses a straightforward - yet perhaps not intuitive - syntax. Do not fear! Everything is explained below. A fully commented example that covers many different configurations is available in /usr/share/doc/netifrc-*/net.example.bz2. First install net-misc/netifrc</p> <pre><code>root #emerge --ask --noreplace net-misc/netifrc\n</code></pre> <p>If the network connection needs to be configured because of specific DHCP options or because DHCP is not used at all, then open /etc/conf.d/net:</p> <pre><code>root #nano /etc/conf.d/net\n</code></pre> <p>Set both config_eth0 and routes_eth0 to enter IP address information and routing information:</p> <p>This assumes that the network interface will be called eth0. This is, however, very system dependent. It is recommended to assume that the interface is named the same as the interface name when booted from the installation media if the installation media is sufficiently recent. More information can be found in the Network interface naming section.</p> <p>Static IP definition</p> <p>FILE  /etc/conf.d/net</p> <p>config_eth0=\"192.168.0.2 netmask 255.255.255.0 brd 192.168.0.255\"</p> <p>routes_eth0=\"default via 192.168.0.1\"</p> <p>To use DHCP, define config_eth0:</p> <p>DHCP definition</p> <p>FILE  /etc/conf.d/net</p> <p>config_eth0=\"dhcp\"</p> <p>Go over /usr/share/doc/netifrc-*/net.example.bz2 for a list of additional configuration options. Be sure to also read up on the DHCP client man page if specific DHCP options need to be set.</p>"},{"location":"Linux/Gentoo/System/#automatically-start-networking-at-boot","title":"Automatically start networking at boot","text":"<pre><code>root #cd /etc/init.d\nroot #ln -s net.lo net.eth0\nroot #rc-update add net.eth0 default\n</code></pre> <p>Infor</p> <p>If the system has several network interfaces, then the appropriate net.* files need to be created just like we did with net.eth0.</p> <p>If, after booting the system, it is discovered the network interface name (which is currently documented as eth0) was wrong, then execute the following steps to rectify:</p> <ol> <li>Update the /etc/conf.d/net file with the correct interface name (like enp3s0 or enp5s0, instead of eth0).</li> <li>Create new symbolic link (like /etc/init.d/net.enp3s0).</li> <li>Remove the old symbolic link (rm /etc/init.d/net.eth0).</li> <li>Add the new one to the default runlevel.</li> <li>Remove the old one using rc-update del net.eth0 default.</li> </ol>"},{"location":"Linux/Gentoo/System/#the-hosts-file","title":"The hosts file","text":"<pre><code>root #nano /etc/hosts\n</code></pre> <p>Filling in the network information</p> <p>FILE  /etc/hosts</p> <p># This defines the current system and must be set</p> <p>127.0.0.1     tux.homenetwork tux localhost</p> <p># Optional definition of extra systems on the network</p> <p>192.168.0.5   jenny.homenetwork jenny  192.168.0.6   benny.homenetwork benny</p>"},{"location":"Linux/Gentoo/System/#system-information","title":"System information","text":""},{"location":"Linux/Gentoo/System/#root-password","title":"Root password","text":"<pre><code>root #passwd\n</code></pre>"},{"location":"Linux/Gentoo/System/#init-and-boot-configuration","title":"Init and boot configuration","text":"<p>When using OpenRC with Gentoo, it uses /etc/rc.conf to configure the services, startup, and shutdown of a system. Open up /etc/rc.conf and enjoy all the comments in the file. Review the settings and change where needed.</p>"},{"location":"Linux/Gentoo/System/#openrc","title":"OpenRC","text":"<pre><code>root #nano /etc/rc.conf\n</code></pre> <p>Next, open /etc/conf.d/keymaps to handle keyboard configuration. Edit it to configure and select the right keyboard.</p> <pre><code>root #nano /etc/conf.d/keymaps\n</code></pre> <p>Take special care with the keymap variable. If the wrong keymap is selected, then weird results will come up when typing on the keyboard.</p> <p>Finally, edit /etc/conf.d/hwclock to set the clock options. Edit it according to personal preference.</p> <pre><code>root #nano /etc/conf.d/hwclock\n</code></pre> <p>If the hardware clock is not using UTC, then it is necessary to set clock=\"local\" in the file. Otherwise the system might show clock skew behavior. systemd</p>"},{"location":"Linux/Gentoo/System/#systemd_1","title":"systemd","text":"<p>First, it is recommended to run systemd-firstboot which will prepare various components of the system are set correctly for the first boot into the new systemd environment. The passing the following options will include a prompt for the user to set a locale, timezone, hostname, root password, and root shell values. It will also assign a random machine ID to the installation:</p> <pre><code>root #systemd-firstboot --prompt --setup-machine-id\nroot #systemctl preset-all --preset-mode=enable-only\nroot #systemctl preset-all\n</code></pre>"},{"location":"Linux/Gentoo/Tools/","title":"Installing tools","text":""},{"location":"Linux/Gentoo/Tools/#system-logger","title":"System logger","text":""},{"location":"Linux/Gentoo/Tools/#openrc","title":"OpenRC","text":"<p>Some tools are missing from the stage3 archive because several packages provide the same functionality. It is now up to the user to choose which ones to install. Gentoo offers several system logger utilities. A few of these include:</p> <ul> <li>app-admin/sysklogd - Offers the traditional set of system logging daemons. The default logging configuration works well out of the box which makes this package a good option for beginners.</li> <li>app-admin/syslog-ng - An advanced system logger. Requires additional configuration for anything beyond logging to one big file. More advanced users may choose this package based on its logging potential; be aware additional configuration is a necessity for any kind of smart logging.</li> <li>app-admin/metalog - A highly-configurable system logger.</li> </ul> <p>There may be other system logging utilities available through the Gentoo ebuild repository as well, since the number of available packages increases on a daily basis.</p> <p>If syslog-ng is going to be used, it is recommended to install and configure logrotate. syslog-ng does not provide any rotation mechanism for the log files. Newer versions (&gt;= 2.0) of sysklogd however handle their own log rotation.</p> <p>To install the system logger of choice, emerge it. On OpenRC, add it to the default runlevel using rc-update. The following example installs and activates app-admin/sysklogd as the system's syslog utility:</p> <pre><code>root #emerge --ask app-admin/sysklogd\nroot #rc-update add sysklogd default\n</code></pre>"},{"location":"Linux/Gentoo/Tools/#systemd","title":"systemd","text":"<p>While a selection of logging mechanisms are presented for OpenRC-based systems, systemd includes a built-in logger called the systemd-journald service. The systemd-journald service is capable of handling most of the logging functionality outlined in the previous system logger section. That is to say, the majority of installations that will run systemd as the system and service manager can safely skip adding a additional syslog utilities.</p> <p>See man journalctl for more details on using journalctl to query and review the systems logs.</p> <p>For a number of reasons, such as the case of forwarding logs to a central host, it may be important to include redundant system logging mechanisms on a systemd-based system. This is a irregular occurrence for the handbook's typical audience and considered an advanced use case. It is therefore not covered by the handbook.</p>"},{"location":"Linux/Gentoo/Tools/#file-indexing","title":"File indexing","text":"<p>In order to index the file system to provide faster file location capabilities, install sys-apps/mlocate.</p>"},{"location":"Linux/Gentoo/Tools/#remote-shell-access","title":"Remote shell access","text":"<p>opensshd's default configuration does not allow root to login as a remote user. Please create a non-root user and configure it appropriately to allow access post-installation if required, or adjust /etc/ssh/sshd_config to allow root.</p> <p>To be able to access the system remotely after installation, sshd must be configured to start on boot.</p>"},{"location":"Linux/Gentoo/Tools/#openrc_1","title":"OpenRC","text":"<p>To add the sshd init script to the default runlevel on OpenRC: <pre><code>root #rc-update add sshd default\n</code></pre></p> <p>If serial console access is needed (which is possible in case of remote servers), agetty must be configured. Uncomment the serial console section in /etc/inittab: <pre><code>root #nano -w /etc/inittab\n\n# SERIAL CONSOLES\ns0:12345:respawn:/sbin/agetty 9600 ttyS0 vt100\ns1:12345:respawn:/sbin/agetty 9600 ttyS1 vt100\n</code></pre></p>"},{"location":"Linux/Gentoo/Tools/#systemd_1","title":"systemd","text":"<p>To enable the SSH server, run: <pre><code>root #systemctl enable sshd\nroot #systemctl enable getty@tty1.service\n</code></pre></p>"},{"location":"Linux/Gentoo/Tools/#time-synchronization","title":"Time synchronization","text":"<p>It is important to use some method of synchronizing the system clock. This is usually done via the NTP protocol and software. Other implementations using the NTP protocol exist, like Chrony. <pre><code>root #emerge --ask net-misc/chrony\n</code></pre></p>"},{"location":"Linux/Gentoo/Tools/#openrc_2","title":"OpenRC","text":"<pre><code>root #rc-update add chronyd default\n</code></pre>"},{"location":"Linux/Gentoo/Tools/#systemd_2","title":"systemd","text":"<pre><code>root #systemctl enable chronyd.service\nroot #systemctl enable systemd-timesyncd.service\n</code></pre>"},{"location":"Linux/Gentoo/Tools/#filesystem-tools","title":"Filesystem tools","text":"<p>Depending on the filesystems used, it is necessary to install the required file system utilities (for checking the filesystem integrity, creating additional file systems etc.). Note that tools for managing ext4 filesystems (sys-fs/e2fsprogs) are already installed as a part of the @system set.</p> <p>The following table lists the tools to install if a certain filesystem is used:</p> Filesystem Package Ext4 sys-fs/e2fsprogs XFS sys-fs/xfsprogs ReiserFS sys-fs/reisefsprogs JFS sys-fs/jfsutils VFAT(FAT32,...) sys-fs/dosfstools Btrfs sys-fs/btrfs-progs ZFS sys-fs/zfs <p>For more information on filesystems in Gentoo see the filesystem article.</p>"},{"location":"Linux/Gentoo/Tools/#networking-tools","title":"Networking tools","text":"<p>If networking was previously configured in the Configuring the system step and network setup is complete, then this 'networking tools' section can be safely skipped. In this case, proceed with the section on Configuring a bootloader.</p>"},{"location":"Linux/Gentoo/Tools/#installing-a-dhcp-client","title":"Installing a DHCP client","text":"<p>Warning</p> <p>Most users will need a DHCP client to connect to their network. If none was installed, then the system might not be able to get on the network thus making it impossible to download a DHCP client afterwards.</p> <pre><code>root #emerge --ask net-misc/dhcpcd\n</code></pre>"},{"location":"Linux/Gentoo/Tools/#installing-a-pppoe-client","title":"Installing a PPPoE client","text":"<pre><code>root #emerge --ask net-dialup/ppp\n</code></pre>"},{"location":"Linux/Gentoo/Tools/#installing-wireless-networking-tools","title":"Installing wireless networking tools","text":"<p>If the system will be connecting to wireless networks, install the net-wireless/iw package for Open or WEP networks and/or the net-wireless/wpa_supplicant package for WPA or WPA2 networks. iw is also a useful basic diagnostic tool for scanning wireless networks. <pre><code>root #emerge --ask net-wireless/iw net-wireless/wpa_supplicant\n</code></pre></p>"},{"location":"Linux/Network/NetCat/","title":"NetCat","text":""},{"location":"Linux/Network/NetCat/#netcat-or-nc-is-a-command-line-utility-that-reads-and-writes-data-across-network-connections-using-the-tcp-or-udp-protocols-it-is-one-of-the-most-powerful-tools-in-the-network-and-system-administrators-arsenal-and-it-as-considered-as-a-swiss-army-knife-of-networking-tools","title":"Netcat (or nc) is a command-line utility that reads and writes data across network connections, using the TCP or UDP protocols. It is one of the most powerful tools in the network and system administrators arsenal, and it as considered as a Swiss army knife of networking tools.","text":""},{"location":"Linux/Network/NetCat/#syntax","title":"Syntax","text":"<pre><code>nc [option] host port\n</code></pre> <p>By default, Netcat will attempt to start a TCP connection to the specified host and port. If you would like toestablish a UDP connection, use the -u option: <pre><code>nc -u host port\n</code></pre></p>"},{"location":"Linux/Network/NetCat/#example","title":"Example","text":"<p>To scan for open ports in the range 20-80 you would use the following command: <pre><code>nc -zv $IP $PORT\n\n# Print only the lines with the open ports, filter the results with the grep command.\nnc -zv $IP $PORT 2&gt;&amp;1 | grep succeeded\n\n# Output\nConnection to 10.10.8.8 22 port [tcp/ssh] succeeded!\nConnection to 10.10.8.8 80 port [tcp/http] succeeded!\n</code></pre></p>"},{"location":"Linux/Network/NetCat/#_1","title":"Network","text":""},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2021/11/01/cloud-native-containerd/","title":"Cloud Native-Containerd","text":"<p>A daemon for Linux and Windows. It manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments and beyond.</p>"},{"location":"blog/2021/11/01/cloud-native-containerd/#architecture","title":"Architecture","text":""},{"location":"blog/2023/12/22/autommatic-https-certifications-for-services-on-internal-home-network/","title":"Autommatic HTTPS Certifications for Services on Internal Home Network","text":"<p>For internal home network without opening a port on your firewall, this article explains how to setup automatic HPPTS certifications via Let's Encrypt for services.</p>"},{"location":"blog/2023/12/22/autommatic-https-certifications-for-services-on-internal-home-network/#requirements","title":"Requirements","text":"<p>Today, encryption of data in transit via HTTPS is considered mandatory and is often easy to implement via the excellent free service Let\u2019s Encrypt. However, it typically requires that the service in question be reachable from the internet for domain verification purposes. Exposing internal services to the internet is often not desirable, however, as this makes the services attackable. Therefore looking for a way to obtain Let\u2019s Encrypt certificates without creating a security risk by opening firewall ports. In a nutshell:</p> <ul> <li>Internal services on a home network are to be made accessible via HTTPS without certificate warnings.</li> <li>No access to internal services from the internet.<ul> <li>No open ports. No open ports.</li> <li>No cloud-based tunneling service</li> </ul> </li> </ul>"},{"location":"blog/2023/12/22/autommatic-https-certifications-for-services-on-internal-home-network/#dns-challenge-caddy-to-the-rescue","title":"DNS Challenge + Caddy to the Rescue","text":"<p>Luckily, there is an alternative to the Let\u2019s Encrypt HTTP challenge: DNS. There\u2019s also an excellent free reverse proxy that happily handles everything related to HTTPS, including the Let\u2019s Encrypt DNS challenge: Caddy. This is how the setup works:</p> <ul> <li>Use a subdomain of a public DNS domain for the hostnames of our services.<ul> <li>For security reasons, the subdomain should be used exclusively for the purposes described in this article, not for email (see below).</li> </ul> </li> <li>HTTPS certificates are handled by a Caddy instance that acts as a reverse proxy.</li> <li>Caddy is configured to auto-manage Let\u2019s Encrypt certificates via the DNS challenge, which uses TXT records for verification.</li> </ul> Let\u2019s Encrypt DNS Challenge Explained <p>Here\u2019s what happens when a certificate is requested via the Let\u2019s Encrypt DNS challenge: -   The Let\u2019s Encrypt client creates a special _acme-challenge DNS TXT record. -   Let\u2019s Encrypt queries DNS for that record. If the record\u2019s data is in order, Let\u2019s Encrypt issues the certificate. -   The Let\u2019s Encrypt client deletes the _acme-challenge DNS TXT record as it\u2019s not needed any more.</p> Let\u2019s Encrypt DNS Challenge &amp; DNS Zone Security <ul> <li>API write access to the DNS record _acme-challenge is required for automatic renewal.</li> <li>Problem: most DNS providers don\u2019t have granular access control. API tokens are often valid for the entire zone or even account (Cloudflare). This would allow modification of existing MX records (redirecting your email).</li> <li>DNS alias mode could be a solution (but isn\u2019t).<ul> <li>In DNS alias mode, we\u2019d set up a second DNS zone used exclusively for ACME (Let\u2019s Encrypt) validation. API access is only required for this validation domain.</li> <li>We\u2019d set up a CNAME record to point from the main domain to the validation domain.</li> <li>Unfortunately, this doesn\u2019t work with Caddy\u2019s Cloudflare DNS module.</li> </ul> </li> <li>Instead, we use a DNS domain exclusively for the purposes described in this article (i.e., ACME challenges and, optionally, name resolution).</li> </ul>"},{"location":"blog/2023/12/22/autommatic-https-certifications-for-services-on-internal-home-network/#preparation","title":"Preparation","text":"<p>Cloudflare API Token</p> <p>A free Cloudflare account to manage the DNS domain for the hostnames of the services. Alternatively, you can use any DNS provider that\u2019s supported by Caddy (search the list of modules for dns.providers).</p> <p>In Cloudflare account, create an API token with the following properties for own Domain:</p> <ul> <li>Required permissions:<ul> <li>Zone - Zone -read</li> <li>Zone - DNS - edit</li> </ul> </li> <li>Scope: Include Specific zone<ul> <li>The own public domain is required for the zone resources.</li> </ul> </li> </ul> <p>DNS &amp; Name Resolution</p> <p>Host Name Resolution</p> <p>Let\u2019s Encrypt certificates are for hostname exclusively, IP addresses are not included (which would technically be possible). It is, therefore, not possible to access the services via HTTPS by IP address, e.g. <code>https://192.168.0.4</code>. Instead, to access the services by fully-qualified domain name (FQDN) and need a way to resolve those names into IP addresses. On this example, we need to add one A record for the whoami service that testing with the following:</p> \ud83d\udc81\u200d\u2642\ufe0f    DNS Resolve<pre><code>#Replace with the IP Address of the host that Docker running\nsubdomain.domain.com     IP Address\n\ne.g.\nwhoami.abc.com    192.168.0.4\n</code></pre>"},{"location":"blog/2023/12/22/autommatic-https-certifications-for-services-on-internal-home-network/#caddy-container-on-proxmos-ve","title":"Caddy Container on Proxmos VE","text":"<p>Run through the preparatory steps to set up Docker on the Proxmox host</p> <p>Preparation: Increase UDP Buffer Sizes for QUIC</p> <p>The QUIC protocol (implemented by Caddy) requires larger buffers than normally available in Linux (source). Add the following to /etc/sysctl.conf: <pre><code>net.core.rmem_max=2500000\nnet.core.wmem_max=2500000\n</code></pre></p> <p>Reboot and check the values with the following commands: <pre><code>sysctl net.core.rmem_max\nsysctl net.core.wmem_max\n</code></pre></p> <p>Dockerized Caddy Directory Structure</p> <p>This is what the directory structure will look like when it's done: <pre><code>/rpool/\n \u2514\u2500\u2500 data/\n     \u2514\u2500\u2500 docker/\n         \u2514\u2500\u2500 caddy/\n             \u251c\u2500\u2500 config/\n             \u251c\u2500\u2500 data/\n             \u251c\u2500\u2500 dockerfile-dns/\n                 \u2514\u2500\u2500 Dockerfile\n             \u251c\u2500\u2500 container-vars.env\n             \u251c\u2500\u2500 Caddyfile\n             \u2514\u2500\u2500 docker-compose.yml\n</code></pre></p> <p>Docker convention</p> <p>Create the <code>caddy</code> directory. The subdirectories <code>config</code> and <code>data</code> are created by docker compose on the first run.</p> <p>Customized Caddy Docker Image</p> <p>We need a custom Caddy Docker image that includes the Cloudflare DNS plugin, which is required for the Let\u2019s Encrypt DNS challenge. Create the file dockerfile-dns/Dockerfile with the following content: \ud83d\udc81\u200d\u2642\ufe0f Dockfile<pre><code>ARG VERSION=2\n\nFROM caddy:${VERSION}-builder AS builder\n\nRUN xcaddy build \\\n    --with github.com/caddy-dns/cloudflare\n\nFROM caddy:${VERSION}\n\nCOPY --from=builder /usr/bin/caddy /usr/bin/caddy\n</code></pre></p> <p>Comments</p> <p>The second FROM instruction \u2013 means produce a much smaller image by simply overlaying the newly-built binary on top of the regular caddy image</p> <p>Caddy container-vars.env File</p> <p>Everything that is specific to the deployment goes into the container-vars.env file. This includes domain names, IP addresses, API keys, email addresses, and so on.</p> <p>Create with the following content: <pre><code>MY_DOMAIN=home.yourdomain.com               # replace with your domain\nMY_HOST_IP=192.168.0.4                      # replace with your Docker host's IP address\nCLOUDFLARE_API_TOKEN=&lt;YOUR_TOKEN&gt;           # add your token\n</code></pre></p> <p>Caddy Docker Compose File</p> <p>Create docker-compose.yml with the following content: <sup>1</sup> <sup>2</sup> <sup>3</sup> <pre><code>services:\n\n  caddy:\n    build: ./dockerfile-dns\n    container_name: caddy\n    hostname: caddy\n    restart: unless-stopped\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n      - \"443:443/udp\"\n    networks:\n      - caddynet\n    env_file:\n      - container-vars.env\n    volumes:\n      - ./Caddyfile:/etc/caddy/Caddyfile:ro\n      - ./data:/data\n      - ./config:/config\n\n  whoami:\n    image: \"containous/whoami\"\n    container_name: \"whoami\"\n    hostname: \"whoami\"\n    networks:\n      - caddynet\n\nnetworks:\n\n  caddynet:\n    attachable: true\n    driver: bridge\n    driver_opts:\n      com.docker.network.bridge.name: docker_caddy\n    ipam:\n      driver: default\n      config:\n        - subnet: 172.19.0.0/16\n</code></pre></p> <p>Tip</p> <p>The <code>file</code> and <code>directories</code> in the section volumes are bind-mounted from the Docker host. Their content persists when the container is destroyed. <code>Caddyfile</code> is mounted read-only (<code>:ro</code>).</p> <p>The whoami service is created strictly for testing purposes. You can remove it once all are working as expected.</p> <p>Caddyfile for the whoami Container</p> <p>Create Caddy\u2019s configuration file Caddyfile in the same directory as the docker-compose.yml file and paste the following content: <pre><code>whoami.{$MY_DOMAIN} {\n    reverse_proxy whoami:80\n    tls {\n        dns cloudflare {env.CLOUDFLARE_API_TOKEN}\n    }\n}\n</code></pre></p> <p>Start the Containers</p> <p>Navigate into the directory with the docker-compose.yml file and run: <pre><code>docker compose up -d\n</code></pre></p> <p>Docker logs</p> <p>Inspect the container logs for errors with the command. <pre><code>docker compose logs --tail 30 --timestamps\n</code></pre></p> <p>Test</p> <p>Open <code>https://whoami.abc.com</code> in your browser. It should display without certificate warnings or errors.</p>"},{"location":"blog/2023/12/22/autommatic-https-certifications-for-services-on-internal-home-network/#proxmox-lets-encrypt-certificate","title":"Proxmox Let\u2019s Encrypt Certificate","text":"<p>Proxmox is accessible via HTTPS exclusively but comes, understandably, only with a self-signed certificate. Proxmox\u2019s built-in support for Let\u2019s Encrypt does not include the DNS challenge, but now have everything in place to use Caddy container to proxy access to the host\u2019s web interface, too.</p> <p>Proxmox Caddyfile</p> <p>Add the following to <code>Caddyfile</code>: <pre><code>proxmox.{$MY_DOMAIN} {\n    reverse_proxy {$MY_HOST_IP}:8006 {\n      transport http {\n         tls_insecure_skip_verify\n      }\n   }\n    tls {\n        dns cloudflare {env.CLOUDFLARE_API_TOKEN}\n    }\n}\n</code></pre> DNS A Record</p> <p>Add the following A record to DNS domain provider: \ud83d\udc81\u200d\u2640\ufe0f DNS Resolve<pre><code>#Replace with the IP address of the host that Docker runnning\ne.g\nproxmox.abc.com 192.168.0.4\n</code></pre></p> <p>Reload Caddy\u2019s Configuration</p> <p>Instruct Caddy to reload its configuration by running: <pre><code>docker exec -w /etc/caddy caddy caddy reload\n</code></pre></p> <p>Now be able to access the Proxmox web interface at <code>https://proxmox.abc.com</code> without getting a certificate warning from your browser.</p>"},{"location":"blog/2023/12/22/autommatic-https-certifications-for-services-on-internal-home-network/#adjust-proxmox-proxy-settings","title":"Adjust Proxmox Proxy Settings","text":"<p>Right now, it should actually be possible to reach the Proxmox web interface with HTTPS with valid SSL keys via port 443. Unfortunately the original Port 8006 is also still open. So let's going to fix that now.</p> <p>Edit the /etc/default/pveproxy configuration file with the following contents: \ud83d\udc81\u200d\u2642\ufe0f Pveproxy Config<pre><code>ALLOW_FROM=\"172.19.0.0/16\"\nDENY_FROM=\"all\"\nPOLICY=\"allow\"\n</code></pre></p> <p>Warning</p> <p>This configuration file tells the native Proxmox webserver to only accept requests from inside the same machine (172.19.0.0/16 -&gt; localhost), so only requests coming from Caddy, running as a neighbor so to speak, are actually processed.</p> <p>Finally make sure to restart pveproxy service: \ud83d\udc81\u200d\u2640\ufe0f Pveproxy service<pre><code>service pveproxy restart\n</code></pre> Congratulations! Now the Proxmox web interface should run on HTTPS Port 443 exclusively.</p> <p> Helge Klein --- Caddy Container on Proxmox</p> <ol> <li> <p>Added the option com.docker.network.bridge.name in <code>docker-compose.yml</code> to set the name of the network interface Docker creates for Caddy.<p>[2024-06-30]</p> <pre><code>networks:\n\n  caddynet:\n    driver_opts:\n      com.docker.network.bridge.name: docker_caddy\n</code></pre> \u21a9</p> </li> <li> <p>Configured a static IPv4 subnet to be used in firewall rules.<p>[2024-06-30]</p> <pre><code>networks:\n\n  caddynet:\n    ipam:\n      driver: default\n      config:\n        - subnet: 172.19.0.0/16\n</code></pre> \u21a9</p> </li> <li> <p>Removed the version from <code>docker-compose.yml</code>; a warning mentions that it\u2019s obsolete.<p>[2024-06-20]</p> <pre><code>version: \"3.9\"\n</code></pre> \u21a9</p> </li> </ol>"},{"location":"blog/2022/12/13/simple-caddy-what-is-caddy/","title":"What is Caddy?","text":"<p>At the most basic level, a web server like nginx or httpd. Like other web servers, caddy can serve content, or it can intercept web traffic to serve other sites (this is the core feature of a reverse proxy).at the most basic level, a web server like nginx or httpd. Like other web servers, caddy can serve content, or it can intercept web traffic to serve other sites (this is the core feature of a reverse proxy).</p> <p>Unlike other web servers, caddy has some nice features that make it more palatable as a host:</p> <ul> <li>Automatic SSL, whether you want to generate your own certs or use ACME servers</li> <li>Very simple syntax for writing our configs. Most sites require a single directive.</li> <li>Based on go, a modern language with memory safety controls</li> <li>Sane defaults. By default caddy sets up sites securely and (usually) without any additional configuration</li> <li>A single, static binary. While our implementation will be docker focused, nothing is stopping you from installing caddy as a normal package or even as an executable on Windows.</li> </ul>"},{"location":"blog/2022/12/13/simple-caddy-what-is-caddy/#caddy-config-file","title":"Caddy config file","text":"<p>\"Caddyfile\" on which caddy calls the config for use. The syntac for more details for read up.</p> <p>For inside, you can refer as offocial documentation</p>"},{"location":"blog/2022/12/13/simple-caddy-what-is-caddy/#example","title":"Example","text":"<pre><code>&lt;subdomain&gt;.&lt;domain&gt; {\n    reverse_proxy &lt;IP&gt;:&lt;Port&gt;\n}\n</code></pre>"},{"location":"blog/2022/12/13/simple-caddy-what-is-caddy/#acme-portocol","title":"ACME Portocol","text":"<p>How can we get a valid HTTPS certificate, signed by a certificate authority, so our site looks valid? Well we can do it the boring manual way, generating a Certificate Signing Request (CSR), having an authority sign that request, and returning a signed certificate. No thanks.</p> <p>Instead, the good people at let\u2019s encrypt developed the ACME protocol: a completely automated way to generate and provide signed certificates to sites like yours or mine. Let\u2019s encrypt (and other services like zeroSSL) even provide a completely free service to do this for us!</p> <p>The two most widely used options are the DNS and HTTP challenge methods.</p>"},{"location":"blog/2022/12/13/simple-caddy-what-is-caddy/#dns-acme-challenges","title":"DNS ACME Challenges","text":"<p>Pre-requisites:</p> <ul> <li>A Public DNS Domain, registered with a Domain Register. cloudflare is the recommendation.</li> <li>DNS provider to be available from the list of providers found in the dns.providers modules.</li> </ul>"},{"location":"blog/2022/12/13/simple-caddy-what-is-caddy/#setup-with-docker-service","title":"Setup with Docker service","text":"<ul> <li>Create the docker-compose.yml in the folder named as caddy. <pre><code>version: \"3.9\"\n\nservices:\n\n  caddy:\n    image: caddy:latest\n    container_name: caddy\n    hostname: caddy\n    restart: unless-stopped\n    security_opt:\n      - label:disable\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n      - \"443:443/udp\"\n    volumes:\n      - ./container-config:/etc/caddy\n      - /etc/localtime:/etc/localtime:ro\n</code></pre> <ul> <li>Bring it up with following way: <pre><code>$ docker compose up -d\n</code></pre></li> </ul>"},{"location":"blog/2024/06/29/intro/","title":"Intro","text":"<p>One painful part of keeping a dotfiles directory is managing symbolic links between your configuration files and the locations where they need to go in your home directory.</p>"},{"location":"blog/2024/06/29/intro/#whats-is-gnu-stow","title":"What's is GNU-Stow","text":"<p>GNU Stow is a symlink farm manager which takes distinct packages of software and/or data located in separate directories on the filesystem, and makes them appear to be installed in the same place.</p>"},{"location":"blog/2024/06/29/intro/#how-to-install","title":"How to install","text":"<pre><code># For Arch\nsudo pacman -Sy stow\n</code></pre>"},{"location":"blog/2024/06/29/intro/#basic-usage","title":"Basic Usage","text":"<p>Let\u2019s say we\u2019ve got our configuration files stored in a directory under the home directory called .dotfiles. We can easily create symbolic links to the files in this directory to the equivalent locations in the home directory using the following command:</p> \ud83d\udc81\u200d\u2640\ufe0f Basic Ref<pre><code># Create the soft link in the parent folder\nstow .\n\n# Example\nmkdir -p $HOME/dotfiles/ &amp;&amp; cd $_   # Create the dot file\nmkdir .config   # Prepare for .config folder for stow\nmv $HOME/.config/nvim $HOME/dotfiles/.config    # Move the folder that need for stow\nstow .  # Stow trigger for syslink\n</code></pre>"},{"location":"blog/2024/06/29/intro/#how-it-works","title":"How it works","text":"<p>GNU Stow walks the file and directory hierarchy of the directory passed as the first parameter to the stow command and creates symbolic links to those files in the equivalent locations in the target directory.</p> <p>The important thing to be aware of here is that our dotfiles directory must have the same layout as where the files would be placed under the home directory. This means you will need to have the equivalent subdirectory structure in your dotfiles folder so that all symbolic links get created in the right place.</p>"},{"location":"blog/2024/06/29/intro/#ignoring-files-and-directories","title":"Ignoring files and directories","text":"<p>By default, GNU Stow does a good job of ignoring common files and directories you might not want to be linked back to your home directory like README and LICENSE files, source control folders.</p> <p>To skip files like this, we can create a file in our dotfiles folder called .stow-local-ignore. Each line of this file should be a string or regular expression representing any file or directory you don\u2019t want to link to your home folder.</p> \ud83d\udc81\u200d\u2642\ufe0f    Ignore files and directories<pre><code># Example\n\\.git\nmisc\n#LICENSE\n^/.*\\.org\n</code></pre>"},{"location":"blog/2024/06/29/intro/#cleaning-up-symbolic-links","title":"Cleaning up symbolic links","text":"<p>If for some reason you\u2019d like to get rid of all the symbolic links that GNU Stow created in your home folder, you can do that with one extra parameter to the command we\u2019ve been running so far:</p> <pre><code>stow -D .   # Under that folder which did run \"stow .\" command before.\n</code></pre>"},{"location":"blog/2024/06/29/intro/#check-out-the-gnu-stow-manual","title":"Check out the GNU Stow manual","text":"<p>For more information about GNU Stow and details on other ways it can be used, check out the GNU-Stow Manual</p> <p> SYSTEM CRAFTERS</p>"},{"location":"blog/2022/06/10/groff--mom/","title":"Groff &amp; MOM","text":""},{"location":"blog/2022/06/10/groff--mom/#groff","title":"Groff","text":"<p>GNU troff (or groff) is a system for typesetting documents. troff is very flexible and has been used extensively for some thirty years. It is well entrenched in the Unix community.</p>"},{"location":"blog/2022/06/10/groff--mom/#mom","title":"MOM","text":"<p>A flexible typesetting and document formatting package that allows you to create high-quality Portable Document Format (.pdf) or PostScript (.ps) files for viewing and printing.) - mom is a flexible typesetting and document formatting package that allows you to create high-quality Portable Document Format (.pdf) or PostScript (.ps) files for viewing and printing.</p>"},{"location":"blog/2023/12/18/the-issue-on-proxmox-8/","title":"The issue on Proxmox 8:","text":"<p>MDS CPU bug present and SMT on, data leak possible. More Detials refer on Kernel.org</p>"},{"location":"blog/2023/12/18/the-issue-on-proxmox-8/#reproduced-condistion","title":"Reproduced Condistion:","text":"<ul> <li>Proxmox platform always reboot automatically espcially when trigger the vm no matter the CPUs or Memory in which assign on the configuration.</li> <li>Startup and work on for multiple VMs.</li> <li>Login to proxmox recovery mode.</li> <li>Keep VM running over night on UTF-8 time zone.</li> <li>Host hardware power on to grub mode for system preparation.</li> </ul>"},{"location":"blog/2023/12/18/the-issue-on-proxmox-8/#work-around","title":"Work around:","text":"<ul> <li>Disable the option on BIOS for intel hyper-threading if it has.</li> <li>Add \"mitigations=off\" feature to grub for ext file system. <pre><code>$ #/etc/default/grub\n$ GRUB_CMDLINE_LINUX_DEFAULT=\"quiet mitigations=off\"\n\n$ update-grub\n$ reboot\n</code></pre></li> <li>Modify for zfs file system. <pre><code>$ #/etc/kernel/cmdfile\n$ root=ZFS=rpool/ROOT/pve-1 boot=zfs mitigations=off\n\n$ proxmox-boot-tool refresh\n4 reboot\n</code></pre></li> </ul>"},{"location":"blog/2022/03/26/kvm-bridge/","title":"KVM Bridge","text":""},{"location":"blog/2022/03/26/kvm-bridge/#linux","title":"Linux","text":"<p>Use <code>nmtui</code> command to config the Bridge adapter on-premise.</p> <pre><code>NAME\nnmtui - Text User Interface for controlling NetworkManager\n</code></pre> <p>Click \"Edit a connection\" option to start walking through. </p> <p>Open the page once click \"Add\" button for \"Bridge\" creation</p> <p></p> <p></p> <p>Tips</p> <p>Leave the empty on the slave device for communication automatically by adapter and router.</p> <p>Delete the default wired connection and waiting a while. New Bridge connection will be activated soon later.</p> <pre><code>btctl show\n\nbridge name     bridge id               STP enabled     interfaces\nnm-bridge       ID                      status          adapterName\n</code></pre>"},{"location":"blog/2023/05/25/openwrt/","title":"OpenWRT","text":""},{"location":"blog/2023/05/25/openwrt/#parition-switch","title":"Parition switch","text":"<pre><code>root@lede:~# fw_printenv boot_part\nboot_part=1\n</code></pre> <pre><code>fw_setenv boot_part 2\nreboot\n</code></pre>"},{"location":"blog/2023/12/23/installing-docker/","title":"Installing Docker","text":""},{"location":"blog/2023/12/23/installing-docker/#install-docker-on-proxmox","title":"Install Docker on Proxmox","text":"<p>We\u2019re installing Docker along with Docker Compose from the official Docker repository according to the docs:</p> <p>Update: <pre><code>apt update\n</code></pre></p> <p>Install the required packages to access the Docker repository via HTTPS: <pre><code>apt install ca-certificates curl gnupg lsb-release\n</code></pre></p> <p>Add Docker\u2019s GPG key: <pre><code>mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n</code></pre></p> <p>Add Docker\u2019s stable repository: <pre><code>echo \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian $(lsb_release -cs) stable\" | tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre></p> <p>Update and install Docker: <pre><code>apt update\napt install docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre></p> <p>Verify that Docker is installed correctly by running an image that prints a message and exits: <pre><code>systemctl status docker\ndocker run --rm hello-world\n</code></pre></p>"},{"location":"blog/2023/12/23/installing-docker/#configuring-docker","title":"Configuring Docker","text":""},{"location":"blog/2023/12/23/installing-docker/#log-rotation","title":"Log Rotation","text":"<p>By default, log rotation is disabled (docs). To enable log rotation, create a Docker config file /etc/docker/daemon.json with the following content:</p> <pre><code>{\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\"\n  }\n}\n</code></pre> <p>Restart the Docker daemon (<code>service docker restart</code>\u0013). To verify, run <code>docker info</code>.</p>"},{"location":"blog/2023/12/23/installing-docker/#docker-data-directory","title":"Docker Data Directory","text":"<p>On the Proxmox host, create Docker data directories: <pre><code>mkdir /rpool/data/docker\nmkdir /rpool/encrypted/docker\n</code></pre></p>"},{"location":"blog/2023/12/23/installing-docker/#troubleshooting-docker","title":"Troubleshooting Docker","text":""},{"location":"blog/2023/12/23/installing-docker/#inspecting-container-logs","title":"Inspecting Container Logs","text":"<p>Docker captures stdout and stderr output from within containers and writes it to log files. To inspect, use docker compose logs (docker logs works, too), e.g., like this:</p> <pre><code>docker compose logs &lt;container&gt; --tail 30 --timestamps\n</code></pre> <p> Helge Klein --- Docker Host on Proxmox</p> <p> Gurucomputing --- Installing a Docker environment</p> <p>Unprivileged LXC containers</p>"},{"location":"blog/2021/11/21/proxmox-v8-source-in-pandaworld/","title":"Proxmox v8 Source in PandaWorld","text":""},{"location":"blog/2021/11/21/proxmox-v8-source-in-pandaworld/#usage","title":"Usage","text":"<p>Note</p> <p>comment the context for disable the ENTERPRISE repo in following file <pre><code>nano /etc/apt/sources.list.d/pve-enterprise.list\n</code></pre></p>"},{"location":"blog/2021/11/21/proxmox-v8-source-in-pandaworld/#domestic-atp-resource-of-ustc","title":"Domestic ATP resource of ustc","text":"<p><pre><code># GPG key sync up\nwget https://mirrors.ustc.edu.cn/proxmox/debian/proxmox-release-bookworm.gpg -O /etc/apt/trusted.gpg.d/proxmox-release-bookworm.gpg\n</code></pre> <pre><code># The apt resource for no-subscription\necho \"deb https://mirrors.ustc.edu.cn/proxmox/debian bookworm pve-no-subscription\" &gt; /etc/apt/sources.list.d/pve-no-subscription.list\n</code></pre> <pre><code># Replace the ustc mirrors on latest Proxmox Debian system\nsed -i 's|^deb http://ftp.debian.org|deb https://mirrors.ustc.edu.cn|g' /etc/apt/sources.list\nsed -i 's|^deb http://security.debian.org|deb https://mirrors.ustc.edu.cn/debian-security|g' /etc/apt/sources.list\n</code></pre> <pre><code># Replace the Ceph resource\necho \"deb https://mirrors.ustc.edu.cn/proxmox/debian/ceph-quincy bookworm no-subscription\" &gt; /etc/apt/sources.list.d/ceph.list\n</code></pre> <pre><code># Replace the CT(Container) resource\nsed -i 's|http://download.proxmox.com|https://mirrors.ustc.edu.cn/proxmox|g' /usr/share/perl5/PVE/APLInfo.pm\n</code></pre></p>"},{"location":"blog/2021/11/21/proxmox-v8-source-in-pandaworld/#proxmox-v7-source-in-pandaworld","title":"Proxmox v7 Source in PandaWorld","text":""},{"location":"blog/2021/11/21/proxmox-v8-source-in-pandaworld/#usage_1","title":"Usage","text":"<p>Note</p> <p>Moving into /etc/apt/sources.list.d/pve-enterprise.list and comments the original enterprise sources as below:</p> <pre><code>echo \"#deb https://enterprise.proxmox.com/debian/pve bullseye pve-enterprise\"\n&gt; /etc/apt/sources.list.d/pve-enterprise.list\n</code></pre>"},{"location":"blog/2021/11/21/proxmox-v8-source-in-pandaworld/#domestic-apt-resource","title":"Domestic APT resource","text":"<pre><code>vi /etc/apt/sources.list\n</code></pre> <pre><code>deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-free\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-free\ndeb https://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-free\ndeb-src https://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-free\n</code></pre>"},{"location":"blog/2021/11/16/proxmox-v7-subscription/","title":"Proxmox v7 Subscription","text":""},{"location":"blog/2021/11/16/proxmox-v7-subscription/#usage","title":"Usage","text":"<p>Note</p> <p>Move into /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js and chage as following:</p> <p>Code</p> <p><pre><code>    if (res === null || res === undefined ||!res || res.data.status.toLowerCase() !== 'active') {\n\n    if (false) {\n</code></pre> <pre><code>    systemctl restart pveproxy.service\n    reboot\n</code></pre></p>"},{"location":"blog/2022/12/28/proxmox-disk-cleanup/","title":"Proxmox disk cleanup","text":"<pre><code># use ncdu command to check the directory usage under root\nnudc /\n</code></pre>"},{"location":"blog/2022/12/28/proxmox-disk-cleanup/#linux-old-kernel-cleanup","title":"Linux old kernel cleanup","text":"<pre><code>OLD_KERNEL= dpkg -l | awk '{print $2}' | grep -e ^pve-.*$c\n\napt-get remove --purge $OLD_KERNEL\n</code></pre>"},{"location":"blog/2022/01/25/rocky-linux/","title":"Rocky Linux","text":"<p>Rocky Linux is an open-source enterprise operating system designed to be 100% bug-for-bug compatible with Red Hat Enterprise Linux\u00ae. It is under intensive development by the community.</p>"},{"location":"blog/2022/01/25/rocky-linux/#mirror-resources","title":"Mirror Resources","text":"Country Mirror Name Categories Bandwidth JP JAIST ftp.jaist.ac.jp 60000 CN eScience Center, Nanjing University mirrors.nju.edu.cn 10000 SG RPMDB.org sg.rpmdb.org 10000 Replace Method<pre><code>sed -e 's|^mirrorlist=|#mirrorlist=|g' \\\n    -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://ftp.jaist.ac.jp/rocky|g'\n    -i.bak \\\n    /etc/yum.repos.d/Rocky-*.repo\n\ndnf makecache\n</code></pre> Revert Method<pre><code>sed -e 's|^#mirrorlist=|mirrorlist=|g' \\\n    -e 's|^baseurl=https://mirrors.sjtug.sjtu.edu.cn/rocky|#baseurl=http://dl.rockylinux.org/$contentdir|g' \\\n    -i.bak \\\n    /etc/yum.repos.d/Rocky-*.repo\n\ndnf makecache\n</code></pre>"},{"location":"blog/2022/01/25/rocky-linux/#docker","title":"Docker","text":"docker repo &amp; service control<pre><code>sudo dnf config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo &amp;&amp; dnf update\nsudo dnf install -y docker-ce docker-ce-cli containerd.io\nsudo systemctl enable --now docker\nsudo systemctl status docker\n</code></pre>"},{"location":"blog/2022/01/25/rocky-linux/#docker-user-group","title":"Docker User Group","text":"<pre><code>sudo usermod -aG docker $USER\n</code></pre> <p>KMS Service</p> <pre><code>docker pull mikolatero/vlmcsd   # updated last year ago\ndocker run -d -p 1688:1688 --restart=always --name vlmcsd mikolatero/vlmcsd\n\ndocker pull mogeko/vlmcsd       # the latest replacement\ndocker run -d --name vlmcsd -p 1688:1688 --restart unless-stopped ghcr.io/mogeko/vlmcsd:latestj\n</code></pre>"},{"location":"blog/2022/01/25/rocky-linux/#docker-compose","title":"Docker-Compose","text":"docker-compose installation<pre><code>e.g. #VERSION = 1.29.2\nsudo curl -L \"https://github.com/docker/compose/releases/download/#VERSION/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\nchmod +x /usr/local/bin/docker-compose\nln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\n</code></pre>"},{"location":"blog/2023/06/05/secure-store/","title":"Secure-store","text":""},{"location":"blog/2023/06/05/secure-store/#gpg","title":"GPG","text":"<p>GnuPG is a complete and free implementation of the OpenPGP standard as defined by RFC4880 (also known as PGP). GnuPG allows you to encrypt and sign your data and communications; it features a versatile key management system, along with access modules for all kinds of public key directories.</p> <pre><code>gpg --gen-key\n\n# edit expire if needed\ngpg --edit-key GPG_PUB_KEY\n\n# export GPG pub key\ngpg --output GPG_PUB_KEY --armor --export EMAIL_ID\n\n# export GPG pri key\ngpg --output GPG_PRI_KEY --armor --export-secret-key EMAIL_ID\n\n# import keys\ngpg --import GPG_PRI_KEY\ngpg --import GPG_PUB_KEY\n</code></pre> <p>The standard unix password manager</p>"},{"location":"blog/2023/06/05/secure-store/#pass","title":"pass","text":"<pre><code>pass init GPG_PUB_KEY\n\n# init git repo for pass secret-store\npass git init\n\npass insert github\npass generate aws\npass show\npass git log\n\n# set it up for sys env\nexport key=$(pass show KEY_PATH/KEY_ID)\n</code></pre>"},{"location":"blog/2022/06/28/sublime-unlimited-for-linux-user/","title":"SublimE (Unlimited) for Linux User","text":"<ul> <li>bash \"sublime_text\" is in /opt/sublime_text/sublime_text</li> <li>Go to https://hexed.it/</li> <li>Open and input \"sublime_text\"</li> <li>Search for \"80 78 05 00 0F 94 C1\"</li> <li>Change into \"C6 40 05 01 48 85 C9\"</li> <li>Do export</li> </ul> <pre><code>sudo mv /opt/sublime_text/sublime_text /opt/sublime_text/sublime_text.old\n</code></pre> <pre><code>sudo mv $HOME/Downloads/sublime_text /opt/sublime_text/sublime_text\n</code></pre> <pre><code>sudo chmod 755 /opt/sublime_text/sublime_text\n</code></pre> <pre><code>sudo chown root:root /opt/sublime_text/sublime_text\n</code></pre>"},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/","title":"Secure Synology NAS with a custom domain, dynamic DNS and a free SSL certification","text":""},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#acme","title":"ACME","text":"CA valid days ECC algorithm max domains wildcard IPv4 IPv6 NotAfter IDN Let's Encrypt 90 Yes 100 Yes No No No Yes ZeroSSL 90 Yes 100 Yes No No Yes Yes Google 90 Yes 100 Yes No No Yes No Buypass 180 Yes 5 Paid No No No Yes SSL.com 90 Yes 2 Paid No No No Yes HiCA 180 Paid 10 (1 if wildcard) Yes Paid Paid No Paid"},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#pre-requisites","title":"Pre-requisites","text":"<p>This is what you will need:</p> <ul> <li>A FreeDNS account to have dynamic DNS. Create an account if you don't have it.</li> <li>A domain of your own with full control over it. If you don't have one, you can also do register.</li> <li>acme.sh - A pure Unix shell script implementing script</li> </ul>"},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#dynamic-dns-with-freedns","title":"Dynamic DNS with FreeDNS","text":"<p>Your ISP can change your public IP without warning, and usually does it each time your router is rebooted, so you need a way to update the DNS name servers whenever that happens. FreeDNS is a service that allow you to freely update DNS records so that your domain name points to your public IP all the time</p>"},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#forward-your-custom-domain-to-freedns-servers","title":"Forward your custom domain to FreeDNS servers","text":"<p>Change NS records on registrar(eg. GoDaddy,Namecheap,etc) <pre><code>NS1.AFRAID.ORG\nNS2.AFRAID.ORG\nNS3.AFRAID.ORG\nNS4.AFRAID.ORG\n</code></pre></p>"},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#add-subdomain-of-type-a","title":"Add subdomain of type A","text":"<p>The most important ones right now are your generic domain (eg, <code>whatever.com</code>) and the www host ( <code>www.whatever.com</code>). The IP is not that important at this point, since we will be updating it dynamically.</p>"},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#edit-ddns-on-synology","title":"Edit DDNS on Synology","text":"<ul> <li>Open Control Panel</li> <li>Browse to <code>Connectivity</code> -&gt; <code>External Access</code></li> <li>Click the DDNS tab and then click the <code>Add</code> button</li> <li>In <code>Service Provider</code>, choose Freedns.org</li> <li>In Hostname, type the domain name, eg <code>whatever.com</code></li> <li>Fill in Username and Password with Free credentials</li> <li>Click the Test Connection button</li> </ul> <p>Waiting until the status going on <code>Normal</code> for configuration save.</p>"},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#install-docker","title":"Install Docker","text":""},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#create-a-group-for-docker","title":"Create a group for Docker","text":"<ul> <li>Open Control Panel</li> <li>Browse to User&amp;group</li> <li>Click on Group and create a new group called <code>docker</code></li> <li>Click on Members</li> <li>Add user to the group</li> </ul>"},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#grant-docker-group-permission-to-run-docker","title":"Grant docker group permission to run Docker","text":"<p>Issue the command to be able to use docker without having to <code>sudo</code>everything: <pre><code>sudo chown root:docker /var/run/docker.sock\n</code></pre></p>"},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#certificate-with-zerossl","title":"Certificate with ZeroSSL","text":"<p>Browsers these days are more an more concerned about security, and while you can connect to your NAS ignoring the warnings, I prefer not having that feeling of something\u2019s wrong by issuing my own certificate and publishing it in my Synology. ZeroSSL is a service that allows you to issue a certificate for your domain at no cost.</p>"},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#to-come-to-the-git-service-which-not-install-on-synology","title":"To come to the git service which not install on Synology.","text":"<pre><code>docker run -ti --rm -v ${HOME}:/root -v $(pwd):/git alpine/git clone https://github.com/acmesh-official/acme.sh.git acmesh\n\ncd acmesh\nchmod +ux ./acme.sh\n</code></pre>"},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#export-variable-for-freedns-script-for-work","title":"Export variable for FreeDNS script for work","text":"<pre><code>export FREEDNS_User=\"&lt;FreeDNS username&gt;\"\nexport FREEDNS_Password=\"&lt;FreeDNS password\"\n</code></pre>"},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#test","title":"Test","text":"<pre><code>./acme.sh --issue --dns dns_freedns -d whatever.com -d www.whatever.com --test\n</code></pre>"},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#register-in-zerossl","title":"Register in ZeroSSL","text":"<pre><code># Replace the email address with -m option\n./acme.sh --register-account -m me@whatever.com --server zerossl\n</code></pre>"},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#issue-the-certificate","title":"Issue the certificate","text":"<pre><code># The --force flag is required only if you did the --test before.\n/acme.sh --issue --dns dns_freedns -d whatever.com -d www.whatever.com --force\n</code></pre> <p>After a while, the feedback will be showed up <pre><code>Your cert is in: /acme.sh/whatever.com/whatever.com.cer\nYour cert key is in: /acme.sh/whatever.com/whatever.com.key\nThe intermediate CA cert is in: /acme.sh/whatever.com/ca.cer\nAnd the full chain certs is there: /acme.sh/whatever.com/fullchain.cer\n</code></pre></p>"},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#publish-the-certification-to-synology","title":"Publish the certification to Synology","text":"<p>To deploy the certificate in NAS. ACME has to do something called deploy hooks, that take care of the required steps to publish certificate in different environments. The Synology_dsm_hook will be used.</p> <pre><code>export SYNO_Username=\"&lt;DSM Admin Username&gt;\"\nexport SYNO_Password=\"&lt;DSM Admin Password&gt;\"\nexport SYNO_Certificate=\"acme.sh whatever.com ZS certificate\"\nexport SYNO_Create=1 # default is off, means setting is not saved.  By setting to 1 we create the certificate if it's not in DSM\nacme.sh --deploy -d whatever.com --deploy-hook synology_dsm\n</code></pre>"},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#program-the-renewal-of-the-cert","title":"Program the renewal of the cert","text":"<pre><code>./acme.sh --cron\n</code></pre>"},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#schedule-a-task-for-cron-job-on-synology","title":"Schedule a task for cron job on Synology","text":"<ul> <li>Control Panel -&gt; Task Scheduler</li> <li>Create -&gt; Scheduled Task -&gt; User-defined script</li> <li>Give a task name eg \"Renew cert\" and choose the use under docker group</li> <li>Set a repeat time for job loop</li> <li>Add following script into User-defined tab: <pre><code>/var/services/homes/fernando/acmesh/acme.sh --cron\n</code></pre></li> </ul>"},{"location":"blog/2024/07/10/secure-synology-nas-with-a-custom-domain-dynamic-dns-and-a-free-ssl-certification/#change-the-cert-in-dsm-as-the-default","title":"Change the cert in DSM as the default","text":"<ul> <li>Control Panel -&gt; Connectivity -&gt; Security</li> <li>Click Certificate tab</li> <li>The new cert should be showing up</li> <li>Select the cert as the default for the services</li> <li>Click OK for save.</li> </ul> <p> Medium dr-b.io</p>"},{"location":"blog/2022/03/10/terraform-works-on-kvm-for-iaas/","title":"Terraform works on KVM for IaaS","text":"<p>Terraform is an open-source infrastructure as code software tool that provides a consistent CLI workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files.</p>"},{"location":"blog/2022/03/10/terraform-works-on-kvm-for-iaas/#summary","title":"Summary","text":"<p>PopOS</p> <p>libvirt_domain.domain: Error creating libvirt domain: virError(Code=1, Domain=10, Message='internal error: process exited while connecting to monitor: 2019-01-28T02:29:14.861688Z qemu-system-x86_64: -drive file=/var/lib/libvirt/images/volume-0,format=qcow2,if=none,id=drive-virtio-disk0: Could not open '/var/lib/libvirt/images/volume-0': Permission denied')</p> <p>{++The reason on the problem++}:</p> <p>{++On Ubuntu/Debian distros SELinux is enforced by qemu even if it is disabled globally, this might cause unexpected++}</p> <p>Walkaround:</p> <pre><code>#/etc/libvirt/qemu.conf\n~ security_driver = \"none\"\n~ sudo systemctl restart libvirtd.service\n</code></pre>"},{"location":"blog/2022/06/15/tmux/","title":"Tmux","text":""},{"location":"blog/2022/06/15/tmux/#cheat-sheet-quick-reference","title":"\ud83d\udd17 Cheat Sheet &amp; Quick Reference","text":""},{"location":"blog/2022/06/15/tmux/#sessions","title":"<p>Sessions</p>","text":"Description Command Description Command Start a new settion tmux Show all sessions tmux ls tmux new tmux list-sessions tmux new-session Attach to last session tmux a :new tmux at tmux attach tmux attach-session Start a new session with the name mysession tmux new -s mysession kill/delete session mysession tmux kill-ses -t mysession :new -s mysession tmux kill-session -t mysession Attach to a session with the name mysession tmux a -t mysession kill/delete all session but mysession tmux kill-session -a -t mysession tmux at -t mysession kill/delete all session but the current tmux kill-session -a tmux attach -t myssesion tmux attach-session -t mysession Rename session Ctrl + b  $ Session and Window Preview Ctrl + b  w Detach from session Ctrl + b  d Move to previous session Ctrl + b  ( Detach others on the session (Maximize window by detch other clients) :attac -d Move to next session Ctrl + b  )"},{"location":"blog/2022/06/15/tmux/#windows","title":"<p>Windows</p>","text":"Description Command Description Command Start a new session with the name mysession tmux new -s mysession -n mywindow Next window Ctrl + b  n Create window Ctrl + b  c Switc/select window by number Ctrl + b  0...9 Rename current window Ctrl + b  , Toggle last active window Ctrl + b  l Close current window Ctrl + b  &amp; Reorder windows, swap window number 2(src) and 1(dst) :swap-window -s 2 -t 1 List windows Ctrl + b  w Move current window to the left by one position :swap-window -t -1 Previous window Ctrl + b  p"},{"location":"blog/2022/06/15/tmux/#panes","title":"<p>Panes</p>","text":"Description Command Description Command Toggle last active pane Ctrl + b  ; Show pane numbers Ctrl + b  q Split pane with horizontal layout Ctrl + b  % Switch/select pane by number Ctrl + b  q  0..9 Split pane with vertical layout Ctrl + b  \" Toggle pane zoom Ctrl + b  z Move te current pane left Ctrl + b  { Convert pane into a window Ctrl + b  ! Move the current pane right Ctrl + b  } Resize current pane height(holding second key is optional) Ctrl +b  KEY Ctrl + b   Ctrl + KEY Switch to pane to the direction Ctrl + b KEY Resize current pane width(holding scond key is optional) Ctrl + b  KEY Ctrl + b   Ctrl + KEY Toggle synchronize-panes(send command to all panes) :setw synchronize-panes Close curent pane Ctrl + b  X Switch to next pane Ctrl + b  o"},{"location":"blog/2022/06/15/tmux/#copy-mode","title":"<p>Copy Mode</p>","text":"Description Command Description Command use vi keys in buffer :setw -g mode-keys vi Search forward / Enter copy mode Ctrl +b  [ Search backward ? Enter copy mode and scroll one page up Ctrl + b  PgUp Next keyword occurance n Quit mode q Previous keyword occurance N Go to top line g Start selection Spacebar Go to bottom line G Clear selection ESC Scroll up KEY Copy selection Enter Scroll down KEY Paste contents of buffer_0i Ctrl + b  ] Move cursor left h disaplay buffer_0 contents :show-buffer Move cursor down j copy entire visible contents of pane to a buffer :capture-pane Move cursor up k Show all buffers :list-buffers Move cursor right l Show all buffers and paste selected :choose-buffer Move cursor forward one word at a time w Save buffer contents to buf.txt :save-buffer buf.txt Move cursor backward one word at a time b delete buffer_1 :delete-buffer -b 1/span&gt;"},{"location":"blog/2022/06/15/tmux/#misc","title":"<p>Misc</p>","text":"Description Command Description Command Enter command mode Ctrl + b  : Set OPTION for all windows :setw -g OPTION Set OPTION for all sessions :set -g OPTION Enable mouse mode :set mouse on"},{"location":"blog/2022/06/15/tmux/#help","title":"<p>Help</p>","text":"Description Command Description Command tmux list-keys Show every session, window, pane, etc... tmux info List key bindings(shortcuts) Ctrl + b  ?"},{"location":"blog/2022/03/21/transparent-gnome-shell/","title":"Transparent Gnome Shell","text":""},{"location":"blog/2022/03/21/transparent-gnome-shell/#to-use-opacitytransparency-occasionally-it-is-better-use-xprop-command","title":"To use opacity/transparency occasionally - it is better use xprop command.","text":"<pre><code>xprop -format _NET_WM_WINDOW_OPACITY 32c -set _NET_WM_WINDOW_OPACITY 0x7FFFFFFF\n</code></pre> <ul> <li><code>0x7FFFFFFF</code> - 50% opacity</li> <li><code>0xFFFFFFFF</code> - 100% opacity</li> </ul> <p>Forum Reference</p>"},{"location":"blog/2022/03/21/transparent-gnome-shell/#one-tiny-shell-script-for-reference","title":"One tiny shell script for reference:","text":"<pre><code>#!/bin/bash\n\nread -p \"Set transparency percentage ? [Enter for 100%]\" mydectrans\n# only accept 10 to 99, rest is considered 100\n[[ \"$mydectrans\" != [1-9][0-9] ]] &amp;&amp; mydectrans=100\n# Convert decimal to 32bit hex representation\nmy32bhextrans=$(printf 0x%x $((0xffffffff * $((mydectrans)) / 100)))\n# Execute the action\nxprop -f _NET_WM_WINDOW_OPACITY 32c -set _NET_WM_WINDOW_OPACITY $my32bhextrans\n</code></pre> <p>Forum Reference</p>"},{"location":"blog/2022/09/28/vagrant-plugin/","title":"Vagrant Plugin","text":""},{"location":"blog/2022/09/28/vagrant-plugin/#background","title":"Background:","text":"<p>Due to the network in China is not good to go on plugin download and upgrade. Hence replace to local source is the better method.</p>"},{"location":"blog/2022/09/28/vagrant-plugin/#workaround","title":"Workaround:","text":"<ul> <li>Rubygems</li> <li>Rubygems-China <pre><code># Installation\n# Download the relevant package from upon website\n# Unpack into a dir and cd to the folder\n# run following command for Installation\n$ ruby setup.rb\n</code></pre></li> <li>If the permission error message warning up, export ENV for tackle down. <pre><code>$ export GEM_HOME=$HOME/.gem/ruby/3.1.0/\n</code></pre></li> </ul>"},{"location":"blog/2022/09/28/vagrant-plugin/#rubygems-install-the-package","title":"Rubygems install the package","text":"<pre><code>$ gem sources --add https://gems.ruby-china.com/ --remove https://rubygems.org/\n\n$ gem sources -l\nhttps://gems.ruby-china.com\n\n$ gem install vagrant-vbguest\n</code></pre>"},{"location":"blog/2024/07/22/openwrt-on-xiaomi-r3p/","title":"Openwrt on XiaoMi R3P","text":""},{"location":"blog/2024/07/22/openwrt-on-xiaomi-r3p/#ready-for-go","title":"Ready for go","text":"<ul> <li>Download Developer ROM from XiaoMi     ROM</li> <li>Make sure to bundle router with XiaoMi App, then download SSH firmware, file the password     Firmware</li> <li>Download Openwrt</li> <li>One USB with FAT32</li> <li>One toothpick</li> </ul>"},{"location":"blog/2024/07/22/openwrt-on-xiaomi-r3p/#xiaomi-dev-firmware","title":"XiaoMi Dev Firmware","text":""},{"location":"blog/2024/07/22/openwrt-on-xiaomi-r3p/#option-1","title":"Option 1:","text":"<p>Login to XiaoMi router and flush dev firmware</p>"},{"location":"blog/2024/07/22/openwrt-on-xiaomi-r3p/#option-2","title":"Option 2:","text":"<ol> <li>Copy XiaoMi firmware(dev) into USB.</li> <li>Poweroff XiaoMi router, plug in USB and press&amp;hold \"reset\" button, then power router on.</li> <li>Waiting for light flashing to yellow, then release \"reset\" button.</li> <li>Succeed when light flashing to blue.</li> </ol>"},{"location":"blog/2024/07/22/openwrt-on-xiaomi-r3p/#xiaomi-ssh","title":"XiaoMi SSH","text":""},{"location":"blog/2024/07/22/openwrt-on-xiaomi-r3p/#refer-as-firmware-option-2","title":"Refer as firmware option 2","text":""},{"location":"blog/2024/07/22/openwrt-on-xiaomi-r3p/#login-router-through-ssh","title":"Login router through ssh","text":"<pre><code>ssh root@192.168.31.1\n</code></pre>"},{"location":"blog/2024/07/22/openwrt-on-xiaomi-r3p/#flush-openwrt","title":"Flush Openwrt","text":"<pre><code>scp $PATH/Openwrt.bin root@192.168.31.1:/tmp\ncd /tmp\n</code></pre> <p><pre><code># \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\nnvram set flag_try_sys1_failed=1\nnvram set flag_try_sys2_failed=0\nnvram set flag_boot_success=0\nnvram commit # save ENV into RAM\ndd if=factory.bin bs=1M count=4 | mtd write - kernel1 # invoke openwrt.bin and write into disk partition\nmtd erase rootfs0 # Erase all data on rootfs0 partition\nmtd erase rootfs1 # Erase all data on rootfs1 partition\nmtd erase overlay # Erase all data on overlay partition\ndd if=factory.bin bs=1M skip=4 | mtd write - rootfs0 # invoke openwrt.bin and write into rootfs0 partition\nreboot\n</code></pre> dd mtd</p> <pre><code># review system partition\ncat /proc/mtd\n</code></pre>"},{"location":"blog/2024/07/22/openwrt-on-xiaomi-r3p/#revert-to-xiaomi-official-firmware","title":"Revert to XiaoMi Official Firmware","text":"<p>Step 1</p> <pre><code># ENV\nfw_setenv flag_try_sys1_failed 0\nfw_setenv flag_try_sys2_failed 1\nfw_setenv flag_boot_success 0\n</code></pre> <p>Failure</p> <p>Install Openwrt Official R3P upgrade firmware if fails do upon command for ENV. Do it again after then.</p> <p>Step 2</p> <p>Poweroff router, refer as XiaoMi Dev Firmware</p> <p> mucen</p>"},{"location":"blog/2022/03/05/ibus/","title":"iBus","text":"<p>IBus (Intelligent Input Bus) is an input method framework, a type of application that allows for easily switching between different keyboard layouts. When combined with an input method editor, it also allows for typing non-Latin characters using a keyboard that does not natively support them.</p> <p></p>"},{"location":"blog/2022/03/05/ibus/#installation","title":"Installation","text":"<pre><code>paru/yay ibus-rime\nparu/yay rime-double-pinyin\n</code></pre> <p>environment (optional)</p> <pre><code># /etc/environment\n  GTK_IM_MODULE=ibus\n  QT_IM_MODULE=ibus\n  XMODIFIERS=@im=ibus\n</code></pre>"},{"location":"blog/2022/03/05/ibus/#rime","title":"Rime","text":"<p>Create the file of <code>\"default.custom.yaml\"</code> in the $HOME/.config/ibus/rime.</p> <pre><code># default.custom.yaml\n# save it to:\n#   ~/.config/ibus/rime  (linux)\n#   ~/Library/Rime       (macos)\n#   %APPDATA%\\Rime       (windows)\n\npatch:\n  schema_list:\n    - schema: luna_pinyin          # \u6719\u6708\u62fc\u97f3\n    - schema: luna_pinyin_simp     # \u6719\u6708\u62fc\u97f3 \u7b80\u5316\u5b57\u6a21\u5f0f\n    - schema: double_pinyin_flypy  # \u5c0f\u9db4\u96d9\u62fc\n    - schema: emoji                # emoji \u8868\u60c5\n</code></pre>"},{"location":"blog/2022/03/05/ibus/#wizard-configuration","title":"Wizard Configuration","text":"<pre><code>Ctrol+`\n</code></pre>"},{"location":"blog/2022/03/05/ibus/#autostart","title":"AutoStart","text":"<p>To launch IBus on user login, create an autostart entry with the following method:</p> <p>profile</p> <pre><code># /etc/profile.d\ncat /etc/profile.d/ibus.sh &lt;&lt; EOF\nibus-daemon -drxR\nEOF\n</code></pre>"},{"location":"blog/2022/01/13/vaultbitwarden/","title":"vaultBitwarden","text":"<p>This is a Bitwarden server API implementation written in Rust compatible with upstream Bitwarden clients*, perfect for self-hosted deployment where running the official resource-heavy service might not be ideal.</p>"},{"location":"blog/2022/01/13/vaultbitwarden/#features","title":"Features","text":"<p>Basically full implementation of Bitwarden API is provided including:</p> <ul> <li>Basic single user functionality</li> <li>Organizations support</li> <li>Attachments</li> <li>Vault API support</li> <li>Serving the static files for Vault interface</li> <li>Website icons API</li> <li>Authenticator and U2F support</li> <li>YubiKey OTP</li> </ul>"},{"location":"blog/2022/01/13/vaultbitwarden/#installation","title":"Installation","text":"<p>Work_Dir: <pre><code>mkdir ${WORK_DIR}\n</code></pre></p> <p>Cert: certification<pre><code>openssl req -x509 -newkey rsa:4096 -keyout ${NAME}.key \\\n-out ${NAME}.crt -days 720 -nodes\n</code></pre></p> <p>Container: docker run<pre><code># Get into the Work_Dir and run following dockers command\n\ndocker run -d --restart always --name ${CONTAINER_NAME} -e \\\nROCKET_TLS='{certs=\"/${SSL_PATH}/${SSL_NAME}.crt\",key=\"/${SSL_PATH}/${SSL_NAME}.key\"}' \\\n-v /$HOME/vaultwarden/ssl/:/ssl/ \\\n-v /$HOME/vaultwarden/vw-data:/data/ \\\n-p 443:80 vaultwarden/server:latest\n</code></pre></p> <p>Parameter help</p> <p>-e      set the environment</p> <p>-v      set the volume</p> <p>-p      set the port</p> <p>Warning</p> <p>${SSL_PATH} should be resided at the same path in Work_Dir when invoke the docker run.</p> <p>DB Backup:</p>  Systemd Bash DB Crontab systemd.service<pre><code>[Unit]\nDescription=bitwarden-watch-for-changes\n\n[Service]\nType=simple\nExecStart=/home/USER/bitwardenrs/watch-for-changes.sh\nRestart=always\nWorkingDirectory=/home/USER\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> systemd.sh<pre><code>sudo sed -i 's/^SELINUX=.*/SELINUX=[disabled|permissive]/g' /etc/selinux/config &amp;&amp; sudo sestatus\nsudo reboot\nsudo cp /PATH/bitwarden-watch-for-changes.service /etc/systemd/system/multi-user.target.wants\nsudo systemctl enable/start/status bitwarden-watch-for-changes.service\nsudo journalctl         //troubleshoot\nsudo chmod +x watch-for-changes.shd\n</code></pre> watcher.sh<pre><code>#!/usr/bin/env bash\nexport PATH=/usr/local/inotify-tools/bin/\npath_to_watch=/home/USER/bitwardenrs/bw-data\n\ninotifywait -m \"$path_to_watch\" -e create -e moved_to -e modify |\n    while read path action file; do\n    if [[ $file =~ (wal|config.json) ]]; then\n        echo '1' &gt; /home/USER/bitwardenrs/bitwarden-folder-updated;\n    fi\n    done\n</code></pre> <p>inotify-tools: inotify's features to be used from within shell scripts</p> <p><pre><code>dnf -y install autoconf automake libtool\n\nwget -c https://github.com/inotify-tools/inotify-tools/archive/refs/tags/3.21.9.6.tar.gz\n\ntar -zvxf 3.21.9.6.tar.gz -C /usr/local/src/\n</code></pre> <pre><code>cd /usr/local/src/inotify-tools-3.21.9.6/\n./autogen.sh &amp;&amp; \\\n    ./configure --prefix=/usr/local/inotify-tools &amp;&amp; \\\n    make &amp;&amp; \\\n    make install\nls /usr/local/inotify-tools/bin/\n</code></pre></p> <p>Github Reference</p> db-backup.sh<pre><code>#!/usr/bin/env bash\n\nset -e\n\nbitwarden_folder_updated=/home/USER/bitwarden/bitwarden-folder-updated\ntouch $bitwarden_folder_updated\n\nif [[ \"$(cat $bitwarden_folder_updated)\" == \"1\" ]]; then\nrm -f /home/USER/bw-bk.tar.gz\n\ndocker exec bitwarden bash -c 'mkdir -p /data/db-backup &amp;&amp; sqlite3 /data/db.sqlite3 \".backup /data/db-backup/backup.sqlite3\"'\n\ncd /home/USER/bitwarden/bw-data\ntar -czvf /home/USER/bw-bk.tar.gz \\\n    config.json \\\n    icon_cache \\\n    db-backup/backup.sqlite3\n\ncd /home/USER/bitwarden/\ntar -czvf /home/USER/bw-scripts.tar.gz \\\n    backup.sh \\\n    bitwarden-watch-for-changes.service \\\n    watch-for-changes.sh\n\nmv /home/USER/bw-bk.tar.gz /SMB,NFS.AW3.../BitwardenBak/bw-bk-$(date +\"%Y-%m-%d\").tar.gz\nmv /home/USER/bw-scripts.tar.gz /SMB,NFS.AW3.../BitwardenBak/bw-scripts-$(date +\"%Y-%m-%d\").tar.gz\n\necho \"0\" &gt; /home/USER/bitwarden/bitwarden-folder-updated\nelse\necho 'nothing to backup'\nfi\n</code></pre> Sqlite3 sqlite3: curl the source and complie. sqlite3 in docker env: <pre><code>    # dnf install make, gcc\n    # tar xvfz sqlite-autoconf-&lt;version&gt;.tar.gz\n    # cd sqlite-autoconf-&lt;version&gt;\n    # ./configure\n    # make\n    # make install\n</code></pre> <pre><code>    # docker exec -it CONTAINER bash -c 'cp /datasource/sqlite3 /usr/local/bin'\n</code></pre> <p>Example</p> <pre><code>crontab -e\n0 3 * * * $PATH/*.sh &gt; /dev/null 2&gt;&amp;1   # Rsync on 3:00 AM\n0 * * * * $PATH/*.sh &gt; /dev/null 2&gt;&amp;1   # Rsync every hour\n\ncrontab -l              # List the schedule\n</code></pre>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2022/","title":"2022","text":""},{"location":"blog/archive/2021/","title":"2021","text":""},{"location":"blog/category/router/","title":"Router","text":""},{"location":"blog/category/synology/","title":"Synology","text":""},{"location":"blog/category/gnu-project/","title":"GNU Project","text":""},{"location":"blog/category/proxmox/","title":"Proxmox","text":""},{"location":"blog/category/container/","title":"Container","text":""},{"location":"blog/category/vaultwarden/","title":"vaultWarden","text":""},{"location":"blog/category/proxy/","title":"Proxy","text":""},{"location":"blog/category/security/","title":"Security","text":""},{"location":"blog/category/openwrt/","title":"OpenWRT","text":""},{"location":"blog/category/vagrant/","title":"Vagrant","text":""},{"location":"blog/category/sublime/","title":"Sublime","text":""},{"location":"blog/category/tmux/","title":"Tmux","text":""},{"location":"blog/category/groff--mom/","title":"Groff &amp; MOM","text":""},{"location":"blog/category/kvm/","title":"KVM","text":""},{"location":"blog/category/gnome/","title":"Gnome","text":""},{"location":"blog/category/terraform/","title":"Terraform","text":""},{"location":"blog/category/ibus-rime/","title":"ibus-rime","text":""},{"location":"blog/category/rockylinux/","title":"RockyLinux","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/page/3/","title":"Blog","text":""},{"location":"blog/archive/2022/page/2/","title":"2022","text":""}]}